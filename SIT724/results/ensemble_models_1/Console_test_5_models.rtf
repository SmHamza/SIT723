{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf610
{\fonttbl\f0\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs20 \cf2 \cb3 \CocoaLigature0 ============Start Ensemble Strategy============\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20151002\
======A2C Training========\
Training time (A2C):  1.823607603708903  minutes\
======A2C Validation from:  20151002 to  20160104\
A2C Sharpe Ratio:  -0.06620190043809195\
\
======PPO Training========\
Training time (PPO):  6.24608952999115  minutes\
======PPO Validation from:  20151002 to  20160104\
PPO Sharpe Ratio:  0.037589230227289064\
\
======DDPG Training========\
Training time (DDPG):  1.1340885321299234  minutes\
======DDPG Validation from:  20151002 to  20160104\
DDPG Sharpe Ratio:  0.043871325576890684\
======TD3 Training========\
Training time (TD3):  1.611016567548116  minutes\
======TD3 Validation from:  20151002 to  20160104\
TD3 Sharpe Ratio:  -0.054605425851795744\
======GAIL Training========\
WARNING:tensorflow:From /Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/stable_baselines/sac/policies.py:63: The name tf.log is deprecated. Please use tf.math.log instead.\
\
actions (16990, 30)\
obs (16990, 181)\
rewards (16990,)\
episode_returns (10,)\
episode_starts (16990,)\
actions (16990, 30)\
obs (16990, 181)\
rewards (16990,)\
episode_returns (10,)\
episode_starts (16990,)\
Total trajectories: 10\
Total transitions: 16990\
Average returns: 100.00346330809407\
Std for returns: 0.0\
WARNING:tensorflow:From /Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/stable_baselines/common/mpi_running_mean_std.py:42: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\
\
WARNING:tensorflow:From /Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/stable_baselines/gail/adversary.py:116: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\
\
WARNING:tensorflow:From /Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\
\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 5.295 seconds\
computegrad\
WARNING:tensorflow:From /Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/stable_baselines/trpo_mpi/trpo_mpi.py:364: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.\
\
done in 0.467 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        1.1          0\
         1       69.3    0.00343\
         2      0.408     0.0581\
         3       6.19      0.438\
         4      0.445      0.439\
         5       1.53      0.609\
         6        221       1.29\
         7      0.383       1.31\
         8       1.01       1.49\
         9        810       1.63\
        10      0.532       2.02\
done in 0.591 seconds\
Expected: 0.136 Actual: 0.091\
Stepsize OK!\
vf\
done in 0.329 seconds\
sampling\
done in 5.529 seconds\
computegrad\
done in 0.011 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.22          0\
         1       12.2   1.54e-05\
         2       11.9     0.0207\
         3   2.39e+03      0.227\
         4       6.81       0.23\
         5       6.66      0.257\
         6   1.81e+04      0.282\
         7       38.7      0.814\
         8        646       1.32\
         9       13.6       1.32\
        10       21.7       1.98\
done in 0.060 seconds\
Expected: 0.208 Actual: 0.064\
Stepsize OK!\
vf\
done in 0.071 seconds\
sampling\
done in 4.782 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       71.5          0\
         1       2.04   0.000892\
         2        108      0.144\
         3       4.56      0.158\
         4       21.6      0.412\
         5       21.8      0.416\
         6       58.7       1.07\
         7        173       2.23\
         8       33.3       2.26\
         9   1.13e+03       2.66\
        10       19.4       2.81\
done in 0.054 seconds\
Expected: 0.253 Actual: 0.059\
Stepsize OK!\
vf\
done in 0.071 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.77034 |       0.96514 |       0.63835 |      -0.00064 |       0.47363 |       0.27344\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 24.5        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.535503   |\
| explained_variance_t... | 0.0484      |\
| meankl                  | 0.011590269 |\
| optimgain               | 0.058722228 |\
| reference_Q_mean        | 4.16        |\
| reference_Q_std         | 3.93        |\
| reference_action_mean   | -0.189      |\
| reference_action_std    | 0.949       |\
| reference_actor_Q_mean  | 4.74        |\
| reference_actor_Q_std   | 3.57        |\
| rollout/Q_mean          | 2.42        |\
| rollout/actions_mean    | -0.124      |\
| rollout/actions_std     | 0.808       |\
| rollout/episode_steps   | 1.7e+03     |\
| rollout/episodes        | 5           |\
| rollout/return          | 123         |\
| rollout/return_history  | 123         |\
| surrgain                | 0.058722228 |\
| total/duration          | 66.8        |\
| total/episodes          | 5           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 150         |\
| train/loss_actor        | -4.6        |\
| train/loss_critic       | 1.5         |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.4292141874631246  minutes\
======GAIL Validation from:  20151002 to  20160104\
GAIL Sharpe Ratio:  -0.04017740648968674\
======Trading from:  20160104 to  20160405\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x145fbe518>\
previous_total_asset:1000000\
end_total_asset:1087655.9013443242\
total_reward:87655.90134432423\
total_cost:  1220.4161070211308\
total trades:  783\
Sharpe:  0.26910741766777363\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20160104\
======A2C Training========\
Training time (A2C):  2.0542810479799907  minutes\
======A2C Validation from:  20160104 to  20160405\
A2C Sharpe Ratio:  0.24826752888796383\
======PPO Training========\
Training time (PPO):  7.004160634676615  minutes\
======PPO Validation from:  20160104 to  20160405\
PPO Sharpe Ratio:  0.15709095854132474\
======DDPG Training========\
Training time (DDPG):  1.0650375843048097  minutes\
======DDPG Validation from:  20160104 to  20160405\
DDPG Sharpe Ratio:  0.0023154201021166484\
======TD3 Training========\
Training time (TD3):  1.2181429346402486  minutes\
======TD3 Validation from:  20160104 to  20160405\
TD3 Sharpe Ratio:  0.09231350722917459\
======GAIL Training========\
actions (17620, 30)\
obs (17620, 181)\
rewards (17620,)\
episode_returns (10,)\
episode_starts (17620,)\
actions (17620, 30)\
obs (17620, 181)\
rewards (17620,)\
episode_returns (10,)\
episode_starts (17620,)\
Total trajectories: 10\
Total transitions: 17620\
Average returns: 217.10649917880073\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.563 seconds\
computegrad\
done in 0.440 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.06          0\
         1      0.802     0.0817\
         2       2.85      0.375\
         3       1.01      0.564\
         4      0.645       1.77\
         5        0.5       3.38\
         6       2.12       3.69\
         7      0.658       4.75\
         8      0.818       5.79\
         9       1.21       5.96\
        10       2.91       6.38\
done in 0.928 seconds\
Expected: 0.249 Actual: 0.104\
violated KL constraint. shrinking step.\
Expected: 0.249 Actual: 0.066\
Stepsize OK!\
vf\
done in 0.184 seconds\
sampling\
done in 5.458 seconds\
computegrad\
done in 0.018 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.35          0\
         1       1.45      0.101\
         2       3.93      0.125\
         3       3.82      0.405\
         4       3.97      0.513\
         5       1.53      0.588\
         6       2.77       1.43\
         7      0.645       1.57\
         8       9.53       2.26\
         9       4.18       2.39\
        10      0.393       2.86\
done in 0.068 seconds\
Expected: 0.191 Actual: 0.120\
Stepsize OK!\
vf\
done in 0.096 seconds\
sampling\
done in 4.643 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.07          0\
         1       4.98    0.00423\
         2       20.1     0.0303\
         3       3.55       0.15\
         4       8.56       0.31\
         5       25.1      0.633\
         6       4.14       1.37\
         7       24.9       1.39\
         8       47.6       2.01\
         9       50.4       2.19\
        10       6.21       3.51\
done in 0.055 seconds\
Expected: 0.253 Actual: 0.095\
violated KL constraint. shrinking step.\
Expected: 0.253 Actual: 0.048\
Stepsize OK!\
vf\
done in 0.087 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.88253 |       0.97083 |       0.63484 |      -0.00063 |       0.34766 |       0.27637\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 17.2        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.57808    |\
| explained_variance_t... | 0.0365      |\
| meankl                  | 0.009174638 |\
| optimgain               | 0.04799886  |\
| reference_Q_mean        | 2.67        |\
| reference_Q_std         | 2.97        |\
| reference_action_mean   | -0.0424     |\
| reference_action_std    | 0.961       |\
| reference_actor_Q_mean  | 3.55        |\
| reference_actor_Q_std   | 2.99        |\
| rollout/Q_mean          | 1.93        |\
| rollout/actions_mean    | 0.0303      |\
| rollout/actions_std     | 0.791       |\
| rollout/episode_steps   | 1.76e+03    |\
| rollout/episodes        | 5           |\
| rollout/return          | 114         |\
| rollout/return_history  | 114         |\
| surrgain                | 0.04799886  |\
| total/duration          | 63          |\
| total/episodes          | 5           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 159         |\
| train/loss_actor        | -3.62       |\
| train/loss_critic       | 1.28        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.0849084695180258  minutes\
======GAIL Validation from:  20160104 to  20160405\
GAIL Sharpe Ratio:  0.017589563716591688\
======Trading from:  20160405 to  20160705\
Used Model:  <stable_baselines.a2c.a2c.A2C object at 0x14041a9b0>\
previous_total_asset:1087655.9013443242\
end_total_asset:1091569.7783425373\
total_reward:3913.87699821312\
total_cost:  4989.403263449004\
total trades:  1453\
Sharpe:  0.021842607988605923\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20160405\
======A2C Training========\
Training time (A2C):  2.0161526203155518  minutes\
======A2C Validation from:  20160405 to  20160705\
A2C Sharpe Ratio:  -0.03629118197579135\
======PPO Training========\
Training time (PPO):  6.179561932881673  minutes\
======PPO Validation from:  20160405 to  20160705\
PPO Sharpe Ratio:  0.10576869711867946\
======DDPG Training========\
Training time (DDPG):  1.0472078005472818  minutes\
======DDPG Validation from:  20160405 to  20160705\
DDPG Sharpe Ratio:  0.08093748153164225\
======TD3 Training========\
Training time (TD3):  1.1982518633206685  minutes\
======TD3 Validation from:  20160405 to  20160705\
TD3 Sharpe Ratio:  0.015776159689921423\
======GAIL Training========\
actions (18250, 30)\
obs (18250, 181)\
rewards (18250,)\
episode_returns (10,)\
episode_starts (18250,)\
actions (18250, 30)\
obs (18250, 181)\
rewards (18250,)\
episode_returns (10,)\
episode_starts (18250,)\
Total trajectories: 10\
Total transitions: 18250\
Average returns: 124.52463093894767\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.041 seconds\
computegrad\
done in 0.198 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.29          0\
         1       1.51     0.0792\
         2       3.13      0.223\
         3       1.23      0.665\
         4        2.2        2.4\
         5       2.35       2.76\
         6       6.37       4.11\
         7        1.3        6.1\
         8       3.24       9.51\
         9       3.14       9.84\
        10       5.62       10.4\
done in 0.409 seconds\
Expected: 0.348 Actual: 0.099\
Stepsize OK!\
vf\
done in 0.147 seconds\
sampling\
done in 4.111 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       23.3          0\
         1       2.63    0.00254\
         2       68.2     0.0846\
         3       6.19       0.16\
         4       15.4      0.205\
         5       13.7      0.327\
         6        821       1.32\
         7       9.09       1.82\
         8       78.7       2.92\
         9        109       3.09\
        10       28.3        3.5\
done in 0.051 seconds\
Expected: 0.288 Actual: 0.069\
Stepsize OK!\
vf\
done in 0.072 seconds\
sampling\
done in 3.874 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       12.7          0\
         1       28.6     0.0187\
         2        312       1.02\
         3        261       1.67\
         4        194       1.84\
         5        720       2.73\
         6   1.47e+03       7.18\
         7        570       10.6\
         8        245       20.1\
         9   3.95e+03       39.6\
        10        368       42.1\
done in 0.052 seconds\
Expected: 1.532 Actual: 0.043\
Stepsize OK!\
vf\
done in 0.068 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.69675 |       0.66828 |       0.58013 |      -0.00058 |       0.62793 |       0.59863\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 13.6        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.46648    |\
| explained_variance_t... | -0.202      |\
| meankl                  | 0.011560372 |\
| optimgain               | 0.04250685  |\
| reference_Q_mean        | 1.88        |\
| reference_Q_std         | 2.36        |\
| reference_action_mean   | -0.0239     |\
| reference_action_std    | 0.96        |\
| reference_actor_Q_mean  | 2.55        |\
| reference_actor_Q_std   | 2.32        |\
| rollout/Q_mean          | 1.37        |\
| rollout/actions_mean    | -0.0355     |\
| rollout/actions_std     | 0.807       |\
| rollout/episode_steps   | 1.82e+03    |\
| rollout/episodes        | 5           |\
| rollout/return          | 121         |\
| rollout/return_history  | 121         |\
| surrgain                | 0.04250685  |\
| total/duration          | 61.9        |\
| total/episodes          | 5           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 161         |\
| train/loss_actor        | -3.14       |\
| train/loss_critic       | 1.09        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.0457655628522238  minutes\
======GAIL Validation from:  20160405 to  20160705\
GAIL Sharpe Ratio:  -0.027586475064305557\
======Trading from:  20160705 to  20161003\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x14041a588>\
previous_total_asset:1091569.7783425373\
end_total_asset:1120456.078924056\
total_reward:28886.30058151856\
total_cost:  6114.6256643588185\
total trades:  1569\
Sharpe:  0.1429966699933243\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20160705\
======A2C Training========\
Training time (A2C):  1.7833893855412801  minutes\
======A2C Validation from:  20160705 to  20161003\
A2C Sharpe Ratio:  0.046967325321216974\
======PPO Training========\
Training time (PPO):  6.054240218798319  minutes\
======PPO Validation from:  20160705 to  20161003\
PPO Sharpe Ratio:  0.14099882130488467\
======DDPG Training========\
Training time (DDPG):  1.047289748986562  minutes\
======DDPG Validation from:  20160705 to  20161003\
DDPG Sharpe Ratio:  0.15271015589978645\
======TD3 Training========\
Training time (TD3):  1.1996725479761758  minutes\
======TD3 Validation from:  20160705 to  20161003\
TD3 Sharpe Ratio:  0.10405694373293804\
======GAIL Training========\
actions (18880, 30)\
obs (18880, 181)\
rewards (18880,)\
episode_returns (10,)\
episode_starts (18880,)\
actions (18880, 30)\
obs (18880, 181)\
rewards (18880,)\
episode_returns (10,)\
episode_starts (18880,)\
Total trajectories: 10\
Total transitions: 18880\
Average returns: 61.55574813997373\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.030 seconds\
computegrad\
done in 0.195 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.26          0\
         1      0.535     0.0767\
         2      0.684      0.451\
         3       1.82       1.36\
         4      0.596       1.61\
         5       1.59       2.71\
         6      0.454       2.98\
         7      0.947          4\
         8      0.379       5.15\
         9      0.437       5.27\
        10      0.579       5.97\
done in 0.393 seconds\
Expected: 0.228 Actual: 0.095\
violated KL constraint. shrinking step.\
Expected: 0.228 Actual: 0.060\
Stepsize OK!\
vf\
done in 0.146 seconds\
sampling\
done in 4.071 seconds\
computegrad\
done in 0.008 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       40.1          0\
         1   2.59e+03       5.71\
         2        869        7.7\
         3   3.08e+03       9.27\
         4        273       15.9\
         5         33       16.3\
         6       5.85       16.4\
         7       19.8       16.5\
         8        3.4       16.8\
         9       21.6       16.8\
        10       42.1       16.9\
done in 0.050 seconds\
Expected: 1.453 Actual: 0.007\
Stepsize OK!\
vf\
done in 0.065 seconds\
sampling\
done in 3.910 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.59          0\
         1       12.1      0.059\
         2       2.28     0.0754\
         3       2.34      0.117\
         4       18.3      0.327\
         5       10.3      0.741\
         6       7.13      0.857\
         7       2.23      0.927\
         8       31.7      0.984\
         9       19.8       1.58\
        10       4.49       3.25\
done in 0.050 seconds\
Expected: 0.218 Actual: 0.087\
Stepsize OK!\
vf\
done in 0.065 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.61857 |       0.68379 |       0.64912 |      -0.00065 |       0.66406 |       0.56836\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 13.5        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.604008   |\
| explained_variance_t... | 0.0792      |\
| meankl                  | 0.010079371 |\
| optimgain               | 0.087381236 |\
| reference_Q_mean        | 2.89        |\
| reference_Q_std         | 2.87        |\
| reference_action_mean   | -0.0798     |\
| reference_action_std    | 0.965       |\
| reference_actor_Q_mean  | 3.48        |\
| reference_actor_Q_std   | 2.92        |\
| rollout/Q_mean          | 1.98        |\
| rollout/actions_mean    | -0.0417     |\
| rollout/actions_std     | 0.807       |\
| rollout/episode_steps   | 1.89e+03    |\
| rollout/episodes        | 5           |\
| rollout/return          | 125         |\
| rollout/return_history  | 125         |\
| surrgain                | 0.087381236 |\
| total/duration          | 61.9        |\
| total/episodes          | 5           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 161         |\
| train/loss_actor        | -3.81       |\
| train/loss_critic       | 1.23        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.028409218788147  minutes\
======GAIL Validation from:  20160705 to  20161003\
GAIL Sharpe Ratio:  -0.08685009412555739\
======Trading from:  20161003 to  20170103\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x1417a87b8>\
previous_total_asset:1120456.078924056\
end_total_asset:1188889.9810928975\
total_reward:68433.90216884157\
total_cost:  1900.9664219840154\
total trades:  1005\
Sharpe:  0.31904214008656995\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20161003\
======A2C Training========\
Training time (A2C):  1.798216418425242  minutes\
======A2C Validation from:  20161003 to  20170103\
A2C Sharpe Ratio:  0.7111778184091405\
======PPO Training========\
Training time (PPO):  6.197301435470581  minutes\
======PPO Validation from:  20161003 to  20170103\
PPO Sharpe Ratio:  0.3059196493181287\
======DDPG Training========\
Training time (DDPG):  1.07432861328125  minutes\
======DDPG Validation from:  20161003 to  20170103\
DDPG Sharpe Ratio:  0.3464225965966902\
======TD3 Training========\
Training time (TD3):  1.1976369857788085  minutes\
======TD3 Validation from:  20161003 to  20170103\
TD3 Sharpe Ratio:  0.5141595673343533\
======GAIL Training========\
actions (19510, 30)\
obs (19510, 181)\
rewards (19510,)\
episode_returns (10,)\
episode_starts (19510,)\
actions (19510, 30)\
obs (19510, 181)\
rewards (19510,)\
episode_returns (10,)\
episode_starts (19510,)\
Total trajectories: 10\
Total transitions: 19510\
Average returns: 147.45919174450682\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.092 seconds\
computegrad\
done in 0.205 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.19          0\
         1       5.77   0.000887\
         2       0.53     0.0607\
         3      0.712      0.398\
         4      0.462      0.398\
         5       1.55      0.624\
         6       13.9       1.28\
         7      0.462       1.29\
         8       1.44       1.56\
         9        114       2.09\
        10       1.11       2.14\
done in 0.424 seconds\
Expected: 0.138 Actual: 0.067\
Stepsize OK!\
vf\
done in 0.156 seconds\
sampling\
done in 4.099 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        1.1          0\
         1       16.8     0.0193\
         2        5.5     0.0396\
         3      0.783     0.0678\
         4       4.43      0.134\
         5         25      0.267\
         6       1.84      0.464\
         7       22.1      0.534\
         8       1.11      0.564\
         9       7.72      0.741\
        10       8.34       1.56\
done in 0.053 seconds\
Expected: 0.135 Actual: 0.078\
Stepsize OK!\
vf\
done in 0.076 seconds\
sampling\
done in 4.192 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.56          0\
         1       55.6      0.172\
         2       19.4      0.285\
         3       27.3      0.372\
         4       40.9       5.76\
         5        283       6.16\
         6        651       13.4\
         7        298       21.3\
         8       58.8       33.9\
         9   2.61e+03       63.4\
        10        523       70.8\
done in 0.050 seconds\
Expected: 1.481 Actual: 0.041\
violated KL constraint. shrinking step.\
Expected: 1.481 Actual: 0.027\
Stepsize OK!\
vf\
done in 0.077 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.75148 |       0.65945 |       0.65668 |      -0.00066 |       0.48438 |       0.61230\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 14          |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.58804    |\
| explained_variance_t... | 0.00855     |\
| meankl                  | 0.007539604 |\
| optimgain               | 0.027252108 |\
| reference_Q_mean        | 3.65        |\
| reference_Q_std         | 3.51        |\
| reference_action_mean   | 0.0538      |\
| reference_action_std    | 0.958       |\
| reference_actor_Q_mean  | 4.43        |\
| reference_actor_Q_std   | 3.55        |\
| rollout/Q_mean          | 2.28        |\
| rollout/actions_mean    | 0.0676      |\
| rollout/actions_std     | 0.798       |\
| rollout/episode_steps   | 1.95e+03    |\
| rollout/episodes        | 5           |\
| rollout/return          | 219         |\
| rollout/return_history  | 219         |\
| surrgain                | 0.027252108 |\
| total/duration          | 63.6        |\
| total/episodes          | 5           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 157         |\
| train/loss_actor        | -4.3        |\
| train/loss_critic       | 2.04        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.1137403845787048  minutes\
======GAIL Validation from:  20161003 to  20170103\
GAIL Sharpe Ratio:  0.6532895364870723\
======Trading from:  20170103 to  20170404\
Used Model:  <stable_baselines.a2c.a2c.A2C object at 0x1406d1828>\
previous_total_asset:1188889.9810928975\
end_total_asset:1214262.409790667\
total_reward:25372.42869776953\
total_cost:  4960.563781869036\
total trades:  1557\
Sharpe:  0.12394756106074374\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20170103\
======A2C Training========\
Training time (A2C):  1.789423151810964  minutes\
======A2C Validation from:  20170103 to  20170404\
A2C Sharpe Ratio:  0.27680588285157676\
======PPO Training========\
Training time (PPO):  6.2231506188710535  minutes\
======PPO Validation from:  20170103 to  20170404\
PPO Sharpe Ratio:  0.1296253617267595\
======DDPG Training========\
Training time (DDPG):  1.0609517653783163  minutes\
======DDPG Validation from:  20170103 to  20170404\
DDPG Sharpe Ratio:  0.12000494775620536\
======TD3 Training========\
Training time (TD3):  1.2035683512687683  minutes\
======TD3 Validation from:  20170103 to  20170404\
TD3 Sharpe Ratio:  0.3172641958933023\
======GAIL Training========\
actions (20140, 30)\
obs (20140, 181)\
rewards (20140,)\
episode_returns (10,)\
episode_starts (20140,)\
actions (20140, 30)\
obs (20140, 181)\
rewards (20140,)\
episode_returns (10,)\
episode_starts (20140,)\
Total trajectories: 10\
Total transitions: 20140\
Average returns: 189.78260033176048\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.147 seconds\
computegrad\
done in 0.211 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0         14          0\
         1       1.35    0.00309\
         2       0.73     0.0993\
         3       5.55      0.199\
         4       4.73      0.338\
         5       0.89      0.345\
         6       1.32      0.439\
         7      0.862       1.02\
         8       1.39       1.12\
         9      0.789       1.13\
        10        2.4       1.95\
done in 0.465 seconds\
Expected: 0.146 Actual: 0.092\
Stepsize OK!\
vf\
done in 0.159 seconds\
sampling\
done in 4.347 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       6.14          0\
         1       2.22    0.00492\
         2       9.03     0.0961\
         3       2.46      0.128\
         4       2.92      0.159\
         5       9.98      0.393\
         6       7.43      0.652\
         7         18       1.06\
         8       8.99       1.12\
         9       9.93       1.41\
        10       15.8       2.25\
done in 0.052 seconds\
Expected: 0.193 Actual: 0.074\
Stepsize OK!\
vf\
done in 0.073 seconds\
sampling\
done in 3.939 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       53.8          0\
         1   1.54e+04       3.44\
         2   2.35e+04       16.4\
         3   1.12e+04       17.1\
         4   1.53e+04       26.6\
         5   7.93e+03       46.3\
         6   9.09e+04        237\
         7   2.27e+04        239\
         8   1.37e+04        274\
         9    1.2e+05        457\
        10   7.26e+04        495\
done in 0.053 seconds\
Expected: 8.478 Actual: 0.030\
Stepsize OK!\
vf\
done in 0.073 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.77193 |       0.59204 |       0.64799 |      -0.00065 |       0.45215 |       0.67285\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 14.1        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.500805   |\
| explained_variance_t... | 0.0467      |\
| meankl                  | 0.009455557 |\
| optimgain               | 0.02964916  |\
| reference_Q_mean        | 4.38        |\
| reference_Q_std         | 2.93        |\
| reference_action_mean   | -0.0121     |\
| reference_action_std    | 0.956       |\
| reference_actor_Q_mean  | 5.42        |\
| reference_actor_Q_std   | 3.13        |\
| rollout/Q_mean          | 2.76        |\
| rollout/actions_mean    | -0.0124     |\
| rollout/actions_std     | 0.808       |\
| rollout/episode_steps   | 2.01e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 201         |\
| rollout/return_history  | 201         |\
| surrgain                | 0.02964916  |\
| total/duration          | 62.8        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 159         |\
| train/loss_actor        | -5.41       |\
| train/loss_critic       | 1.8         |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.161583666006724  minutes\
======GAIL Validation from:  20170103 to  20170404\
GAIL Sharpe Ratio:  0.25156102026683663\
======Trading from:  20170404 to  20170705\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x141f155c0>\
previous_total_asset:1214262.409790667\
end_total_asset:1231030.9256125006\
total_reward:16768.51582183363\
total_cost:  3139.6529373354124\
total trades:  971\
Sharpe:  0.08724913624109705\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20170404\
======A2C Training========\
Training time (A2C):  1.811866549650828  minutes\
======A2C Validation from:  20170404 to  20170705\
A2C Sharpe Ratio:  0.18772620866314874\
======PPO Training========\
Training time (PPO):  6.317903602123261  minutes\
======PPO Validation from:  20170404 to  20170705\
PPO Sharpe Ratio:  0.2363670912397959\
======DDPG Training========\
Training time (DDPG):  1.0719499985376995  minutes\
======DDPG Validation from:  20170404 to  20170705\
DDPG Sharpe Ratio:  0.027738145924817794\
======TD3 Training========\
Training time (TD3):  1.239368697007497  minutes\
======TD3 Validation from:  20170404 to  20170705\
TD3 Sharpe Ratio:  0.2845508543310296\
======GAIL Training========\
actions (20770, 30)\
obs (20770, 181)\
rewards (20770,)\
episode_returns (10,)\
episode_starts (20770,)\
actions (20770, 30)\
obs (20770, 181)\
rewards (20770,)\
episode_returns (10,)\
episode_starts (20770,)\
Total trajectories: 10\
Total transitions: 20770\
Average returns: 209.4544570708531\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.130 seconds\
computegrad\
done in 0.197 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.21          0\
         1      0.828     0.0823\
         2      0.998      0.582\
         3       1.43      0.665\
         4       3.15       0.89\
         5       1.18       1.93\
         6       2.79        2.4\
         7       3.18       3.62\
         8      0.643       4.65\
         9       2.03       9.58\
        10        3.3       9.87\
done in 0.418 seconds\
Expected: 0.303 Actual: 0.110\
violated KL constraint. shrinking step.\
Expected: 0.303 Actual: 0.059\
Stepsize OK!\
vf\
done in 0.148 seconds\
sampling\
done in 3.910 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.72          0\
         1       28.7      0.512\
         2        141       1.18\
         3        194       2.58\
         4        171       6.25\
         5       51.7       10.3\
         6        370       21.4\
         7       46.8       30.9\
         8        115       34.2\
         9        668       36.6\
        10        304         52\
done in 0.051 seconds\
Expected: 1.339 Actual: 0.059\
Stepsize OK!\
vf\
done in 0.066 seconds\
sampling\
done in 4.151 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       8.16          0\
         1       84.5      0.283\
         2        237      0.877\
         3        138        2.1\
         4        672       4.71\
         5        195       15.3\
         6    1.5e+03       21.7\
         7        201       98.8\
         8   1.47e+03        108\
         9   1.06e+03        112\
        10   1.37e+03        121\
done in 0.052 seconds\
Expected: 2.496 Actual: 0.078\
Stepsize OK!\
vf\
done in 0.068 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.82288 |       0.86585 |       0.65815 |      -0.00066 |       0.35449 |       0.29395\
-----------------------------------------\
| EpLenMean               | 2.08e+03    |\
| EpRewMean               | 637.56604   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 135         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 13.7        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.556812   |\
| explained_variance_t... | -0.276      |\
| meankl                  | 0.007061011 |\
| optimgain               | 0.07782584  |\
| reference_Q_mean        | 2.75        |\
| reference_Q_std         | 2.62        |\
| reference_action_mean   | 0.00366     |\
| reference_action_std    | 0.966       |\
| reference_actor_Q_mean  | 3.31        |\
| reference_actor_Q_std   | 2.7         |\
| rollout/Q_mean          | 1.62        |\
| rollout/actions_mean    | -0.0409     |\
| rollout/actions_std     | 0.81        |\
| rollout/episode_steps   | 2.08e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 90.2        |\
| rollout/return_history  | 90.2        |\
| surrgain                | 0.07782584  |\
| total/duration          | 63.4        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 158         |\
| train/loss_actor        | -3.05       |\
| train/loss_critic       | 0.91        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.1779411673545837  minutes\
======GAIL Validation from:  20170404 to  20170705\
GAIL Sharpe Ratio:  0.21568332760877099\
======Trading from:  20170705 to  20171003\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x1425413c8>\
previous_total_asset:1231030.9256125006\
end_total_asset:1265293.780734853\
total_reward:34262.85512235225\
total_cost:  2735.4251380694886\
total trades:  972\
Sharpe:  0.193987310650145\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20170705\
======A2C Training========\
Training time (A2C):  1.8086437145868937  minutes\
======A2C Validation from:  20170705 to  20171003\
A2C Sharpe Ratio:  0.12436027150566135\
======PPO Training========\
Training time (PPO):  6.292840262254079  minutes\
======PPO Validation from:  20170705 to  20171003\
PPO Sharpe Ratio:  0.21511700254853028\
======DDPG Training========\
Training time (DDPG):  1.078119985262553  minutes\
======DDPG Validation from:  20170705 to  20171003\
DDPG Sharpe Ratio:  0.3469516476397748\
======TD3 Training========\
Training time (TD3):  1.2063966671625772  minutes\
======TD3 Validation from:  20170705 to  20171003\
TD3 Sharpe Ratio:  0.19173063511740374\
======GAIL Training========\
actions (21400, 30)\
obs (21400, 181)\
rewards (21400,)\
episode_returns (10,)\
episode_starts (21400,)\
actions (21400, 30)\
obs (21400, 181)\
rewards (21400,)\
episode_returns (10,)\
episode_starts (21400,)\
Total trajectories: 10\
Total transitions: 21400\
Average returns: 222.55515374871902\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.189 seconds\
computegrad\
done in 0.222 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.11          0\
         1      0.706     0.0795\
         2       3.89      0.255\
         3      0.529      0.491\
         4       5.16      0.679\
         5       3.19       1.19\
         6      0.683       1.71\
         7       3.77       2.68\
         8       1.12       3.64\
         9      0.762        4.9\
        10       3.99       5.48\
done in 0.425 seconds\
Expected: 0.224 Actual: 0.095\
Stepsize OK!\
vf\
done in 0.149 seconds\
sampling\
done in 3.996 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        1.5          0\
         1       9.26     0.0615\
         2       2.45     0.0905\
         3       1.56      0.104\
         4       3.12      0.119\
         5         19      0.237\
         6       21.6      0.558\
         7       8.43      0.706\
         8       2.72      0.878\
         9       11.3       1.57\
        10       11.2       2.46\
done in 0.050 seconds\
Expected: 0.182 Actual: 0.064\
Stepsize OK!\
vf\
done in 0.065 seconds\
sampling\
done in 4.182 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.59          0\
         1       8.57     0.0505\
         2       3.12     0.0999\
         3       1.97      0.113\
         4       30.4      0.799\
         5       6.19       1.22\
         6       14.5        1.4\
         7       16.4       1.73\
         8       70.1       2.89\
         9       13.8        4.4\
        10       69.6       4.89\
done in 0.052 seconds\
Expected: 0.275 Actual: 0.055\
Stepsize OK!\
vf\
done in 0.071 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.59898 |       0.55537 |       0.65104 |      -0.00065 |       0.71875 |       0.76758\
-----------------------------------------\
| EpLenMean               | 2.14e+03    |\
| EpRewMean               | 2019.2969   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 124         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14          |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.52769    |\
| explained_variance_t... | 0.00208     |\
| meankl                  | 0.01237537  |\
| optimgain               | 0.054740325 |\
| reference_Q_mean        | 4.48        |\
| reference_Q_std         | 3.42        |\
| reference_action_mean   | 0.209       |\
| reference_action_std    | 0.95        |\
| reference_actor_Q_mean  | 5.32        |\
| reference_actor_Q_std   | 3.52        |\
| rollout/Q_mean          | 3.12        |\
| rollout/actions_mean    | 0.0974      |\
| rollout/actions_std     | 0.818       |\
| rollout/episode_steps   | 2.14e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 156         |\
| rollout/return_history  | 156         |\
| surrgain                | 0.054740325 |\
| total/duration          | 63.8        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 157         |\
| train/loss_actor        | -5.3        |\
| train/loss_critic       | 1.36        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.2220939675966898  minutes\
======GAIL Validation from:  20170705 to  20171003\
GAIL Sharpe Ratio:  0.3113514522654922\
======Trading from:  20171003 to  20180103\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x141acdeb8>\
previous_total_asset:1265293.780734853\
end_total_asset:1353927.4252810944\
total_reward:88633.6445462415\
total_cost:  1144.6303253732035\
total trades:  1162\
Sharpe:  0.4608332022697609\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20171003\
======A2C Training========\
Training time (A2C):  1.79878675142924  minutes\
======A2C Validation from:  20171003 to  20180103\
A2C Sharpe Ratio:  0.4105794926449285\
======PPO Training========\
Training time (PPO):  6.327128295103709  minutes\
======PPO Validation from:  20171003 to  20180103\
PPO Sharpe Ratio:  0.3676488711985721\
======DDPG Training========\
Training time (DDPG):  1.1044566829999287  minutes\
======DDPG Validation from:  20171003 to  20180103\
DDPG Sharpe Ratio:  0.4482402194458056\
======TD3 Training========\
Training time (TD3):  1.2078511635462443  minutes\
======TD3 Validation from:  20171003 to  20180103\
TD3 Sharpe Ratio:  0.7467446055605166\
======GAIL Training========\
actions (22030, 30)\
obs (22030, 181)\
rewards (22030,)\
episode_returns (10,)\
episode_starts (22030,)\
actions (22030, 30)\
obs (22030, 181)\
rewards (22030,)\
episode_returns (10,)\
episode_starts (22030,)\
Total trajectories: 10\
Total transitions: 22030\
Average returns: 137.87239418808895\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.425 seconds\
computegrad\
done in 0.213 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.31          0\
         1      0.582     0.0716\
         2      0.647      0.563\
         3       1.17       2.01\
         4       3.36       2.66\
         5       4.12       3.79\
         6       1.38       5.46\
         7       2.77       6.41\
         8       1.57       10.2\
         9       3.99         11\
        10       1.22       12.4\
done in 0.401 seconds\
Expected: 0.336 Actual: 0.064\
violated KL constraint. shrinking step.\
Expected: 0.336 Actual: 0.040\
Stepsize OK!\
vf\
done in 0.146 seconds\
sampling\
done in 3.970 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.64          0\
         1       13.3      0.225\
         2       13.6      0.387\
         3        116       1.37\
         4       37.6       5.65\
         5        190       16.5\
         6        117       19.3\
         7       73.7       24.3\
         8       66.8       72.5\
         9       47.1       76.6\
        10   1.16e+03       90.9\
done in 0.050 seconds\
Expected: 1.628 Actual: 0.044\
Stepsize OK!\
vf\
done in 0.074 seconds\
sampling\
done in 4.206 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.17          0\
         1        7.8   0.000677\
         2       33.6     0.0358\
         3       4.24      0.152\
         4       10.4      0.202\
         5        140      0.216\
         6       18.6      0.344\
         7       5.24       1.22\
         8        147       1.27\
         9       8.66       1.29\
        10       5.46       1.31\
done in 0.050 seconds\
Expected: 0.167 Actual: 0.056\
Stepsize OK!\
vf\
done in 0.070 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.73239 |       0.71708 |       0.64843 |      -0.00065 |       0.53027 |       0.51953\
-----------------------------------------\
| EpLenMean               | 2.2e+03     |\
| EpRewMean               | 2724.0051   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 165         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.2        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.558548   |\
| explained_variance_t... | -0.00332    |\
| meankl                  | 0.007998599 |\
| optimgain               | 0.056270085 |\
| reference_Q_mean        | 3.25        |\
| reference_Q_std         | 2.34        |\
| reference_action_mean   | -0.326      |\
| reference_action_std    | 0.902       |\
| reference_actor_Q_mean  | 4.33        |\
| reference_actor_Q_std   | 2.59        |\
| rollout/Q_mean          | 1.71        |\
| rollout/actions_mean    | -0.132      |\
| rollout/actions_std     | 0.811       |\
| rollout/episode_steps   | 2.2e+03     |\
| rollout/episodes        | 4           |\
| rollout/return          | 168         |\
| rollout/return_history  | 168         |\
| surrgain                | 0.056270085 |\
| total/duration          | 65.1        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 154         |\
| train/loss_actor        | -3.97       |\
| train/loss_critic       | 1.55        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.2367797017097473  minutes\
======GAIL Validation from:  20171003 to  20180103\
GAIL Sharpe Ratio:  0.6279862417217359\
======Trading from:  20180103 to  20180405\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x149f6fc50>\
previous_total_asset:1353927.4252810944\
end_total_asset:1385694.993950413\
total_reward:31767.568669318687\
total_cost:  1701.8941154425445\
total trades:  218\
Sharpe:  0.13908450275509368\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20180103\
======A2C Training========\
Training time (A2C):  1.8143616795539856  minutes\
======A2C Validation from:  20180103 to  20180405\
A2C Sharpe Ratio:  0.04062671720324751\
======PPO Training========\
Training time (PPO):  6.31417738199234  minutes\
======PPO Validation from:  20180103 to  20180405\
PPO Sharpe Ratio:  0.08440992810634382\
======DDPG Training========\
Training time (DDPG):  1.0748831550280253  minutes\
======DDPG Validation from:  20180103 to  20180405\
DDPG Sharpe Ratio:  -0.06083489058495059\
======TD3 Training========\
Training time (TD3):  1.2251694361368815  minutes\
======TD3 Validation from:  20180103 to  20180405\
TD3 Sharpe Ratio:  -0.0584756086506302\
======GAIL Training========\
actions (22660, 30)\
obs (22660, 181)\
rewards (22660,)\
episode_returns (10,)\
episode_starts (22660,)\
actions (22660, 30)\
obs (22660, 181)\
rewards (22660,)\
episode_returns (10,)\
episode_starts (22660,)\
Total trajectories: 10\
Total transitions: 22660\
Average returns: 108.43042282469105\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.550 seconds\
computegrad\
done in 0.224 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.19          0\
         1      0.431      0.066\
         2      0.502      0.459\
         3      0.386       1.08\
         4       1.94       1.45\
         5      0.393       2.05\
         6      0.305       2.38\
         7      0.208       3.61\
         8      0.622       3.84\
         9      0.198       4.05\
        10      0.474       4.96\
done in 0.450 seconds\
Expected: 0.193 Actual: 0.112\
violated KL constraint. shrinking step.\
Expected: 0.193 Actual: 0.064\
Stepsize OK!\
vf\
done in 0.154 seconds\
sampling\
done in 4.369 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.52          0\
         1       1.07     0.0826\
         2       11.4      0.255\
         3       1.98       1.12\
         4       14.8        2.5\
         5       2.81       4.38\
         6         11       6.41\
         7       6.66       7.68\
         8       8.13       7.94\
         9       2.33       10.1\
        10       22.6       14.1\
done in 0.053 seconds\
Expected: 0.429 Actual: 0.095\
Stepsize OK!\
vf\
done in 0.073 seconds\
sampling\
done in 4.324 seconds\
computegrad\
done in 0.011 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.05          0\
         1       56.5    0.00062\
         2       35.8     0.0875\
         3       14.8      0.169\
         4    1.3e+03      0.202\
         5       8.89      0.228\
         6       29.4      0.343\
         7   1.63e+03      0.585\
         8       91.1       0.61\
         9       71.1       1.14\
        10       55.7       1.54\
done in 0.051 seconds\
Expected: 0.207 Actual: 0.070\
Stepsize OK!\
vf\
done in 0.070 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.76300 |       0.88379 |       0.65302 |      -0.00065 |       0.47656 |       0.27344\
-----------------------------------------\
| EpLenMean               | 2.27e+03    |\
| EpRewMean               | 2106.0833   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 145         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.9        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.555355   |\
| explained_variance_t... | -0.177      |\
| meankl                  | 0.009396179 |\
| optimgain               | 0.07028465  |\
| reference_Q_mean        | 3.24        |\
| reference_Q_std         | 2.68        |\
| reference_action_mean   | 0.0881      |\
| reference_action_std    | 0.967       |\
| reference_actor_Q_mean  | 4.13        |\
| reference_actor_Q_std   | 2.89        |\
| rollout/Q_mean          | 2.28        |\
| rollout/actions_mean    | 0.0884      |\
| rollout/actions_std     | 0.809       |\
| rollout/episode_steps   | 2.27e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 237         |\
| rollout/return_history  | 237         |\
| surrgain                | 0.07028465  |\
| total/duration          | 63.6        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 157         |\
| train/loss_actor        | -4.27       |\
| train/loss_critic       | 1.78        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.2689217487970987  minutes\
======GAIL Validation from:  20180103 to  20180405\
GAIL Sharpe Ratio:  -0.06328651326759639\
======Trading from:  20180405 to  20180705\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x1419f4240>\
previous_total_asset:1385694.993950413\
end_total_asset:1398420.4278672477\
total_reward:12725.433916834649\
total_cost:  6640.694971666121\
total trades:  1074\
Sharpe:  0.06254167107449837\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20180405\
======A2C Training========\
Training time (A2C):  1.8033272663752238  minutes\
======A2C Validation from:  20180405 to  20180705\
A2C Sharpe Ratio:  -0.1687069997149775\
======PPO Training========\
Training time (PPO):  6.2709500352541605  minutes\
======PPO Validation from:  20180405 to  20180705\
PPO Sharpe Ratio:  -0.06308562212720553\
======DDPG Training========\
Training time (DDPG):  1.0760516325632732  minutes\
======DDPG Validation from:  20180405 to  20180705\
DDPG Sharpe Ratio:  -0.07544967226321689\
======TD3 Training========\
Training time (TD3):  1.215260128180186  minutes\
======TD3 Validation from:  20180405 to  20180705\
TD3 Sharpe Ratio:  0.06130567937144843\
======GAIL Training========\
actions (23290, 30)\
obs (23290, 181)\
rewards (23290,)\
episode_returns (10,)\
episode_starts (23290,)\
actions (23290, 30)\
obs (23290, 181)\
rewards (23290,)\
episode_returns (10,)\
episode_starts (23290,)\
Total trajectories: 10\
Total transitions: 23290\
Average returns: 305.9003030308522\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.193 seconds\
computegrad\
done in 0.223 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.05          0\
         1       3.53     0.0596\
         2      0.791     0.0747\
         3      0.585      0.537\
         4       1.25        1.3\
         5       1.63        1.8\
         6       8.66       1.89\
         7      0.555       2.06\
         8      0.801       2.71\
         9      0.608       3.18\
        10      0.867       3.36\
done in 0.443 seconds\
Expected: 0.184 Actual: 0.126\
Stepsize OK!\
vf\
done in 0.162 seconds\
sampling\
done in 4.041 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       7.32          0\
         1       48.8     0.0018\
         2        128      0.335\
         3         93        1.1\
         4   2.04e+03       1.57\
         5        132       1.66\
         6        391       6.78\
         7   1.28e+03       11.8\
         8    2.4e+03       11.9\
         9        363       20.2\
        10        254       26.4\
done in 0.053 seconds\
Expected: 1.110 Actual: 0.052\
Stepsize OK!\
vf\
done in 0.072 seconds\
sampling\
done in 4.283 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       13.6          0\
         1       6.58   1.23e-05\
         2   4.77e+03       0.15\
         3       41.1      0.406\
         4        193      0.406\
         5       39.4      0.595\
         6        621      0.833\
         7        291      0.834\
         8        371      0.847\
         9        394       2.23\
        10   3.98e+05       2.77\
done in 0.052 seconds\
Expected: 0.328 Actual: 0.059\
Stepsize OK!\
vf\
done in 0.073 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.86265 |       0.99701 |       0.62763 |      -0.00063 |       0.37988 |       0.25391\
-----------------------------------------\
| EpLenMean               | 2.33e+03    |\
| EpRewMean               | 1716.9806   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 174         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.2        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.573254   |\
| explained_variance_t... | -0.00789    |\
| meankl                  | 0.005967076 |\
| optimgain               | 0.058996584 |\
| reference_Q_mean        | 3.09        |\
| reference_Q_std         | 2.68        |\
| reference_action_mean   | -0.0651     |\
| reference_action_std    | 0.968       |\
| reference_actor_Q_mean  | 3.83        |\
| reference_actor_Q_std   | 2.71        |\
| rollout/Q_mean          | 2.11        |\
| rollout/actions_mean    | -0.0651     |\
| rollout/actions_std     | 0.82        |\
| rollout/episode_steps   | 2.33e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 252         |\
| rollout/return_history  | 252         |\
| surrgain                | 0.058996584 |\
| total/duration          | 63.7        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 157         |\
| train/loss_actor        | -4.09       |\
| train/loss_critic       | 1.97        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.3393223365147908  minutes\
======GAIL Validation from:  20180405 to  20180705\
GAIL Sharpe Ratio:  -0.24020652658730668\
======Trading from:  20180705 to  20181003\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x146f42d30>\
previous_total_asset:1398420.4278672477\
end_total_asset:1411789.4071874395\
total_reward:13368.979320191778\
total_cost:  5835.676937288441\
total trades:  669\
Sharpe:  0.1249472699936058\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20180705\
======A2C Training========\
Training time (A2C):  1.8503706852595012  minutes\
======A2C Validation from:  20180705 to  20181003\
A2C Sharpe Ratio:  0.08571324029129991\
======PPO Training========\
Training time (PPO):  6.440763449668884  minutes\
======PPO Validation from:  20180705 to  20181003\
PPO Sharpe Ratio:  0.07093225799364683\
======DDPG Training========\
Training time (DDPG):  1.081152049700419  minutes\
======DDPG Validation from:  20180705 to  20181003\
DDPG Sharpe Ratio:  0.30264104421442384\
======TD3 Training========\
Training time (TD3):  1.2345688025156656  minutes\
======TD3 Validation from:  20180705 to  20181003\
TD3 Sharpe Ratio:  0.23461284161621296\
======GAIL Training========\
actions (23920, 30)\
obs (23920, 181)\
rewards (23920,)\
episode_returns (10,)\
episode_starts (23920,)\
actions (23920, 30)\
obs (23920, 181)\
rewards (23920,)\
episode_returns (10,)\
episode_starts (23920,)\
Total trajectories: 10\
Total transitions: 23920\
Average returns: 118.21080660734151\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.216 seconds\
computegrad\
done in 0.221 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.09          0\
         1      0.742     0.0844\
         2       4.02      0.221\
         3      0.504      0.518\
         4       4.88      0.782\
         5      0.598       1.12\
         6       1.07       1.31\
         7      0.446       2.72\
         8      0.509       3.31\
         9       1.26       3.99\
        10        0.9       4.22\
done in 0.407 seconds\
Expected: 0.195 Actual: 0.176\
Stepsize OK!\
vf\
done in 0.154 seconds\
sampling\
done in 4.007 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       7.65          0\
         1        174    0.00289\
         2       80.6     0.0116\
         3        240       0.24\
         4       95.6      0.752\
         5   5.23e+03       1.14\
         6        117       1.22\
         7        458       3.45\
         8   2.92e+03       4.26\
         9   5.03e+03       5.23\
        10        639       5.31\
done in 0.050 seconds\
Expected: 0.511 Actual: 0.054\
Stepsize OK!\
vf\
done in 0.069 seconds\
sampling\
done in 4.281 seconds\
computegrad\
done in 0.008 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.75          0\
         1         64     0.0413\
         2       46.9      0.113\
         3       7.79       0.18\
         4       9.91      0.281\
         5       28.2      0.663\
         6       17.5       2.14\
         7       27.7       2.16\
         8       84.1       3.71\
         9        444       4.62\
        10       10.2       6.95\
done in 0.051 seconds\
Expected: 0.410 Actual: 0.076\
Stepsize OK!\
vf\
done in 0.067 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.60308 |       0.59901 |       0.64826 |      -0.00065 |       0.66113 |       0.72168\
-----------------------------------------\
| EpLenMean               | 2.39e+03    |\
| EpRewMean               | 870.92993   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 150         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.1        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.538918   |\
| explained_variance_t... | -0.282      |\
| meankl                  | 0.013742783 |\
| optimgain               | 0.075532526 |\
| reference_Q_mean        | 3.64        |\
| reference_Q_std         | 3.09        |\
| reference_action_mean   | 0.00391     |\
| reference_action_std    | 0.97        |\
| reference_actor_Q_mean  | 4.53        |\
| reference_actor_Q_std   | 3.16        |\
| rollout/Q_mean          | 2.61        |\
| rollout/actions_mean    | 0.0219      |\
| rollout/actions_std     | 0.816       |\
| rollout/episode_steps   | 2.39e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 224         |\
| rollout/return_history  | 224         |\
| surrgain                | 0.075532526 |\
| total/duration          | 64          |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 156         |\
| train/loss_actor        | -4.77       |\
| train/loss_critic       | 1.39        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.3626020987828573  minutes\
======GAIL Validation from:  20180705 to  20181003\
GAIL Sharpe Ratio:  0.02223453486005726\
======Trading from:  20181003 to  20190104\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x130ad4d68>\
previous_total_asset:1411789.4071874395\
end_total_asset:1429836.2265758528\
total_reward:18046.819388413336\
total_cost:  977.1290583440299\
total trades:  136\
Sharpe:  0.3543144700601359\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20181003\
======A2C Training========\
Training time (A2C):  1.8194726824760437  minutes\
======A2C Validation from:  20181003 to  20190104\
A2C Sharpe Ratio:  -0.3910969439821172\
======PPO Training========\
Training time (PPO):  6.4496039867401125  minutes\
======PPO Validation from:  20181003 to  20190104\
PPO Sharpe Ratio:  -0.34564116979509085\
======DDPG Training========\
Training time (DDPG):  1.09134281873703  minutes\
======DDPG Validation from:  20181003 to  20190104\
DDPG Sharpe Ratio:  -0.39165093453284383\
======TD3 Training========\
Training time (TD3):  1.2096796035766602  minutes\
======TD3 Validation from:  20181003 to  20190104\
TD3 Sharpe Ratio:  -0.37099090624731323\
======GAIL Training========\
actions (24550, 30)\
obs (24550, 181)\
rewards (24550,)\
episode_returns (10,)\
episode_starts (24550,)\
actions (24550, 30)\
obs (24550, 181)\
rewards (24550,)\
episode_returns (10,)\
episode_starts (24550,)\
Total trajectories: 10\
Total transitions: 24550\
Average returns: 94.6644137101539\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.295 seconds\
computegrad\
done in 0.215 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.22          0\
         1       7.99     0.0208\
         2       1.17     0.0684\
         3      0.797     0.0732\
         4      0.636      0.546\
         5      0.648       1.17\
         6       15.4       1.32\
         7       1.71       1.42\
         8       63.5       2.16\
         9      0.633       2.92\
        10       1.42       4.06\
done in 0.437 seconds\
Expected: 0.192 Actual: 0.081\
Stepsize OK!\
vf\
done in 0.160 seconds\
sampling\
done in 4.078 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       10.5          0\
         1       81.9        1.3\
         2   1.39e+03       6.15\
         3   1.45e+03       11.1\
         4        155         29\
         5       25.3       40.9\
         6       66.9       43.6\
         7        284       45.9\
         8       26.4       46.4\
         9        8.4       47.5\
        10       19.3       49.1\
done in 0.050 seconds\
Expected: 1.719 Actual: 0.015\
Stepsize OK!\
vf\
done in 0.070 seconds\
sampling\
done in 4.331 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       61.9          0\
         1       53.2     0.0131\
         2   2.37e+04       4.74\
         3   5.45e+03       14.4\
         4   2.16e+03       15.2\
         5   5.16e+03        102\
         6   4.92e+03        127\
         7   1.01e+04        143\
         8   1.37e+04        146\
         9   9.26e+04        158\
        10   1.66e+05        315\
done in 0.052 seconds\
Expected: 5.772 Actual: 0.050\
Stepsize OK!\
vf\
done in 0.073 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.77151 |       0.98225 |       0.63114 |      -0.00063 |       0.48926 |       0.27637\
-----------------------------------------\
| EpLenMean               | 2.46e+03    |\
| EpRewMean               | 1311.3704   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 191         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.4        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.581985   |\
| explained_variance_t... | -0.0785     |\
| meankl                  | 0.00805174  |\
| optimgain               | 0.049845196 |\
| reference_Q_mean        | 3.88        |\
| reference_Q_std         | 3.47        |\
| reference_action_mean   | 0.0755      |\
| reference_action_std    | 0.971       |\
| reference_actor_Q_mean  | 4.57        |\
| reference_actor_Q_std   | 3.55        |\
| rollout/Q_mean          | 2.74        |\
| rollout/actions_mean    | 0.00864     |\
| rollout/actions_std     | 0.797       |\
| rollout/episode_steps   | 2.46e+03    |\
| rollout/episodes        | 4           |\
| rollout/return          | 227         |\
| rollout/return_history  | 227         |\
| surrgain                | 0.049845196 |\
| total/duration          | 64.6        |\
| total/episodes          | 4           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 155         |\
| train/loss_actor        | -4.49       |\
| train/loss_critic       | 2.52        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.3832840323448181  minutes\
======GAIL Validation from:  20181003 to  20190104\
GAIL Sharpe Ratio:  -0.3167323741925642\
======Trading from:  20190104 to  20190405\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x147ee2d30>\
previous_total_asset:1429836.2265758528\
end_total_asset:1494546.7651279117\
total_reward:64710.53855205886\
total_cost:  11522.492216665847\
total trades:  1683\
Sharpe:  0.36548780792790136\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20190104\
======A2C Training========\
Training time (A2C):  1.8249988158543904  minutes\
======A2C Validation from:  20190104 to  20190405\
A2C Sharpe Ratio:  0.013020107933304518\
======PPO Training========\
Training time (PPO):  6.410982112089793  minutes\
======PPO Validation from:  20190104 to  20190405\
PPO Sharpe Ratio:  0.053620247214733574\
======DDPG Training========\
Training time (DDPG):  1.082253134250641  minutes\
======DDPG Validation from:  20190104 to  20190405\
DDPG Sharpe Ratio:  0.030220627220289845\
======TD3 Training========\
Training time (TD3):  1.218923564751943  minutes\
======TD3 Validation from:  20190104 to  20190405\
TD3 Sharpe Ratio:  0.1917693182082573\
======GAIL Training========\
actions (25180, 30)\
obs (25180, 181)\
rewards (25180,)\
episode_returns (10,)\
episode_starts (25180,)\
actions (25180, 30)\
obs (25180, 181)\
rewards (25180,)\
episode_returns (10,)\
episode_starts (25180,)\
Total trajectories: 10\
Total transitions: 25180\
Average returns: 267.04940006815013\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.318 seconds\
computegrad\
done in 0.219 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0      0.979          0\
         1       0.37     0.0679\
         2      0.401      0.301\
         3      0.526      0.645\
         4      0.347      0.983\
         5      0.383       2.11\
         6      0.459       2.25\
         7      0.296       3.05\
         8      0.298       3.65\
         9      0.895       4.02\
        10      0.395       4.48\
done in 0.476 seconds\
Expected: 0.174 Actual: 0.089\
violated KL constraint. shrinking step.\
Expected: 0.174 Actual: 0.046\
Stepsize OK!\
vf\
done in 0.157 seconds\
sampling\
done in 4.048 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.43          0\
         1       12.5     0.0194\
         2        4.6      0.103\
         3       4.87      0.289\
         4       39.3      0.706\
         5       8.76       1.45\
         6       38.7        2.1\
         7       10.7       2.19\
         8       32.6        3.3\
         9       4.77       4.95\
        10       35.3       6.33\
done in 0.050 seconds\
Expected: 0.354 Actual: 0.083\
Stepsize OK!\
vf\
done in 0.068 seconds\
sampling\
done in 4.353 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       31.8          0\
         1        3.7   0.000214\
         2       25.5   0.000943\
         3       6.85      0.256\
         4        639      0.392\
         5       22.7      0.407\
         6   2.15e+03      0.667\
         7        8.1      0.788\
         8        227       0.92\
         9       3.88      0.926\
        10        221       2.37\
done in 0.051 seconds\
Expected: 0.248 Actual: 0.028\
Stepsize OK!\
vf\
done in 0.066 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.58064 |       0.43754 |       0.62789 |      -0.00063 |       0.71484 |       0.90137\
-----------------------------------------\
| EpLenMean               | 2.52e+03    |\
| EpRewMean               | 2679.1606   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 109         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.4        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.57537    |\
| explained_variance_t... | 0.0166      |\
| meankl                  | 0.00532128  |\
| optimgain               | 0.027610196 |\
| reference_Q_mean        | 3.25        |\
| reference_Q_std         | 2.81        |\
| reference_action_mean   | 0.0038      |\
| reference_action_std    | 0.959       |\
| reference_actor_Q_mean  | 4.43        |\
| reference_actor_Q_std   | 2.83        |\
| rollout/Q_mean          | 2.32        |\
| rollout/actions_mean    | -0.00636    |\
| rollout/actions_std     | 0.807       |\
| rollout/episode_steps   | 2.52e+03    |\
| rollout/episodes        | 3           |\
| rollout/return          | 150         |\
| rollout/return_history  | 150         |\
| surrgain                | 0.027610196 |\
| total/duration          | 64          |\
| total/episodes          | 3           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 156         |\
| train/loss_actor        | -4.65       |\
| train/loss_critic       | 1.53        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.4012018998463949  minutes\
======GAIL Validation from:  20190104 to  20190405\
GAIL Sharpe Ratio:  0.2168306364173668\
======Trading from:  20190405 to  20190708\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x14b01ccc0>\
previous_total_asset:1494546.7651279117\
end_total_asset:1497947.6712911462\
total_reward:3400.9061632344965\
total_cost:  904.113644874473\
total trades:  163\
Sharpe:  0.2626124804487068\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20190405\
======A2C Training========\
Training time (A2C):  1.8500510136286417  minutes\
======A2C Validation from:  20190405 to  20190708\
A2C Sharpe Ratio:  0.0748920849111163\
======PPO Training========\
Training time (PPO):  6.550831580162049  minutes\
======PPO Validation from:  20190405 to  20190708\
PPO Sharpe Ratio:  0.3415332839957667\
======DDPG Training========\
Training time (DDPG):  1.1092956145604451  minutes\
======DDPG Validation from:  20190405 to  20190708\
DDPG Sharpe Ratio:  0.26998177230538206\
======TD3 Training========\
Training time (TD3):  1.231289835770925  minutes\
======TD3 Validation from:  20190405 to  20190708\
TD3 Sharpe Ratio:  0.20753471851156147\
======GAIL Training========\
actions (25810, 30)\
obs (25810, 181)\
rewards (25810,)\
episode_returns (10,)\
episode_starts (25810,)\
actions (25810, 30)\
obs (25810, 181)\
rewards (25810,)\
episode_returns (10,)\
episode_starts (25810,)\
Total trajectories: 10\
Total transitions: 25810\
Average returns: 142.71686921850778\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.262 seconds\
computegrad\
done in 0.203 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        1.3          0\
         1      0.843     0.0884\
         2       1.16      0.467\
         3       1.66       2.05\
         4        4.5       3.22\
         5       5.61       4.31\
         6       2.67        6.8\
         7        2.7       9.53\
         8       2.96       11.1\
         9       5.44       12.1\
        10       2.32       14.2\
done in 0.415 seconds\
Expected: 0.391 Actual: 0.106\
Stepsize OK!\
vf\
done in 0.154 seconds\
sampling\
done in 4.026 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.03          0\
         1       36.2     0.0364\
         2       3.92      0.125\
         3       3.75      0.146\
         4       8.07       0.18\
         5       34.9      0.423\
         6       7.52       1.29\
         7       35.9       1.77\
         8       75.7        1.9\
         9       39.3       2.62\
        10         10       4.23\
done in 0.050 seconds\
Expected: 0.278 Actual: 0.097\
Stepsize OK!\
vf\
done in 0.073 seconds\
sampling\
done in 4.238 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       5.02          0\
         1       51.4      0.107\
         2       89.1      0.576\
         3        122      0.983\
         4        138       1.98\
         5        533       7.19\
         6        499       12.8\
         7        233       14.5\
         8        278       17.2\
         9        520       46.9\
        10        373       58.6\
done in 0.051 seconds\
Expected: 1.530 Actual: 0.068\
Stepsize OK!\
vf\
done in 0.074 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.86088 |       0.79096 |       0.61598 |      -0.00062 |       0.45605 |       0.44434\
-----------------------------------------\
| EpLenMean               | 2.58e+03    |\
| EpRewMean               | 2537.7737   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 109         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.1        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.580032   |\
| explained_variance_t... | -0.0166     |\
| meankl                  | 0.009594951 |\
| optimgain               | 0.067714326 |\
| reference_Q_mean        | 4.47        |\
| reference_Q_std         | 4.63        |\
| reference_action_mean   | 0.206       |\
| reference_action_std    | 0.943       |\
| reference_actor_Q_mean  | 5.69        |\
| reference_actor_Q_std   | 4.6         |\
| rollout/Q_mean          | 3.09        |\
| rollout/actions_mean    | 0.0934      |\
| rollout/actions_std     | 0.796       |\
| rollout/episode_steps   | 2.58e+03    |\
| rollout/episodes        | 3           |\
| rollout/return          | 300         |\
| rollout/return_history  | 300         |\
| surrgain                | 0.067714326 |\
| total/duration          | 65.7        |\
| total/episodes          | 3           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 152         |\
| train/loss_actor        | -5.74       |\
| train/loss_critic       | 2.78        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.479023019472758  minutes\
======GAIL Validation from:  20190405 to  20190708\
GAIL Sharpe Ratio:  0.2912287688738578\
======Trading from:  20190708 to  20191004\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x141d4b208>\
previous_total_asset:1497947.6712911462\
end_total_asset:1499112.9963740236\
total_reward:1165.325082877418\
total_cost:  2276.81999287643\
total trades:  349\
Sharpe:  0.023867845897420602\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20190708\
======A2C Training========\
Training time (A2C):  1.896341900030772  minutes\
======A2C Validation from:  20190708 to  20191004\
A2C Sharpe Ratio:  0.03990646362713551\
======PPO Training========\
Training time (PPO):  6.4911704659461975  minutes\
======PPO Validation from:  20190708 to  20191004\
PPO Sharpe Ratio:  -0.0035392850761550203\
======DDPG Training========\
Training time (DDPG):  1.0892269333203635  minutes\
======DDPG Validation from:  20190708 to  20191004\
DDPG Sharpe Ratio:  0.07844127638120364\
======TD3 Training========\
Training time (TD3):  1.2608891367912292  minutes\
======TD3 Validation from:  20190708 to  20191004\
TD3 Sharpe Ratio:  -0.005012018975521423\
======GAIL Training========\
actions (26440, 30)\
obs (26440, 181)\
rewards (26440,)\
episode_returns (10,)\
episode_starts (26440,)\
actions (26440, 30)\
obs (26440, 181)\
rewards (26440,)\
episode_returns (10,)\
episode_starts (26440,)\
Total trajectories: 10\
Total transitions: 26440\
Average returns: 164.91471294910298\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.341 seconds\
computegrad\
done in 0.218 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.52          0\
         1       77.1     0.0517\
         2      0.559     0.0606\
         3       4.75      0.148\
         4       3.34      0.525\
         5      0.521      0.526\
         6       4.13       0.72\
         7       12.5       1.48\
         8       2.32       1.48\
         9      0.683       1.73\
        10       11.7       1.92\
done in 0.433 seconds\
Expected: 0.145 Actual: 0.115\
Stepsize OK!\
vf\
done in 0.159 seconds\
sampling\
done in 4.145 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       59.1          0\
         1        2.9    0.00523\
         2       6.96     0.0546\
         3       14.3      0.118\
         4       5.14      0.287\
         5        102      0.494\
         6       49.2      0.597\
         7         13       1.86\
         8       36.4       3.52\
         9        159        7.4\
        10       38.5       7.52\
done in 0.055 seconds\
Expected: 0.409 Actual: 0.079\
violated KL constraint. shrinking step.\
Expected: 0.409 Actual: 0.043\
Stepsize OK!\
vf\
done in 0.072 seconds\
sampling\
done in 4.415 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.49          0\
         1       2.02     0.0992\
         2       6.63      0.147\
         3       2.26      0.776\
         4       16.7       1.04\
         5       12.8       2.23\
         6       10.6       4.47\
         7       4.31       5.71\
         8        139       8.41\
         9       27.4       10.6\
        10       3.18       12.7\
done in 0.050 seconds\
Expected: 0.431 Actual: 0.101\
Stepsize OK!\
vf\
done in 0.070 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.73373 |       0.79459 |       0.65404 |      -0.00065 |       0.50684 |       0.41797\
-----------------------------------------\
| EpLenMean               | 2.64e+03    |\
| EpRewMean               | 833.2178    |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 181         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.6        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.546432   |\
| explained_variance_t... | -0.172      |\
| meankl                  | 0.013018403 |\
| optimgain               | 0.10132226  |\
| reference_Q_mean        | 7.55        |\
| reference_Q_std         | 5.69        |\
| reference_action_mean   | -0.0293     |\
| reference_action_std    | 0.966       |\
| reference_actor_Q_mean  | 9.78        |\
| reference_actor_Q_std   | 6.27        |\
| rollout/Q_mean          | 4.36        |\
| rollout/actions_mean    | 0.0252      |\
| rollout/actions_std     | 0.809       |\
| rollout/episode_steps   | 2.64e+03    |\
| rollout/episodes        | 3           |\
| rollout/return          | 481         |\
| rollout/return_history  | 481         |\
| surrgain                | 0.10132226  |\
| total/duration          | 64.4        |\
| total/episodes          | 3           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 155         |\
| train/loss_actor        | -9.42       |\
| train/loss_critic       | 9.95        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.5147883852322896  minutes\
======GAIL Validation from:  20190708 to  20191004\
GAIL Sharpe Ratio:  -0.11581050094581377\
======Trading from:  20191004 to  20200106\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x14c44c0f0>\
previous_total_asset:1499112.9963740236\
end_total_asset:1497465.731363619\
total_reward:-1647.2650104046334\
total_cost:  411.09297613288874\
total trades:  62\
Sharpe:  -0.402006372226195\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20191004\
======A2C Training========\
Training time (A2C):  1.8589323997497558  minutes\
======A2C Validation from:  20191004 to  20200106\
A2C Sharpe Ratio:  -0.43218318214376644\
======PPO Training========\
Training time (PPO):  6.509676921367645  minutes\
======PPO Validation from:  20191004 to  20200106\
PPO Sharpe Ratio:  -0.12239563922834691\
======DDPG Training========\
Training time (DDPG):  1.0998912374178569  minutes\
======DDPG Validation from:  20191004 to  20200106\
DDPG Sharpe Ratio:  -0.4085529371903274\
======TD3 Training========\
Training time (TD3):  1.2352863470713298  minutes\
======TD3 Validation from:  20191004 to  20200106\
TD3 Sharpe Ratio:  -0.38602637980741733\
======GAIL Training========\
actions (27070, 30)\
obs (27070, 181)\
rewards (27070,)\
episode_returns (10,)\
episode_starts (27070,)\
actions (27070, 30)\
obs (27070, 181)\
rewards (27070,)\
episode_returns (10,)\
episode_starts (27070,)\
Total trajectories: 10\
Total transitions: 27070\
Average returns: 223.25739518349292\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.306 seconds\
computegrad\
done in 0.203 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.09          0\
         1       2.93       0.14\
         2       3.39      0.181\
         3       6.24      0.565\
         4       2.79      0.813\
         5       26.9       1.86\
         6       2.01       3.07\
         7       15.5       5.56\
         8       52.9       6.98\
         9       4.85       7.87\
        10       3.08       8.05\
done in 0.405 seconds\
Expected: 0.333 Actual: 0.101\
Stepsize OK!\
vf\
done in 0.145 seconds\
sampling\
done in 4.081 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       7.22          0\
         1        1.8   4.43e-06\
         2         13     0.0483\
         3   2.17e+04      0.115\
         4       2.12      0.155\
         5       49.4      0.269\
         6       4.21      0.269\
         7       10.4      0.753\
         8        310      0.753\
         9       3.83      0.893\
        10   6.88e+04       1.44\
done in 0.053 seconds\
Expected: 0.158 Actual: 0.075\
Stepsize OK!\
vf\
done in 0.071 seconds\
sampling\
done in 4.798 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        300          0\
         1   1.74e+05       58.2\
         2   1.91e+04        137\
         3    2.5e+04        158\
         4    3.3e+03        191\
         5   1.48e+03        193\
         6    1.9e+03        197\
         7   5.56e+03        201\
         8   2.22e+03        209\
         9   3.04e+03        212\
        10   4.67e+03        214\
done in 0.051 seconds\
Expected: 8.598 Actual: 0.003\
Stepsize OK!\
vf\
done in 0.074 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.79176 |       0.94033 |       0.64433 |      -0.00064 |       0.45898 |       0.30176\
------------------------------------------\
| EpLenMean               | 2.71e+03     |\
| EpRewMean               | 1633.3212    |\
| EpThisIter              | 1            |\
| EpTrueRewMean           | 106          |\
| EpisodesSoFar           | 1            |\
| TimeElapsed             | 15           |\
| TimestepsSoFar          | 1024         |\
| entloss                 | 0.0          |\
| entropy                 | 42.571957    |\
| explained_variance_t... | -0.105       |\
| meankl                  | 0.0035601316 |\
| optimgain               | 0.003238828  |\
| reference_Q_mean        | 5.31         |\
| reference_Q_std         | 3.59         |\
| reference_action_mean   | 0.147        |\
| reference_action_std    | 0.953        |\
| reference_actor_Q_mean  | 6.49         |\
| reference_actor_Q_std   | 3.6          |\
| rollout/Q_mean          | 3.33         |\
| rollout/actions_mean    | 0.0629       |\
| rollout/actions_std     | 0.818        |\
| rollout/episode_steps   | 2.71e+03     |\
| rollout/episodes        | 3            |\
| rollout/return          | 375          |\
| rollout/return_history  | 375          |\
| surrgain                | 0.003238828  |\
| total/duration          | 65.1         |\
| total/episodes          | 3            |\
| total/epochs            | 1            |\
| total/steps             | 9998         |\
| total/steps_per_second  | 154          |\
| train/loss_actor        | -6.54        |\
| train/loss_critic       | 2.94         |\
| train/param_noise_di... | 0            |\
------------------------------------------\
Training time (GAIL):  1.5364503502845763  minutes\
======GAIL Validation from:  20191004 to  20200106\
GAIL Sharpe Ratio:  -0.030694772300081802\
======Trading from:  20200106 to  20200406\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x150af9f60>\
previous_total_asset:1497465.731363619\
end_total_asset:1488422.6413321856\
total_reward:-9043.090031433385\
total_cost:  676.639472641206\
total trades:  143\
Sharpe:  -0.45501705631212414\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20200106\
======A2C Training========\
Training time (A2C):  1.8778363863627117  minutes\
======A2C Validation from:  20200106 to  20200406\
A2C Sharpe Ratio:  -0.4581842918803575\
======PPO Training========\
Training time (PPO):  6.515642368793488  minutes\
======PPO Validation from:  20200106 to  20200406\
PPO Sharpe Ratio:  -0.41988025528554634\
======DDPG Training========\
Training time (DDPG):  1.0645084341367086  minutes\
======DDPG Validation from:  20200106 to  20200406\
DDPG Sharpe Ratio:  -0.39128624411348983\
======TD3 Training========\
Training time (TD3):  1.2608430822690329  minutes\
======TD3 Validation from:  20200106 to  20200406\
TD3 Sharpe Ratio:  -0.43395827171444945\
======GAIL Training========\
actions (27700, 30)\
obs (27700, 181)\
rewards (27700,)\
episode_returns (10,)\
episode_starts (27700,)\
actions (27700, 30)\
obs (27700, 181)\
rewards (27700,)\
episode_returns (10,)\
episode_starts (27700,)\
Total trajectories: 10\
Total transitions: 27700\
Average returns: 212.46744343639148\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.298 seconds\
computegrad\
done in 0.202 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.93          0\
         1       1.46       0.13\
         2       2.06      0.974\
         3      0.738       1.83\
         4      0.949       1.94\
         5      0.247       2.35\
         6       0.19       2.39\
         7      0.227       2.48\
         8      0.236       2.75\
         9      0.308       3.02\
        10      0.277       3.13\
done in 0.454 seconds\
Expected: 0.219 Actual: 0.046\
Stepsize OK!\
vf\
done in 0.168 seconds\
sampling\
done in 4.094 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.28          0\
         1       56.3      0.104\
         2       7.46      0.321\
         3        108       1.66\
         4       54.4       6.59\
         5        156       9.99\
         6        500       15.4\
         7        222       16.6\
         8       96.8         21\
         9       62.5       26.5\
        10        156       48.5\
done in 0.051 seconds\
Expected: 1.178 Actual: 0.055\
Stepsize OK!\
vf\
done in 0.066 seconds\
sampling\
done in 4.343 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.66          0\
         1       43.9    0.00591\
         2       2.31     0.0866\
         3       2.68      0.128\
         4        245       1.46\
         5       10.5       1.51\
         6       3.45       1.76\
         7       37.7        3.2\
         8        692       3.36\
         9       8.95       6.48\
        10       16.4       8.18\
done in 0.051 seconds\
Expected: 0.357 Actual: 0.061\
Stepsize OK!\
vf\
done in 0.071 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.81173 |       0.72241 |       0.66081 |      -0.00066 |       0.39160 |       0.49902\
-----------------------------------------\
| EpLenMean               | 2.77e+03    |\
| EpRewMean               | 2579.81     |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 167         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.4        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.61226    |\
| explained_variance_t... | 0.057       |\
| meankl                  | 0.009768106 |\
| optimgain               | 0.060829565 |\
| reference_Q_mean        | 4.39        |\
| reference_Q_std         | 4.17        |\
| reference_action_mean   | -0.322      |\
| reference_action_std    | 0.923       |\
| reference_actor_Q_mean  | 5.82        |\
| reference_actor_Q_std   | 4.24        |\
| rollout/Q_mean          | 2.98        |\
| rollout/actions_mean    | -0.231      |\
| rollout/actions_std     | 0.777       |\
| rollout/episode_steps   | 2.77e+03    |\
| rollout/episodes        | 3           |\
| rollout/return          | 350         |\
| rollout/return_history  | 350         |\
| surrgain                | 0.060829565 |\
| total/duration          | 63          |\
| total/episodes          | 3           |\
| total/epochs            | 1           |\
| total/steps             | 9998        |\
| total/steps_per_second  | 159         |\
| train/loss_actor        | -6.18       |\
| train/loss_critic       | 2.87        |\
| train/param_noise_di... | 0           |\
-----------------------------------------\
Training time (GAIL):  1.5387496987978617  minutes\
======GAIL Validation from:  20200106 to  20200406\
GAIL Sharpe Ratio:  -0.4676425335971481\
======Trading from:  20200406 to  20200707\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x14b65d470>\
previous_total_asset:1488422.6413321856\
end_total_asset:1492431.6974123414\
total_reward:4009.056080155773\
total_cost:  453.22383950494765\
total trades:  69\
Sharpe:  0.2532065750258027\
Ensemble Strategy took:  214.48479246298473  minutes}