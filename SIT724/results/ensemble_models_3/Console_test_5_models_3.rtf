{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf610
{\fonttbl\f0\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgray\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs20 \cf2 \cb3 \CocoaLigature0 ============Start Ensemble Strategy============\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20151002\
======A2C Training========\
Training time (A2C):  1.9024808645248412  minutes\
======A2C Validation from:  20151002 to  20160104\
A2C Sharpe Ratio:  -0.0030096931124770528\
======PPO Training========\
Training time (PPO):  6.465850901603699  minutes\
======PPO Validation from:  20151002 to  20160104\
PPO Sharpe Ratio:  -0.12169558353568984\
======DDPG Training========\
Training time (DDPG):  1.075851821899414  minutes\
======DDPG Validation from:  20151002 to  20160104\
DDPG Sharpe Ratio:  0.015921433896041552\
======TD3 Training========\
Training time (TD3):  1.1938383340835572  minutes\
======TD3 Validation from:  20151002 to  20160104\
TD3 Sharpe Ratio:  0.0824134421069873\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 6.710463  |\
| ent_coef_loss           | -665.624  |\
| entropy                 | 36.56815  |\
| episodes                | 4         |\
| fps                     | 123       |\
| mean 100 episode reward | 142       |\
| n_updates               | 6697      |\
| policy_loss             | 38545.36  |\
| qf1_loss                | 25505.414 |\
| qf2_loss                | 10835.311 |\
| reference_Q_mean        | 4.1       |\
| reference_Q_std         | 3.93      |\
| reference_action_mean   | -0.18     |\
| reference_action_std    | 0.958     |\
| reference_actor_Q_mean  | 5.34      |\
| reference_actor_Q_std   | 3.92      |\
| rollout/Q_mean          | 2.19      |\
| rollout/actions_mean    | -0.131    |\
| rollout/actions_std     | 0.813     |\
| rollout/episode_steps   | 1.7e+03   |\
| rollout/episodes        | 5         |\
| rollout/return          | 142       |\
| rollout/return_history  | 142       |\
| time_elapsed            | 55        |\
| total timesteps         | 6796      |\
| total/duration          | 63.6      |\
| total/episodes          | 5         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 157       |\
| train/loss_actor        | -5.06     |\
| train/loss_critic       | 1.65      |\
| train/param_noise_di... | 0         |\
| value_loss              | 11308.562 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 51.49003   |\
| ent_coef_loss           | -1379.4474 |\
| entropy                 | 36.56815   |\
| episodes                | 8          |\
| fps                     | 124        |\
| mean 100 episode reward | 158        |\
| n_updates               | 13493      |\
| policy_loss             | 291746.2   |\
| qf1_loss                | 1620808.2  |\
| qf2_loss                | 1530733.1  |\
| time_elapsed            | 109        |\
| total timesteps         | 13592      |\
| value_loss              | 822705.0   |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 395.37308  |\
| ent_coef_loss           | -2091.691  |\
| entropy                 | 36.56815   |\
| episodes                | 12         |\
| fps                     | 123        |\
| mean 100 episode reward | 167        |\
| n_updates               | 20289      |\
| policy_loss             | 2212579.5  |\
| qf1_loss                | 92840450.0 |\
| qf2_loss                | 90879630.0 |\
| time_elapsed            | 164        |\
| total timesteps         | 20388      |\
| value_loss              | 11919966.0 |\
----------------------------------------\
------------------------------------------\
| current_lr              | 0.0003       |\
| ent_coef                | 3035.8904    |\
| ent_coef_loss           | -2805.6558   |\
| entropy                 | 36.56815     |\
| episodes                | 16           |\
| fps                     | 122          |\
| mean 100 episode reward | 171          |\
| n_updates               | 27085        |\
| policy_loss             | 17013604.0   |\
| qf1_loss                | 2289755100.0 |\
| qf2_loss                | 2278658800.0 |\
| time_elapsed            | 222          |\
| total timesteps         | 27184        |\
| value_loss              | 630807900.0  |\
------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 23350.027      |\
| ent_coef_loss           | -3508.6182     |\
| entropy                 | 36.56815       |\
| episodes                | 20             |\
| fps                     | 122            |\
| mean 100 episode reward | 174            |\
| n_updates               | 33881          |\
| policy_loss             | 130028296.0    |\
| qf1_loss                | 98376696000.0  |\
| qf2_loss                | 101045110000.0 |\
| time_elapsed            | 277            |\
| total timesteps         | 33980          |\
| value_loss              | 95445385000.0  |\
--------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 179600.08        |\
| ent_coef_loss           | -4237.955        |\
| entropy                 | 36.56815         |\
| episodes                | 24               |\
| fps                     | 122              |\
| mean 100 episode reward | 176              |\
| n_updates               | 40677            |\
| policy_loss             | 986297500.0      |\
| qf1_loss                | 17284342000000.0 |\
| qf2_loss                | 17661176000000.0 |\
| time_elapsed            | 332              |\
| total timesteps         | 40776            |\
| value_loss              | 585889700000.0   |\
----------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 1381451.2          |\
| ent_coef_loss           | -4936.336          |\
| entropy                 | 36.56815           |\
| episodes                | 28                 |\
| fps                     | 120                |\
| mean 100 episode reward | 177                |\
| n_updates               | 47473              |\
| policy_loss             | 6996861400.0       |\
| qf1_loss                | 5209278000000000.0 |\
| qf2_loss                | 6438440500000000.0 |\
| time_elapsed            | 393                |\
| total timesteps         | 47572              |\
| value_loss              | 38644150000000.0   |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 10620038.0    |\
| ent_coef_loss           | -5635.0967    |\
| entropy                 | 36.56815      |\
| episodes                | 32            |\
| fps                     | 121           |\
| mean 100 episode reward | 178           |\
| n_updates               | 54269         |\
| policy_loss             | 29907894000.0 |\
| qf1_loss                | 3.3969175e+18 |\
| qf2_loss                | 4.8906013e+18 |\
| time_elapsed            | 447           |\
| total timesteps         | 54368         |\
| value_loss              | 1.1603477e+16 |\
-------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 81274330.0     |\
| ent_coef_loss           | -6381.1553     |\
| entropy                 | 36.56815       |\
| episodes                | 36             |\
| fps                     | 122            |\
| mean 100 episode reward | 178            |\
| n_updates               | 61065          |\
| policy_loss             | 100391060000.0 |\
| qf1_loss                | 1.6341508e+20  |\
| qf2_loss                | 2.0687524e+20  |\
| time_elapsed            | 501            |\
| total timesteps         | 61164          |\
| value_loss              | 4.581452e+19   |\
--------------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 122      |\
| mean 100 episode reward | 170      |\
| n_updates               | 67861    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 555      |\
| total timesteps         | 67960    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 122      |\
| mean 100 episode reward | 155      |\
| n_updates               | 74657    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 607      |\
| total timesteps         | 74756    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 123      |\
| mean 100 episode reward | 142      |\
| n_updates               | 81453    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 660      |\
| total timesteps         | 81552    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 52       |\
| fps                     | 123      |\
| mean 100 episode reward | 131      |\
| n_updates               | 88249    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 712      |\
| total timesteps         | 88348    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 56       |\
| fps                     | 124      |\
| mean 100 episode reward | 122      |\
| n_updates               | 95045    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 765      |\
| total timesteps         | 95144    |\
| value_loss              | nan      |\
--------------------------------------\
actions (16990, 30)\
obs (16990, 181)\
rewards (16990,)\
episode_returns (10,)\
episode_starts (16990,)\
actions (16990, 30)\
obs (16990, 181)\
rewards (16990,)\
episode_returns (10,)\
episode_starts (16990,)\
Total trajectories: 10\
Total transitions: 16990\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.263 seconds\
computegrad\
done in 0.315 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.53          0\
         1       1.73      0.119\
         2      0.755      0.474\
         3      0.528        1.5\
         4      0.875       1.88\
         5      0.727       2.64\
         6      0.807       2.78\
         7       1.13       3.56\
         8      0.312       4.15\
         9      0.203       4.44\
        10       1.29       5.67\
done in 0.611 seconds\
Expected: 0.243 Actual: 0.100\
violated KL constraint. shrinking step.\
Expected: 0.243 Actual: 0.071\
Stepsize OK!\
vf\
done in 0.202 seconds\
sampling\
done in 4.692 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       4.61          0\
         1        167      0.369\
         2        161      0.408\
         3       32.3       0.69\
         4        429       3.42\
         5        649       10.3\
         6        518       10.4\
         7        197       15.8\
         8   1.02e+03       32.7\
         9   5.06e+04         47\
        10        417       58.4\
done in 0.051 seconds\
Expected: 1.504 Actual: 0.049\
Stepsize OK!\
vf\
done in 0.077 seconds\
sampling\
done in 3.883 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.27          0\
         1       12.6     0.0198\
         2       2.57     0.0633\
         3       1.82     0.0782\
         4       1.74     0.0819\
         5       20.2      0.651\
         6       1.36      0.877\
         7       5.08       1.22\
         8       27.6       1.29\
         9        108       4.18\
        10       4.93       4.42\
done in 0.051 seconds\
Expected: 0.237 Actual: 0.061\
Stepsize OK!\
vf\
done in 0.067 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.76519 |           nan |           nan |           nan |       0.44922 |       0.00000\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 14.9        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.57958    |\
| explained_variance_t... | 0.0277      |\
| meankl                  | 0.014284071 |\
| optimgain               | 0.061088346 |\
| surrgain                | 0.061088346 |\
-----------------------------------------\
Training time (GAIL):  14.280650234222412  minutes\
======GAIL Validation from:  20151002 to  20160104\
GAIL Sharpe Ratio:  -0.057008816384832996\
======Trading from:  20160104 to  20160405\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x146ab3940>\
previous_total_asset:1000000\
end_total_asset:1094078.5440600996\
total_reward:94078.54406009964\
total_cost:  1037.7926902433112\
total trades:  975\
Sharpe:  0.30889527161247304\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20160104\
======A2C Training========\
Training time (A2C):  1.7898251175880433  minutes\
======A2C Validation from:  20160104 to  20160405\
A2C Sharpe Ratio:  0.13221340165290285\
======PPO Training========\
Training time (PPO):  6.732309118906657  minutes\
======PPO Validation from:  20160104 to  20160405\
PPO Sharpe Ratio:  0.15238168332095237\
======DDPG Training========\
Training time (DDPG):  1.1797645489374797  minutes\
======DDPG Validation from:  20160104 to  20160405\
DDPG Sharpe Ratio:  0.11552008752829436\
======TD3 Training========\
Training time (TD3):  1.2884315292040507  minutes\
======TD3 Validation from:  20160104 to  20160405\
TD3 Sharpe Ratio:  0.005039830634260703\
======GAIL Training========\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 6.9870214  |\
| ent_coef_loss           | -557.93726 |\
| entropy                 | 97.75567   |\
| episodes                | 4          |\
| fps                     | 116        |\
| mean 100 episode reward | 135        |\
| n_updates               | 6949       |\
| policy_loss             | 33423.023  |\
| qf1_loss                | 42375.844  |\
| qf2_loss                | 143865.52  |\
| reference_Q_mean        | 3.31       |\
| reference_Q_std         | 2.87       |\
| reference_action_mean   | 0.00729    |\
| reference_action_std    | 0.978      |\
| reference_actor_Q_mean  | 4.02       |\
| reference_actor_Q_std   | 2.88       |\
| rollout/Q_mean          | 2.05       |\
| rollout/actions_mean    | 0.0761     |\
| rollout/actions_std     | 0.816      |\
| rollout/episode_steps   | 1.76e+03   |\
| rollout/episodes        | 5          |\
| rollout/return          | 154        |\
| rollout/return_history  | 154        |\
| time_elapsed            | 60         |\
| total timesteps         | 7048       |\
| total/duration          | 69.8       |\
| total/episodes          | 5          |\
| total/epochs            | 1          |\
| total/steps             | 9998       |\
| total/steps_per_second  | 143        |\
| train/loss_actor        | -3.92      |\
| train/loss_critic       | 0.986      |\
| train/param_noise_di... | 0          |\
| value_loss              | 47923.504  |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 57.056587  |\
| ent_coef_loss           | -1141.4849 |\
| entropy                 | 101.88067  |\
| episodes                | 8          |\
| fps                     | 95         |\
| mean 100 episode reward | 113        |\
| n_updates               | 13997      |\
| policy_loss             | 264636.7   |\
| qf1_loss                | 1576389.0  |\
| qf2_loss                | 5358303.5  |\
| time_elapsed            | 147        |\
| total timesteps         | 14096      |\
| value_loss              | 5599210.0  |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 472.71832  |\
| ent_coef_loss           | -1733.3806 |\
| entropy                 | 102.56817  |\
| episodes                | 12         |\
| fps                     | 98         |\
| mean 100 episode reward | 110        |\
| n_updates               | 21045      |\
| policy_loss             | 2172620.2  |\
| qf1_loss                | 82075460.0 |\
| qf2_loss                | 82708856.0 |\
| time_elapsed            | 213        |\
| total timesteps         | 21144      |\
| value_loss              | 70219380.0 |\
----------------------------------------\
------------------------------------------\
| current_lr              | 0.0003       |\
| ent_coef                | 3915.4033    |\
| ent_coef_loss           | -2327.647    |\
| entropy                 | 102.56817    |\
| episodes                | 16           |\
| fps                     | 103          |\
| mean 100 episode reward | 112          |\
| n_updates               | 28093        |\
| policy_loss             | 17779118.0   |\
| qf1_loss                | 6634564600.0 |\
| qf2_loss                | 6574143000.0 |\
| time_elapsed            | 271          |\
| total timesteps         | 28192        |\
| value_loss              | 685510400.0  |\
------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 32470.004      |\
| ent_coef_loss           | -2928.2874     |\
| entropy                 | 102.56817      |\
| episodes                | 20             |\
| fps                     | 106            |\
| mean 100 episode reward | 110            |\
| n_updates               | 35141          |\
| policy_loss             | 148038850.0    |\
| qf1_loss                | 546553920000.0 |\
| qf2_loss                | 545234400000.0 |\
| time_elapsed            | 331            |\
| total timesteps         | 35240          |\
| value_loss              | 54281445000.0  |\
--------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 269305.6         |\
| ent_coef_loss           | -3523.3835       |\
| entropy                 | 102.56817        |\
| episodes                | 24               |\
| fps                     | 108              |\
| mean 100 episode reward | 105              |\
| n_updates               | 42189            |\
| policy_loss             | 1187083500.0     |\
| qf1_loss                | 60365890000000.0 |\
| qf2_loss                | 59044570000000.0 |\
| time_elapsed            | 388              |\
| total timesteps         | 42288            |\
| value_loss              | 2971854400000.0  |\
----------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 2233340.5          |\
| ent_coef_loss           | -4112.58           |\
| entropy                 | 102.56817          |\
| episodes                | 28                 |\
| fps                     | 110                |\
| mean 100 episode reward | 102                |\
| n_updates               | 49237              |\
| policy_loss             | 8036727000.0       |\
| qf1_loss                | 4.1933076e+16      |\
| qf2_loss                | 3.017053e+16       |\
| time_elapsed            | 444                |\
| total timesteps         | 49336              |\
| value_loss              | 1349061000000000.0 |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 18490666.0    |\
| ent_coef_loss           | -4725.4727    |\
| entropy                 | 102.56817     |\
| episodes                | 32            |\
| fps                     | 112           |\
| mean 100 episode reward | 99.2          |\
| n_updates               | 56285         |\
| policy_loss             | 27592389000.0 |\
| qf1_loss                | 4.4185354e+18 |\
| qf2_loss                | 2.9226072e+18 |\
| time_elapsed            | 502           |\
| total timesteps         | 56384         |\
| value_loss              | 3.1683425e+18 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 152600540.0   |\
| ent_coef_loss           | -5322.583     |\
| entropy                 | 102.56817     |\
| episodes                | 36            |\
| fps                     | 113           |\
| mean 100 episode reward | 97.2          |\
| n_updates               | 63333         |\
| policy_loss             | 92715926000.0 |\
| qf1_loss                | 4.3125502e+19 |\
| qf2_loss                | 2.9008151e+19 |\
| time_elapsed            | 559           |\
| total timesteps         | 63432         |\
| value_loss              | 7.419019e+20  |\
-------------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 114      |\
| mean 100 episode reward | 91.6     |\
| n_updates               | 70381    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 614      |\
| total timesteps         | 70480    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 115      |\
| mean 100 episode reward | 83.2     |\
| n_updates               | 77429    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 670      |\
| total timesteps         | 77528    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 116      |\
| mean 100 episode reward | 76.3     |\
| n_updates               | 84477    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 724      |\
| total timesteps         | 84576    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 52       |\
| fps                     | 117      |\
| mean 100 episode reward | 70.4     |\
| n_updates               | 91525    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 779      |\
| total timesteps         | 91624    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 56       |\
| fps                     | 118      |\
| mean 100 episode reward | 65.4     |\
| n_updates               | 98573    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 835      |\
| total timesteps         | 98672    |\
| value_loss              | nan      |\
--------------------------------------\
actions (17620, 30)\
obs (17620, 181)\
rewards (17620,)\
episode_returns (10,)\
episode_starts (17620,)\
actions (17620, 30)\
obs (17620, 181)\
rewards (17620,)\
episode_returns (10,)\
episode_starts (17620,)\
Total trajectories: 10\
Total transitions: 17620\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.074 seconds\
computegrad\
done in 0.199 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.15          0\
         1      0.804     0.0556\
         2      0.862      0.066\
         3      0.488      0.342\
         4        0.7      0.354\
         5      0.712      0.737\
         6      0.478      0.886\
         7      0.384       1.09\
         8      0.147        1.7\
         9      0.594       1.72\
        10      0.673       1.86\
done in 0.394 seconds\
Expected: 0.126 Actual: 0.078\
Stepsize OK!\
vf\
done in 0.143 seconds\
sampling\
done in 4.095 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.73          0\
         1       42.8      0.132\
         2         23      0.548\
         3        133      0.757\
         4        203       3.33\
         5        218       9.94\
         6        133       13.2\
         7        537       23.3\
         8        130       42.9\
         9        514       43.8\
        10        274       66.7\
done in 0.054 seconds\
Expected: 1.496 Actual: 0.047\
Stepsize OK!\
vf\
done in 0.071 seconds\
sampling\
done in 3.883 seconds\
computegrad\
done in 0.008 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       5.58          0\
         1       3.24    0.00112\
         2       12.5      0.122\
         3       5.46       0.18\
         4       8.53      0.268\
         5        639      0.768\
         6       31.7       1.04\
         7       29.3       1.88\
         8       20.7       2.37\
         9        203       3.97\
        10       88.2       4.06\
done in 0.051 seconds\
Expected: 0.296 Actual: 0.054\
Stepsize OK!\
vf\
done in 0.066 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.64757 |           nan |           nan |           nan |       0.60645 |       0.00000\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 13.6        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.57928    |\
| explained_variance_t... | -0.0807     |\
| meankl                  | 0.010867862 |\
| optimgain               | 0.054139007 |\
| surrgain                | 0.054139007 |\
-----------------------------------------\
Training time (GAIL):  15.003090496857961  minutes\
======GAIL Validation from:  20160104 to  20160405\
GAIL Sharpe Ratio:  -0.05968916401850913\
======Trading from:  20160405 to  20160705\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x148197c18>\
previous_total_asset:1094078.5440600996\
end_total_asset:1079687.5397248743\
total_reward:-14391.004335225327\
total_cost:  2878.2480334527045\
total trades:  1285\
Sharpe:  -0.038586478038462185\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20160405\
======A2C Training========\
Training time (A2C):  1.8781756480534872  minutes\
======A2C Validation from:  20160405 to  20160705\
A2C Sharpe Ratio:  -0.09908452979423474\
======PPO Training========\
Training time (PPO):  6.548886017004649  minutes\
======PPO Validation from:  20160405 to  20160705\
PPO Sharpe Ratio:  -0.058195361336150137\
======DDPG Training========\
Training time (DDPG):  1.1649853984514873  minutes\
======DDPG Validation from:  20160405 to  20160705\
DDPG Sharpe Ratio:  0.11450041189037131\
======TD3 Training========\
Training time (TD3):  1.30413156747818  minutes\
======TD3 Validation from:  20160405 to  20160705\
TD3 Sharpe Ratio:  0.02691522336700474\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 7.912632  |\
| ent_coef_loss           | -815.1964 |\
| entropy                 | -7.431846 |\
| episodes                | 4         |\
| fps                     | 121       |\
| mean 100 episode reward | 114       |\
| n_updates               | 7201      |\
| policy_loss             | 47645.027 |\
| qf1_loss                | 34280.535 |\
| qf2_loss                | 37083.41  |\
| reference_Q_mean        | 3.08      |\
| reference_Q_std         | 2.64      |\
| reference_action_mean   | 0.0252    |\
| reference_action_std    | 0.977     |\
| reference_actor_Q_mean  | 3.57      |\
| reference_actor_Q_std   | 2.71      |\
| rollout/Q_mean          | 1.85      |\
| rollout/actions_mean    | -0.0349   |\
| rollout/actions_std     | 0.813     |\
| rollout/episode_steps   | 1.82e+03  |\
| rollout/episodes        | 5         |\
| rollout/return          | 72.9      |\
| rollout/return_history  | 72.9      |\
| time_elapsed            | 59        |\
| total timesteps         | 7300      |\
| total/duration          | 68.9      |\
| total/episodes          | 5         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 145       |\
| train/loss_actor        | -3.26     |\
| train/loss_critic       | 0.784     |\
| train/param_noise_di... | 0         |\
| value_loss              | 131177.03 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 70.61234   |\
| ent_coef_loss           | -1676.1621 |\
| entropy                 | -7.431846  |\
| episodes                | 8          |\
| fps                     | 115        |\
| mean 100 episode reward | 109        |\
| n_updates               | 14501      |\
| policy_loss             | 443026.06  |\
| qf1_loss                | 1493939.2  |\
| qf2_loss                | 1168508.8  |\
| time_elapsed            | 126        |\
| total timesteps         | 14600      |\
| value_loss              | 973916.25  |\
----------------------------------------\
-----------------------------------------\
| current_lr              | 0.0003      |\
| ent_coef                | 630.83234   |\
| ent_coef_loss           | -2570.175   |\
| entropy                 | -11.556846  |\
| episodes                | 12          |\
| fps                     | 113         |\
| mean 100 episode reward | 106         |\
| n_updates               | 21801       |\
| policy_loss             | 3944154.2   |\
| qf1_loss                | 204966530.0 |\
| qf2_loss                | 215284300.0 |\
| time_elapsed            | 193         |\
| total timesteps         | 21900       |\
| value_loss              | 194797060.0 |\
-----------------------------------------\
------------------------------------------\
| current_lr              | 0.0003       |\
| ent_coef                | 5635.2886    |\
| ent_coef_loss           | -3419.751    |\
| entropy                 | -9.494346    |\
| episodes                | 16           |\
| fps                     | 112          |\
| mean 100 episode reward | 104          |\
| n_updates               | 29101        |\
| policy_loss             | 35179948.0   |\
| qf1_loss                | 7488845300.0 |\
| qf2_loss                | 7417526300.0 |\
| time_elapsed            | 259          |\
| total timesteps         | 29200        |\
| value_loss              | 6452417500.0 |\
------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 50351.188      |\
| ent_coef_loss           | -4294.7207     |\
| entropy                 | -9.494346      |\
| episodes                | 20             |\
| fps                     | 114            |\
| mean 100 episode reward | 103            |\
| n_updates               | 36401          |\
| policy_loss             | 310810240.0    |\
| qf1_loss                | 531886370000.0 |\
| qf2_loss                | 540221240000.0 |\
| time_elapsed            | 318            |\
| total timesteps         | 36500          |\
| value_loss              | 533353920000.0 |\
--------------------------------------------\
-----------------------------------------------\
| current_lr              | 0.0003            |\
| ent_coef                | 449840.03         |\
| ent_coef_loss           | -5187.049         |\
| entropy                 | -11.556847        |\
| episodes                | 24                |\
| fps                     | 115               |\
| mean 100 episode reward | 103               |\
| n_updates               | 43701             |\
| policy_loss             | 2638470000.0      |\
| qf1_loss                | 210233880000000.0 |\
| qf2_loss                | 200597050000000.0 |\
| time_elapsed            | 377               |\
| total timesteps         | 43800             |\
| value_loss              | 132255800000000.0 |\
-----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 4018483.5     |\
| ent_coef_loss           | -5989.8687    |\
| entropy                 | -7.431846     |\
| episodes                | 28            |\
| fps                     | 116           |\
| mean 100 episode reward | 102           |\
| n_updates               | 51001         |\
| policy_loss             | 13152170000.0 |\
| qf1_loss                | 2.5288933e+16 |\
| qf2_loss                | 2.1560646e+16 |\
| time_elapsed            | 437           |\
| total timesteps         | 51100         |\
| value_loss              | 5.5971646e+17 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 35842530.0    |\
| ent_coef_loss           | -6936.1543    |\
| entropy                 | -11.556845    |\
| episodes                | 32            |\
| fps                     | 117           |\
| mean 100 episode reward | 102           |\
| n_updates               | 58301         |\
| policy_loss             | 44128670000.0 |\
| qf1_loss                | 4.1047978e+17 |\
| qf2_loss                | 3.844546e+17  |\
| time_elapsed            | 497           |\
| total timesteps         | 58400         |\
| value_loss              | 9.838219e+19  |\
-------------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 118      |\
| mean 100 episode reward | 102      |\
| n_updates               | 65601    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 556      |\
| total timesteps         | 65700    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 119      |\
| mean 100 episode reward | 91.5     |\
| n_updates               | 72901    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 613      |\
| total timesteps         | 73000    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 119      |\
| mean 100 episode reward | 83.2     |\
| n_updates               | 80201    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 669      |\
| total timesteps         | 80300    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 120      |\
| mean 100 episode reward | 76.2     |\
| n_updates               | 87501    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 726      |\
| total timesteps         | 87600    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 52       |\
| fps                     | 121      |\
| mean 100 episode reward | 70.4     |\
| n_updates               | 94801    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 782      |\
| total timesteps         | 94900    |\
| value_loss              | nan      |\
--------------------------------------\
actions (18250, 30)\
obs (18250, 181)\
rewards (18250,)\
episode_returns (10,)\
episode_starts (18250,)\
actions (18250, 30)\
obs (18250, 181)\
rewards (18250,)\
episode_returns (10,)\
episode_starts (18250,)\
Total trajectories: 10\
Total transitions: 18250\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.115 seconds\
computegrad\
done in 0.213 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.28          0\
         1      0.987     0.0955\
         2      0.941      0.582\
         3       1.91       1.63\
         4       1.34       1.87\
         5       4.04       3.33\
         6       2.04       4.02\
         7        1.1       5.28\
         8       1.35       7.11\
         9       1.99       8.55\
        10       1.83       9.01\
done in 0.430 seconds\
Expected: 0.308 Actual: 0.094\
Stepsize OK!\
vf\
done in 0.151 seconds\
sampling\
done in 4.181 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        107          0\
         1   2.07e+04     0.0525\
         2   2.08e+04       8.63\
         3   6.04e+04       77.5\
         4   2.53e+06       94.5\
         5   4.13e+04        105\
         6    4.9e+04        141\
         7    5.9e+05        399\
         8   8.82e+04        401\
         9   1.56e+04        501\
        10   2.81e+04        552\
done in 0.049 seconds\
Expected: 10.644 Actual: 0.045\
violated KL constraint. shrinking step.\
Expected: 10.644 Actual: 0.026\
Stepsize OK!\
vf\
done in 0.068 seconds\
sampling\
done in 4.077 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       6.87          0\
         1       48.8    0.00168\
         2       4.16     0.0167\
         3       15.3     0.0178\
         4       11.2      0.019\
         5       29.2      0.103\
         6       4.72      0.184\
         7        151      0.195\
         8       9.74      0.247\
         9        119      0.348\
        10       12.4      0.356\
done in 0.050 seconds\
Expected: 0.098 Actual: 0.050\
Stepsize OK!\
vf\
done in 0.062 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.88497 |           nan |           nan |           nan |       0.37109 |       0.00000\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 14          |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.54932    |\
| explained_variance_t... | -0.0762     |\
| meankl                  | 0.010107711 |\
| optimgain               | 0.05015029  |\
| surrgain                | 0.05015029  |\
-----------------------------------------\
Training time (GAIL):  14.649432782332102  minutes\
======GAIL Validation from:  20160405 to  20160705\
GAIL Sharpe Ratio:  -0.009421757434490759\
======Trading from:  20160705 to  20161003\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x143b6d358>\
previous_total_asset:1079687.5397248743\
end_total_asset:1086454.330393657\
total_reward:6766.790668782778\
total_cost:  1397.4544459130043\
total trades:  999\
Sharpe:  0.03474524111768793\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20160705\
======A2C Training========\
Training time (A2C):  1.7878022988637288  minutes\
======A2C Validation from:  20160705 to  20161003\
A2C Sharpe Ratio:  -0.10972162438069002\
======PPO Training========\
Training time (PPO):  6.505349151293436  minutes\
======PPO Validation from:  20160705 to  20161003\
PPO Sharpe Ratio:  0.06963875585778594\
======DDPG Training========\
Training time (DDPG):  1.2508769035339355  minutes\
======DDPG Validation from:  20160705 to  20161003\
DDPG Sharpe Ratio:  0.034676224474849214\
======TD3 Training========\
Training time (TD3):  1.2248607317606608  minutes\
======TD3 Validation from:  20160705 to  20161003\
TD3 Sharpe Ratio:  0.022948040483889806\
======GAIL Training========\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 8.147813   |\
| ent_coef_loss           | -590.61633 |\
| entropy                 | 102.56817  |\
| episodes                | 4          |\
| fps                     | 122        |\
| mean 100 episode reward | 137        |\
| n_updates               | 7453       |\
| policy_loss             | 38959.133  |\
| qf1_loss                | 23076.602  |\
| qf2_loss                | 19960.752  |\
| reference_Q_mean        | 3.18       |\
| reference_Q_std         | 2.9        |\
| reference_action_mean   | 0.34       |\
| reference_action_std    | 0.873      |\
| reference_actor_Q_mean  | 3.8        |\
| reference_actor_Q_std   | 2.93       |\
| rollout/Q_mean          | 2.05       |\
| rollout/actions_mean    | 0.235      |\
| rollout/actions_std     | 0.779      |\
| rollout/episode_steps   | 1.89e+03   |\
| rollout/episodes        | 5          |\
| rollout/return          | 175        |\
| rollout/return_history  | 175        |\
| time_elapsed            | 61         |\
| total timesteps         | 7552       |\
| total/duration          | 74.1       |\
| total/episodes          | 5          |\
| total/epochs            | 1          |\
| total/steps             | 9998       |\
| total/steps_per_second  | 135        |\
| train/loss_actor        | -3.85      |\
| train/loss_critic       | 1.87       |\
| train/param_noise_di... | 0          |\
| value_loss              | 6954.6685  |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 78.38513   |\
| ent_coef_loss           | -1239.7827 |\
| entropy                 | 100.50567  |\
| episodes                | 8          |\
| fps                     | 122        |\
| mean 100 episode reward | 140        |\
| n_updates               | 15005      |\
| policy_loss             | 364994.8   |\
| qf1_loss                | 1750558.5  |\
| qf2_loss                | 2316738.0  |\
| time_elapsed            | 123        |\
| total timesteps         | 15104      |\
| value_loss              | 1085031.2  |\
----------------------------------------\
-----------------------------------------\
| current_lr              | 0.0003      |\
| ent_coef                | 755.226     |\
| ent_coef_loss           | -1866.1455  |\
| entropy                 | 102.56817   |\
| episodes                | 12          |\
| fps                     | 122         |\
| mean 100 episode reward | 141         |\
| n_updates               | 22557       |\
| policy_loss             | 3491771.5   |\
| qf1_loss                | 130683570.0 |\
| qf2_loss                | 120052580.0 |\
| time_elapsed            | 185         |\
| total timesteps         | 22656       |\
| value_loss              | 66114240.0  |\
-----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 7277.9663     |\
| ent_coef_loss           | -2505.0361    |\
| entropy                 | 102.56817     |\
| episodes                | 16            |\
| fps                     | 122           |\
| mean 100 episode reward | 141           |\
| n_updates               | 30109         |\
| policy_loss             | 33454544.0    |\
| qf1_loss                | 11555573000.0 |\
| qf2_loss                | 11526087000.0 |\
| time_elapsed            | 247           |\
| total timesteps         | 30208         |\
| value_loss              | 3072338700.0  |\
-------------------------------------------\
---------------------------------------------\
| current_lr              | 0.0003          |\
| ent_coef                | 70142.57        |\
| ent_coef_loss           | -3176.6372      |\
| entropy                 | 99.47441        |\
| episodes                | 20              |\
| fps                     | 121             |\
| mean 100 episode reward | 141             |\
| n_updates               | 37661           |\
| policy_loss             | 318101250.0     |\
| qf1_loss                | 1285527300000.0 |\
| qf2_loss                | 1295430800000.0 |\
| time_elapsed            | 309             |\
| total timesteps         | 37760           |\
| value_loss              | 1590252300000.0 |\
---------------------------------------------\
-----------------------------------------------\
| current_lr              | 0.0003            |\
| ent_coef                | 676071.5          |\
| ent_coef_loss           | -3797.45          |\
| entropy                 | 101.53691         |\
| episodes                | 24                |\
| fps                     | 121               |\
| mean 100 episode reward | 142               |\
| n_updates               | 45213             |\
| policy_loss             | 2965046000.0      |\
| qf1_loss                | 285944230000000.0 |\
| qf2_loss                | 257534260000000.0 |\
| time_elapsed            | 372               |\
| total timesteps         | 45312             |\
| value_loss              | 27340730000000.0  |\
-----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 6514480.5     |\
| ent_coef_loss           | -4413.091     |\
| entropy                 | 102.56817     |\
| episodes                | 28            |\
| fps                     | 121           |\
| mean 100 episode reward | 142           |\
| n_updates               | 52765         |\
| policy_loss             | 18804550000.0 |\
| qf1_loss                | 8.755433e+17  |\
| qf2_loss                | 4.7754824e+17 |\
| time_elapsed            | 436           |\
| total timesteps         | 52864         |\
| value_loss              | 1.5240994e+16 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 62597756.0    |\
| ent_coef_loss           | -5069.4463    |\
| entropy                 | 102.56817     |\
| episodes                | 32            |\
| fps                     | 121           |\
| mean 100 episode reward | 142           |\
| n_updates               | 60317         |\
| policy_loss             | 64417575000.0 |\
| qf1_loss                | 5.8834265e+19 |\
| qf2_loss                | 3.6705056e+19 |\
| time_elapsed            | 497           |\
| total timesteps         | 60416         |\
| value_loss              | 4.1441957e+19 |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 121      |\
| mean 100 episode reward | 138      |\
| n_updates               | 67869    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 558      |\
| total timesteps         | 67968    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 121      |\
| mean 100 episode reward | 124      |\
| n_updates               | 75421    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 619      |\
| total timesteps         | 75520    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 121      |\
| mean 100 episode reward | 113      |\
| n_updates               | 82973    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 683      |\
| total timesteps         | 83072    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 121      |\
| mean 100 episode reward | 104      |\
| n_updates               | 90525    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 745      |\
| total timesteps         | 90624    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 52       |\
| fps                     | 117      |\
| mean 100 episode reward | 95.5     |\
| n_updates               | 98077    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 832      |\
| total timesteps         | 98176    |\
| value_loss              | nan      |\
--------------------------------------\
actions (18880, 30)\
obs (18880, 181)\
rewards (18880,)\
episode_returns (10,)\
episode_starts (18880,)\
actions (18880, 30)\
obs (18880, 181)\
rewards (18880,)\
episode_returns (10,)\
episode_starts (18880,)\
Total trajectories: 10\
Total transitions: 18880\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.745 seconds\
computegrad\
done in 0.217 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       5.03          0\
         1       69.3       0.54\
         2       39.3      0.725\
         3       67.6       3.83\
         4       27.5       4.56\
         5        108       25.4\
         6        637       62.8\
         7   2.95e+03       75.6\
         8       48.4       85.4\
         9       46.4        131\
        10        103        134\
done in 0.414 seconds\
Expected: 2.223 Actual: 0.056\
Stepsize OK!\
vf\
done in 0.147 seconds\
sampling\
done in 4.411 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       4.14          0\
         1       95.2     0.0635\
         2       33.1       0.43\
         3       49.5      0.792\
         4        155        9.6\
         5        134       11.3\
         6        718       12.6\
         7        188       13.4\
         8        258       48.5\
         9        118       53.3\
        10   1.03e+03       69.7\
done in 0.049 seconds\
Expected: 1.574 Actual: 0.060\
Stepsize OK!\
vf\
done in 0.064 seconds\
sampling\
done in 4.372 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.78          0\
         1       13.6     0.0666\
         2       17.4      0.108\
         3       5.54      0.224\
         4       42.9      0.809\
         5       32.7       1.67\
         6       20.8       2.43\
         7         19       2.91\
         8        135       4.11\
         9        336       4.94\
        10       40.1        9.6\
done in 0.048 seconds\
Expected: 0.471 Actual: 0.068\
Stepsize OK!\
vf\
done in 0.068 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.74107 |           nan |           nan |           nan |       0.53613 |       0.00000\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 15.1        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.60188    |\
| explained_variance_t... | -0.00993    |\
| meankl                  | 0.010663524 |\
| optimgain               | 0.068364605 |\
| surrgain                | 0.068364605 |\
-----------------------------------------\
Training time (GAIL):  15.668031668663025  minutes\
======GAIL Validation from:  20160705 to  20161003\
GAIL Sharpe Ratio:  -0.05666270183587259\
======Trading from:  20161003 to  20170103\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x14ac87f98>\
previous_total_asset:1086454.330393657\
end_total_asset:1135905.95995392\
total_reward:49451.629560262896\
total_cost:  2975.6772908831235\
total trades:  1263\
Sharpe:  0.23797954355275366\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20161003\
======A2C Training========\
Training time (A2C):  1.9903820196787516  minutes\
======A2C Validation from:  20161003 to  20170103\
A2C Sharpe Ratio:  0.23906052668523747\
======PPO Training========\
Training time (PPO):  8.212320299943288  minutes\
======PPO Validation from:  20161003 to  20170103\
PPO Sharpe Ratio:  0.3971114896040041\
======DDPG Training========\
Training time (DDPG):  2.2806103308995564  minutes\
======DDPG Validation from:  20161003 to  20170103\
DDPG Sharpe Ratio:  0.43046837633714435\
======TD3 Training========\
Training time (TD3):  1.4255488991737366  minutes\
======TD3 Validation from:  20161003 to  20170103\
TD3 Sharpe Ratio:  0.5935464105224946\
======GAIL Training========\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 8.769572   |\
| ent_coef_loss           | -661.83923 |\
| entropy                 | 80.56817   |\
| episodes                | 4          |\
| fps                     | 57         |\
| mean 100 episode reward | 147        |\
| n_updates               | 7705       |\
| policy_loss             | 44979.89   |\
| qf1_loss                | 67471.664  |\
| qf2_loss                | 55905.086  |\
| reference_Q_mean        | 3.94       |\
| reference_Q_std         | 2.87       |\
| reference_action_mean   | 0.0817     |\
| reference_action_std    | 0.943      |\
| reference_actor_Q_mean  | 4.78       |\
| reference_actor_Q_std   | 2.96       |\
| rollout/Q_mean          | 2.33       |\
| rollout/actions_mean    | 0.052      |\
| rollout/actions_std     | 0.809      |\
| rollout/episode_steps   | 1.95e+03   |\
| rollout/episodes        | 5          |\
| rollout/return          | 144        |\
| rollout/return_history  | 144        |\
| time_elapsed            | 135        |\
| total timesteps         | 7804       |\
| total/duration          | 135        |\
| total/episodes          | 5          |\
| total/epochs            | 1          |\
| total/steps             | 9998       |\
| total/steps_per_second  | 74         |\
| train/loss_actor        | -4.19      |\
| train/loss_critic       | 1.2        |\
| train/param_noise_di... | 0          |\
| value_loss              | 152627.44  |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 90.74071   |\
| ent_coef_loss           | -1372.9082 |\
| entropy                 | 80.56817   |\
| episodes                | 8          |\
| fps                     | 59         |\
| mean 100 episode reward | 130        |\
| n_updates               | 15509      |\
| policy_loss             | 450831.1   |\
| qf1_loss                | 2131983.0  |\
| qf2_loss                | 2087804.8  |\
| time_elapsed            | 260        |\
| total timesteps         | 15608      |\
| value_loss              | 690204.6   |\
----------------------------------------\
-----------------------------------------\
| current_lr              | 0.0003      |\
| ent_coef                | 942.81744   |\
| ent_coef_loss           | -2084.3772  |\
| entropy                 | 80.56817    |\
| episodes                | 12          |\
| fps                     | 60          |\
| mean 100 episode reward | 125         |\
| n_updates               | 23313       |\
| policy_loss             | 4669790.5   |\
| qf1_loss                | 212962750.0 |\
| qf2_loss                | 201816940.0 |\
| time_elapsed            | 389         |\
| total timesteps         | 23412       |\
| value_loss              | 138241340.0 |\
-----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 9804.209      |\
| ent_coef_loss           | -2796.1614    |\
| entropy                 | 80.56817      |\
| episodes                | 16            |\
| fps                     | 67            |\
| mean 100 episode reward | 122           |\
| n_updates               | 31117         |\
| policy_loss             | 48172580.0    |\
| qf1_loss                | 51102458000.0 |\
| qf2_loss                | 51279655000.0 |\
| time_elapsed            | 462           |\
| total timesteps         | 31216         |\
| value_loss              | 7989519400.0  |\
-------------------------------------------\
---------------------------------------------\
| current_lr              | 0.0003          |\
| ent_coef                | 102032.41       |\
| ent_coef_loss           | -3504.5122      |\
| entropy                 | 80.56817        |\
| episodes                | 20              |\
| fps                     | 72              |\
| mean 100 episode reward | 120             |\
| n_updates               | 38921           |\
| policy_loss             | 493096640.0     |\
| qf1_loss                | 2974065600000.0 |\
| qf2_loss                | 2788101500000.0 |\
| time_elapsed            | 540             |\
| total timesteps         | 39020           |\
| value_loss              | 697305660000.0  |\
---------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 1061861.5          |\
| ent_coef_loss           | -4223.0615         |\
| entropy                 | 80.56817           |\
| episodes                | 24                 |\
| fps                     | 76                 |\
| mean 100 episode reward | 119                |\
| n_updates               | 46725              |\
| policy_loss             | 4791693000.0       |\
| qf1_loss                | 2528862000000000.0 |\
| qf2_loss                | 1300028600000000.0 |\
| time_elapsed            | 612                |\
| total timesteps         | 46824              |\
| value_loss              | 284632050000000.0  |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 11043619.0    |\
| ent_coef_loss           | -4944.6997    |\
| entropy                 | 80.56817      |\
| episodes                | 28            |\
| fps                     | 78            |\
| mean 100 episode reward | 118           |\
| n_updates               | 54529         |\
| policy_loss             | 22658775000.0 |\
| qf1_loss                | 6.1539075e+17 |\
| qf2_loss                | 9.9905415e+16 |\
| time_elapsed            | 692           |\
| total timesteps         | 54628         |\
| value_loss              | 3.1867844e+18 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 114295790.0   |\
| ent_coef_loss           | -5647.623     |\
| entropy                 | 80.56817      |\
| episodes                | 32            |\
| fps                     | 76            |\
| mean 100 episode reward | 118           |\
| n_updates               | 62333         |\
| policy_loss             | 81853050000.0 |\
| qf1_loss                | 5.640032e+18  |\
| qf2_loss                | 5.3622756e+17 |\
| time_elapsed            | 819           |\
| total timesteps         | 62432         |\
| value_loss              | 6.480197e+20  |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 78       |\
| mean 100 episode reward | 111      |\
| n_updates               | 70137    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 897      |\
| total timesteps         | 70236    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 77       |\
| mean 100 episode reward | 99.7     |\
| n_updates               | 77941    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 1008     |\
| total timesteps         | 78040    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 77       |\
| mean 100 episode reward | 90.6     |\
| n_updates               | 85745    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 1112     |\
| total timesteps         | 85844    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 76       |\
| mean 100 episode reward | 83.1     |\
| n_updates               | 93549    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 1218     |\
| total timesteps         | 93648    |\
| value_loss              | nan      |\
--------------------------------------\
actions (19510, 30)\
obs (19510, 181)\
rewards (19510,)\
episode_returns (10,)\
episode_starts (19510,)\
actions (19510, 30)\
obs (19510, 181)\
rewards (19510,)\
episode_returns (10,)\
episode_starts (19510,)\
Total trajectories: 10\
Total transitions: 19510\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 6.726 seconds\
computegrad\
done in 0.339 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.36          0\
         1      0.647     0.0678\
         2       1.25      0.522\
         3      0.934      0.608\
         4      0.835       1.49\
         5      0.537        1.6\
         6      0.365        2.4\
         7      0.546       2.97\
         8      0.586       4.18\
         9      0.266       4.28\
        10       3.72       5.11\
done in 0.652 seconds\
Expected: 0.213 Actual: 0.106\
Stepsize OK!\
vf\
done in 0.213 seconds\
sampling\
done in 6.556 seconds\
computegrad\
done in 0.013 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.84          0\
         1       3.41   0.000664\
         2       5.49     0.0128\
         3       2.48      0.107\
         4       3.09      0.126\
         5       21.8      0.171\
         6        287      0.246\
         7       24.2      0.554\
         8       12.4       0.76\
         9       2.54      0.859\
        10   1.09e+03       1.63\
done in 0.075 seconds\
Expected: 0.164 Actual: 0.057\
Stepsize OK!\
vf\
done in 0.087 seconds\
sampling\
done in 8.005 seconds\
computegrad\
done in 0.017 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.11          0\
         1       4.57      0.126\
         2        6.5      0.184\
         3         33       1.31\
         4       15.1       1.86\
         5       21.6       2.21\
         6       55.6       2.94\
         7        148       7.36\
         8       85.1       9.54\
         9       39.1       11.5\
        10       32.7       13.6\
done in 0.111 seconds\
Expected: 0.528 Actual: 0.073\
Stepsize OK!\
vf\
done in 0.206 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.77488 |           nan |           nan |           nan |       0.49414 |       0.00000\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 24.4        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.52672    |\
| explained_variance_t... | -0.0277     |\
| meankl                  | 0.012792848 |\
| optimgain               | 0.07257245  |\
| surrgain                | 0.07257245  |\
-----------------------------------------\
Training time (GAIL):  23.70877343416214  minutes\
======GAIL Validation from:  20161003 to  20170103\
GAIL Sharpe Ratio:  0.3693078340373123\
======Trading from:  20170103 to  20170404\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x147a10a20>\
previous_total_asset:1135905.95995392\
end_total_asset:1123918.8553613478\
total_reward:-11987.1045925722\
total_cost:  1415.7670299836068\
total trades:  1243\
Sharpe:  -0.04976197047435049\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20170103\
======A2C Training========\
Training time (A2C):  2.850550564130147  minutes\
======A2C Validation from:  20170103 to  20170404\
A2C Sharpe Ratio:  0.13856871837584653\
======PPO Training========\
Training time (PPO):  6.517055034637451  minutes\
======PPO Validation from:  20170103 to  20170404\
PPO Sharpe Ratio:  0.5401486046457911\
======DDPG Training========\
Training time (DDPG):  1.0985542813936868  minutes\
======DDPG Validation from:  20170103 to  20170404\
DDPG Sharpe Ratio:  0.4063011587475247\
======TD3 Training========\
Training time (TD3):  1.250152866045634  minutes\
======TD3 Validation from:  20170103 to  20170404\
TD3 Sharpe Ratio:  0.17693322673193934\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 10.365345 |\
| ent_coef_loss           | -922.1324 |\
| entropy                 | -7.431846 |\
| episodes                | 4         |\
| fps                     | 120       |\
| mean 100 episode reward | 208       |\
| n_updates               | 7957      |\
| policy_loss             | 60218.11  |\
| qf1_loss                | 142564.31 |\
| qf2_loss                | 143739.23 |\
| reference_Q_mean        | 2.24      |\
| reference_Q_std         | 2.92      |\
| reference_action_mean   | 0.127     |\
| reference_action_std    | 0.953     |\
| reference_actor_Q_mean  | 2.96      |\
| reference_actor_Q_std   | 2.99      |\
| rollout/Q_mean          | 1.33      |\
| rollout/actions_mean    | 0.0171    |\
| rollout/actions_std     | 0.802     |\
| rollout/episode_steps   | 2.01e+03  |\
| rollout/episodes        | 4         |\
| rollout/return          | 172       |\
| rollout/return_history  | 172       |\
| time_elapsed            | 67        |\
| total timesteps         | 8056      |\
| total/duration          | 64.8      |\
| total/episodes          | 4         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 154       |\
| train/loss_actor        | -2.92     |\
| train/loss_critic       | 1.21      |\
| train/param_noise_di... | 0         |\
| value_loss              | 215912.06 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 116.163055 |\
| ent_coef_loss           | -1875.8674 |\
| entropy                 | -7.431846  |\
| episodes                | 8          |\
| fps                     | 118        |\
| mean 100 episode reward | 206        |\
| n_updates               | 16013      |\
| policy_loss             | 728351.8   |\
| qf1_loss                | 3587296.5  |\
| qf2_loss                | 3408342.0  |\
| time_elapsed            | 135        |\
| total timesteps         | 16112      |\
| value_loss              | 17120744.0 |\
----------------------------------------\
-----------------------------------------\
| current_lr              | 0.0003      |\
| ent_coef                | 1301.5292   |\
| ent_coef_loss           | -2828.7214  |\
| entropy                 | -7.431846   |\
| episodes                | 12          |\
| fps                     | 119         |\
| mean 100 episode reward | 205         |\
| n_updates               | 24069       |\
| policy_loss             | 8107503.0   |\
| qf1_loss                | 781342700.0 |\
| qf2_loss                | 779394500.0 |\
| time_elapsed            | 202         |\
| total timesteps         | 24168       |\
| value_loss              | 208809380.0 |\
-----------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 14605.498      |\
| ent_coef_loss           | -3776.863      |\
| entropy                 | -7.431846      |\
| episodes                | 16             |\
| fps                     | 119            |\
| mean 100 episode reward | 205            |\
| n_updates               | 32125          |\
| policy_loss             | 90552720.0     |\
| qf1_loss                | 103985480000.0 |\
| qf2_loss                | 97212920000.0  |\
| time_elapsed            | 268            |\
| total timesteps         | 32224          |\
| value_loss              | 7615679500.0   |\
--------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 164021.44        |\
| ent_coef_loss           | -4726.629        |\
| entropy                 | -7.431846        |\
| episodes                | 20               |\
| fps                     | 120              |\
| mean 100 episode reward | 205              |\
| n_updates               | 40181            |\
| policy_loss             | 999252860.0      |\
| qf1_loss                | 17199874000000.0 |\
| qf2_loss                | 15737714000000.0 |\
| time_elapsed            | 335              |\
| total timesteps         | 40280            |\
| value_loss              | 559494600000.0   |\
----------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 1842139.8          |\
| ent_coef_loss           | -5694.4473         |\
| entropy                 | -7.431846          |\
| episodes                | 24                 |\
| fps                     | 119                |\
| mean 100 episode reward | 205                |\
| n_updates               | 48237              |\
| policy_loss             | 8748485000.0       |\
| qf1_loss                | 9.553189e+16       |\
| qf2_loss                | 2.7981306e+16      |\
| time_elapsed            | 402                |\
| total timesteps         | 48336              |\
| value_loss              | 4472728200000000.0 |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 20641328.0    |\
| ent_coef_loss           | -6629.3813    |\
| entropy                 | -7.431846     |\
| episodes                | 28            |\
| fps                     | 117           |\
| mean 100 episode reward | 205           |\
| n_updates               | 56293         |\
| policy_loss             | 34003036000.0 |\
| qf1_loss                | 9.079671e+18  |\
| qf2_loss                | 2.0126525e+18 |\
| time_elapsed            | 479           |\
| total timesteps         | 56392         |\
| value_loss              | 1.6066768e+19 |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 116      |\
| mean 100 episode reward | 205      |\
| n_updates               | 64349    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 551      |\
| total timesteps         | 64448    |\
| value_loss              | nan      |\
--------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 116      |\
| mean 100 episode reward | 182      |\
| n_updates               | 72405    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 620      |\
| total timesteps         | 72504    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 116      |\
| mean 100 episode reward | 164      |\
| n_updates               | 80461    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 691      |\
| total timesteps         | 80560    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 116      |\
| mean 100 episode reward | 149      |\
| n_updates               | 88517    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 761      |\
| total timesteps         | 88616    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 116      |\
| mean 100 episode reward | 136      |\
| n_updates               | 96573    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 832      |\
| total timesteps         | 96672    |\
| value_loss              | nan      |\
--------------------------------------\
actions (20140, 30)\
obs (20140, 181)\
rewards (20140,)\
episode_returns (10,)\
episode_starts (20140,)\
actions (20140, 30)\
obs (20140, 181)\
rewards (20140,)\
episode_returns (10,)\
episode_starts (20140,)\
Total trajectories: 10\
Total transitions: 20140\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.207 seconds\
computegrad\
done in 0.206 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.21          0\
         1      0.869     0.0828\
         2       2.92      0.161\
         3      0.743      0.443\
         4      0.646       1.48\
         5      0.724       3.66\
         6       2.79       4.06\
         7       1.49       6.83\
         8       5.85       7.64\
         9      0.676       7.96\
        10       4.34       8.71\
done in 0.392 seconds\
Expected: 0.283 Actual: 0.131\
violated KL constraint. shrinking step.\
Expected: 0.283 Actual: 0.065\
Stepsize OK!\
vf\
done in 0.148 seconds\
sampling\
done in 4.233 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.21          0\
         1       6.66      0.121\
         2       5.49      0.187\
         3       11.6      0.428\
         4       42.6        1.3\
         5       24.5       2.38\
         6       25.8       2.76\
         7       81.2       6.09\
         8       15.1       8.15\
         9       21.5       8.61\
        10       95.6       17.7\
done in 0.051 seconds\
Expected: 0.600 Actual: 0.069\
Stepsize OK!\
vf\
done in 0.071 seconds\
sampling\
done in 4.082 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       3.99          0\
         1        178    0.00141\
         2       9.46      0.269\
         3        159      0.583\
         4       46.8      0.585\
         5         51       5.13\
         6   1.07e+05       7.35\
         7       47.7       9.31\
         8        165       26.1\
         9       53.9       26.1\
        10        149       35.4\
done in 0.053 seconds\
Expected: 1.049 Actual: 0.057\
Stepsize OK!\
vf\
done in 0.101 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.99819 |           nan |           nan |           nan |       0.31152 |       0.00000\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\
  out=out, **kwargs)\
/Users/mac/opt/anaconda3/envs/Deep-Reinforcement-Learning/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\
  ret = ret.dtype.type(ret / rcount)\
-----------------------------------------\
| EpThisIter              | 0           |\
| EpTrueRewMean           | nan         |\
| EpisodesSoFar           | 0           |\
| TimeElapsed             | 14.1        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.564407   |\
| explained_variance_t... | -0.121      |\
| meankl                  | 0.011381092 |\
| optimgain               | 0.056710415 |\
| surrgain                | 0.056710415 |\
-----------------------------------------\
Training time (GAIL):  15.392552828788757  minutes\
======GAIL Validation from:  20170103 to  20170404\
GAIL Sharpe Ratio:  0.1697961946235441\
======Trading from:  20170404 to  20170705\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x1437488d0>\
previous_total_asset:1123918.8553613478\
end_total_asset:1127399.082286364\
total_reward:3480.226925016148\
total_cost:  6221.829736044054\
total trades:  1270\
Sharpe:  0.026752698107355338\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20170404\
======A2C Training========\
Training time (A2C):  1.9384580453236897  minutes\
======A2C Validation from:  20170404 to  20170705\
A2C Sharpe Ratio:  0.2775434003213178\
======PPO Training========\
Training time (PPO):  6.934277045726776  minutes\
======PPO Validation from:  20170404 to  20170705\
PPO Sharpe Ratio:  0.2458422357934561\
======DDPG Training========\
Training time (DDPG):  1.3542508363723755  minutes\
======DDPG Validation from:  20170404 to  20170705\
DDPG Sharpe Ratio:  0.25605178986715305\
======TD3 Training========\
Training time (TD3):  2.214614951610565  minutes\
======TD3 Validation from:  20170404 to  20170705\
TD3 Sharpe Ratio:  0.08325341300703908\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 10.391874 |\
| ent_coef_loss           | -817.0142 |\
| entropy                 | 36.568157 |\
| episodes                | 4         |\
| fps                     | 91        |\
| mean 100 episode reward | 154       |\
| n_updates               | 8209      |\
| policy_loss             | 57400.5   |\
| qf1_loss                | 251958.2  |\
| qf2_loss                | 99964.16  |\
| reference_Q_mean        | 3.51      |\
| reference_Q_std         | 4.04      |\
| reference_action_mean   | -0.0502   |\
| reference_action_std    | 0.989     |\
| reference_actor_Q_mean  | 4.28      |\
| reference_actor_Q_std   | 4.09      |\
| rollout/Q_mean          | 2.4       |\
| rollout/actions_mean    | 0.0447    |\
| rollout/actions_std     | 0.805     |\
| rollout/episode_steps   | 2.08e+03  |\
| rollout/episodes        | 4         |\
| rollout/return          | 271       |\
| rollout/return_history  | 271       |\
| time_elapsed            | 91        |\
| total timesteps         | 8308      |\
| total/duration          | 80.1      |\
| total/episodes          | 4         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 125       |\
| train/loss_actor        | -4.58     |\
| train/loss_critic       | 2.08      |\
| train/param_noise_di... | 0         |\
| value_loss              | 274381.2  |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 125.57987  |\
| ent_coef_loss           | -1691.947  |\
| entropy                 | 36.568157  |\
| episodes                | 8          |\
| fps                     | 93         |\
| mean 100 episode reward | 167        |\
| n_updates               | 16517      |\
| policy_loss             | 706385.4   |\
| qf1_loss                | 37594484.0 |\
| qf2_loss                | 13881268.0 |\
| time_elapsed            | 177        |\
| total timesteps         | 16616      |\
| value_loss              | 8815005.0  |\
----------------------------------------\
-----------------------------------------\
| current_lr              | 0.0003      |\
| ent_coef                | 1517.5797   |\
| ent_coef_loss           | -2555.4668  |\
| entropy                 | 36.568157   |\
| episodes                | 12          |\
| fps                     | 86          |\
| mean 100 episode reward | 172         |\
| n_updates               | 24825       |\
| policy_loss             | 8516506.0   |\
| qf1_loss                | 843134700.0 |\
| qf2_loss                | 796847800.0 |\
| time_elapsed            | 288         |\
| total timesteps         | 24924       |\
| value_loss              | 395829760.0 |\
-----------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 18366.7        |\
| ent_coef_loss           | -3433.6538     |\
| entropy                 | 36.568157      |\
| episodes                | 16             |\
| fps                     | 90             |\
| mean 100 episode reward | 174            |\
| n_updates               | 33133          |\
| policy_loss             | 102125864.0    |\
| qf1_loss                | 131072150000.0 |\
| qf2_loss                | 126598400000.0 |\
| time_elapsed            | 366            |\
| total timesteps         | 33232          |\
| value_loss              | 19282080000.0  |\
--------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 222431.64        |\
| ent_coef_loss           | -4298.164        |\
| entropy                 | 36.568157        |\
| episodes                | 20               |\
| fps                     | 89               |\
| mean 100 episode reward | 175              |\
| n_updates               | 41441            |\
| policy_loss             | 1209997000.0     |\
| qf1_loss                | 26086700000000.0 |\
| qf2_loss                | 27224212000000.0 |\
| time_elapsed            | 462              |\
| total timesteps         | 41540            |\
| value_loss              | 962040700000.0   |\
----------------------------------------------\
-----------------------------------------------\
| current_lr              | 0.0003            |\
| ent_coef                | 2693982.0         |\
| ent_coef_loss           | -5166.7153        |\
| entropy                 | 36.568157         |\
| episodes                | 24                |\
| fps                     | 90                |\
| mean 100 episode reward | 176               |\
| n_updates               | 49749             |\
| policy_loss             | 10713552000.0     |\
| qf1_loss                | 1.129038e+17      |\
| qf2_loss                | 1.615416e+17      |\
| time_elapsed            | 548               |\
| total timesteps         | 49848             |\
| value_loss              | 621854760000000.0 |\
-----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 32518118.0    |\
| ent_coef_loss           | -6048.757     |\
| entropy                 | 36.568157     |\
| episodes                | 28            |\
| fps                     | 90            |\
| mean 100 episode reward | 176           |\
| n_updates               | 58057         |\
| policy_loss             | 43839190000.0 |\
| qf1_loss                | 2.3932942e+19 |\
| qf2_loss                | 3.0974544e+19 |\
| time_elapsed            | 645           |\
| total timesteps         | 58156         |\
| value_loss              | 8.128543e+18  |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 92       |\
| mean 100 episode reward | 177      |\
| n_updates               | 66365    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 714      |\
| total timesteps         | 66464    |\
| value_loss              | nan      |\
--------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 95       |\
| mean 100 episode reward | 157      |\
| n_updates               | 74673    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 779      |\
| total timesteps         | 74772    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 98       |\
| mean 100 episode reward | 142      |\
| n_updates               | 82981    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 846      |\
| total timesteps         | 83080    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 99       |\
| mean 100 episode reward | 129      |\
| n_updates               | 91289    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 915      |\
| total timesteps         | 91388    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 48       |\
| fps                     | 100      |\
| mean 100 episode reward | 118      |\
| n_updates               | 99597    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 987      |\
| total timesteps         | 99696    |\
| value_loss              | nan      |\
--------------------------------------\
actions (20770, 30)\
obs (20770, 181)\
rewards (20770,)\
episode_returns (10,)\
episode_starts (20770,)\
actions (20770, 30)\
obs (20770, 181)\
rewards (20770,)\
episode_returns (10,)\
episode_starts (20770,)\
Total trajectories: 10\
Total transitions: 20770\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 5.417 seconds\
computegrad\
done in 0.279 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.16          0\
         1       7.02   0.000209\
         2       7.82     0.0309\
         3       1.15     0.0856\
         4        156     0.0904\
         5        4.9      0.299\
         6      0.911      0.566\
         7       1.77      0.566\
         8       2.45      0.767\
         9   2.22e+03       1.53\
        10       1.49       1.67\
done in 0.581 seconds\
Expected: 0.144 Actual: 0.159\
Stepsize OK!\
vf\
done in 0.198 seconds\
sampling\
done in 5.623 seconds\
computegrad\
done in 0.011 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       5.29          0\
         1   1.01e+03     0.0797\
         2        133       0.14\
         3       44.9       0.45\
         4       62.9        1.2\
         5   8.43e+03        1.6\
         6        106       2.81\
         7        290       4.68\
         8   3.66e+03       12.4\
         9        323       12.5\
        10        660       13.5\
done in 0.063 seconds\
Expected: 0.743 Actual: 0.074\
Stepsize OK!\
vf\
done in 0.082 seconds\
sampling\
done in 5.729 seconds\
computegrad\
done in 0.014 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       25.9          0\
         1       60.5      0.285\
         2        480      0.791\
         3   1.68e+03       11.2\
         4      2e+03       13.3\
         5   2.68e+03       70.8\
         6   1.18e+03       80.7\
         7   2.34e+03        212\
         8        780        248\
         9   2.21e+03        250\
        10   1.39e+04        264\
done in 0.065 seconds\
Expected: 4.665 Actual: 0.053\
Stepsize OK!\
vf\
done in 0.086 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.78983 |           nan |           nan |           nan |       0.46777 |       0.00000\
------------------------------------------\
| EpLenMean               | 2.08e+03     |\
| EpRewMean               | 1416.602     |\
| EpThisIter              | 1            |\
| EpTrueRewMean           | 101          |\
| EpisodesSoFar           | 1            |\
| TimeElapsed             | 18.9         |\
| TimestepsSoFar          | 1024         |\
| entloss                 | 0.0          |\
| entropy                 | 42.513325    |\
| explained_variance_t... | -0.203       |\
| meankl                  | 0.0061321706 |\
| optimgain               | 0.05274151   |\
| surrgain                | 0.05274151   |\
------------------------------------------\
Training time (GAIL):  18.014884519577027  minutes\
======GAIL Validation from:  20170404 to  20170705\
GAIL Sharpe Ratio:  0.3998277405360025\
======Trading from:  20170705 to  20171003\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x14b964198>\
previous_total_asset:1127399.082286364\
end_total_asset:1169340.7288653173\
total_reward:41941.64657895337\
total_cost:  10086.789877494895\
total trades:  1728\
Sharpe:  0.40048116379254867\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20170705\
======A2C Training========\
Training time (A2C):  2.048161784807841  minutes\
======A2C Validation from:  20170705 to  20171003\
A2C Sharpe Ratio:  -0.004681581362213006\
======PPO Training========\
Training time (PPO):  6.615915616353353  minutes\
======PPO Validation from:  20170705 to  20171003\
PPO Sharpe Ratio:  0.12407992235887164\
======DDPG Training========\
Training time (DDPG):  1.1286024967829387  minutes\
======DDPG Validation from:  20170705 to  20171003\
DDPG Sharpe Ratio:  0.03888213272532996\
======TD3 Training========\
Training time (TD3):  1.3183122158050538  minutes\
======TD3 Validation from:  20170705 to  20171003\
TD3 Sharpe Ratio:  0.14360860166705677\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 11.341831 |\
| ent_coef_loss           | -794.214  |\
| entropy                 | 58.568153 |\
| episodes                | 4         |\
| fps                     | 116       |\
| mean 100 episode reward | 162       |\
| n_updates               | 8461      |\
| policy_loss             | 61221.785 |\
| qf1_loss                | 35550.035 |\
| qf2_loss                | 64394.906 |\
| reference_Q_mean        | 5.22      |\
| reference_Q_std         | 3.12      |\
| reference_action_mean   | -0.308    |\
| reference_action_std    | 0.925     |\
| reference_actor_Q_mean  | 6.19      |\
| reference_actor_Q_std   | 3.28      |\
| rollout/Q_mean          | 3.88      |\
| rollout/actions_mean    | -0.145    |\
| rollout/actions_std     | 0.818     |\
| rollout/episode_steps   | 2.14e+03  |\
| rollout/episodes        | 4         |\
| rollout/return          | 226       |\
| rollout/return_history  | 226       |\
| time_elapsed            | 73        |\
| total timesteps         | 8560      |\
| total/duration          | 66.7      |\
| total/episodes          | 4         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 150       |\
| train/loss_actor        | -6.48     |\
| train/loss_critic       | 2.41      |\
| train/param_noise_di... | 0         |\
| value_loss              | 193928.66 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 147.52682  |\
| ent_coef_loss           | -1640.5542 |\
| entropy                 | 56.505653  |\
| episodes                | 8          |\
| fps                     | 116        |\
| mean 100 episode reward | 171        |\
| n_updates               | 17021      |\
| policy_loss             | 780644.75  |\
| qf1_loss                | 7606564.0  |\
| qf2_loss                | 7628440.0  |\
| time_elapsed            | 147        |\
| total timesteps         | 17120      |\
| value_loss              | 6576796.5  |\
----------------------------------------\
-----------------------------------------\
| current_lr              | 0.0003      |\
| ent_coef                | 1922.933    |\
| ent_coef_loss           | -2473.3267  |\
| entropy                 | 58.568153   |\
| episodes                | 12          |\
| fps                     | 116         |\
| mean 100 episode reward | 176         |\
| n_updates               | 25581       |\
| policy_loss             | 10185342.0  |\
| qf1_loss                | 980177800.0 |\
| qf2_loss                | 944136300.0 |\
| time_elapsed            | 220         |\
| total timesteps         | 25680       |\
| value_loss              | 275396860.0 |\
-----------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 25069.277      |\
| ent_coef_loss           | -3315.6958     |\
| entropy                 | 58.568153      |\
| episodes                | 16             |\
| fps                     | 116            |\
| mean 100 episode reward | 179            |\
| n_updates               | 34141          |\
| policy_loss             | 130906980.0    |\
| qf1_loss                | 176085480000.0 |\
| qf2_loss                | 179670830000.0 |\
| time_elapsed            | 293            |\
| total timesteps         | 34240          |\
| value_loss              | 12778141000.0  |\
--------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 326859.5         |\
| ent_coef_loss           | -4181.2827       |\
| entropy                 | 56.505653        |\
| episodes                | 20               |\
| fps                     | 116              |\
| mean 100 episode reward | 180              |\
| n_updates               | 42701            |\
| policy_loss             | 1669044700.0     |\
| qf1_loss                | 56543303000000.0 |\
| qf2_loss                | 64318820000000.0 |\
| time_elapsed            | 368              |\
| total timesteps         | 42800            |\
| value_loss              | 18183319000000.0 |\
----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 4260151.5     |\
| ent_coef_loss           | -5020.0703    |\
| entropy                 | 56.505653     |\
| episodes                | 24            |\
| fps                     | 116           |\
| mean 100 episode reward | 181           |\
| n_updates               | 51261         |\
| policy_loss             | 13964704000.0 |\
| qf1_loss                | 2.9607694e+17 |\
| qf2_loss                | 7.2861804e+17 |\
| time_elapsed            | 441           |\
| total timesteps         | 51360         |\
| value_loss              | 1.3814133e+16 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 55427764.0    |\
| ent_coef_loss           | -5851.4824    |\
| entropy                 | 56.505653     |\
| episodes                | 28            |\
| fps                     | 116           |\
| mean 100 episode reward | 182           |\
| n_updates               | 59821         |\
| policy_loss             | 59109966000.0 |\
| qf1_loss                | 3.2599328e+19 |\
| qf2_loss                | 6.0762443e+19 |\
| time_elapsed            | 514           |\
| total timesteps         | 59920         |\
| value_loss              | 5.21976e+19   |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 116      |\
| mean 100 episode reward | 177      |\
| n_updates               | 68381    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 588      |\
| total timesteps         | 68480    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 117      |\
| mean 100 episode reward | 157      |\
| n_updates               | 76941    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 657      |\
| total timesteps         | 77040    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 117      |\
| mean 100 episode reward | 141      |\
| n_updates               | 85501    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 727      |\
| total timesteps         | 85600    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 117      |\
| mean 100 episode reward | 128      |\
| n_updates               | 94061    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 801      |\
| total timesteps         | 94160    |\
| value_loss              | nan      |\
--------------------------------------\
actions (21400, 30)\
obs (21400, 181)\
rewards (21400,)\
episode_returns (10,)\
episode_starts (21400,)\
actions (21400, 30)\
obs (21400, 181)\
rewards (21400,)\
episode_returns (10,)\
episode_starts (21400,)\
Total trajectories: 10\
Total transitions: 21400\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.294 seconds\
computegrad\
done in 0.225 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.19          0\
         1       1.01      0.107\
         2      0.592      0.479\
         3      0.515       1.32\
         4       1.32        2.4\
         5       1.52       2.81\
         6       1.22        3.5\
         7      0.582       4.04\
         8      0.573        5.1\
         9      0.728       6.03\
        10      0.728       6.26\
done in 0.457 seconds\
Expected: 0.243 Actual: 0.132\
violated KL constraint. shrinking step.\
Expected: 0.243 Actual: 0.076\
Stepsize OK!\
vf\
done in 0.158 seconds\
sampling\
done in 4.231 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.65          0\
         1       13.1    0.00092\
         2       4.04     0.0517\
         3       2.12      0.134\
         4       6.53      0.191\
         5       4.02      0.287\
         6       25.1      0.289\
         7       6.94      0.544\
         8       1.52      0.694\
         9        425      0.748\
        10         20       1.09\
done in 0.051 seconds\
Expected: 0.133 Actual: 0.074\
Stepsize OK!\
vf\
done in 0.073 seconds\
sampling\
done in 4.333 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.66          0\
         1       62.4     0.0103\
         2       6.66     0.0948\
         3       2.82      0.165\
         4       5.98       1.51\
         5       8.66       1.51\
         6         23       5.17\
         7        7.5       7.41\
         8   7.76e+03       10.4\
         9       18.5       14.7\
        10       22.5         18\
done in 0.052 seconds\
Expected: 0.551 Actual: 0.073\
Stepsize OK!\
vf\
done in 0.067 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.80685 |           nan |           nan |           nan |       0.43945 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.14e+03    |\
| EpRewMean               | 1982.1212   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 119         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.5        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.528946   |\
| explained_variance_t... | -0.0459     |\
| meankl                  | 0.009649015 |\
| optimgain               | 0.07276251  |\
| surrgain                | 0.07276251  |\
-----------------------------------------\
Training time (GAIL):  15.266683181126913  minutes\
======GAIL Validation from:  20170705 to  20171003\
GAIL Sharpe Ratio:  0.46881433901626196\
======Trading from:  20171003 to  20180103\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x14f332c88>\
previous_total_asset:1169340.7288653173\
end_total_asset:1286479.967855947\
total_reward:117139.23899062979\
total_cost:  9453.797180478608\
total trades:  1694\
Sharpe:  0.6491108573338848\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20171003\
======A2C Training========\
Training time (A2C):  1.889744238058726  minutes\
======A2C Validation from:  20171003 to  20180103\
A2C Sharpe Ratio:  0.3617773668246809\
======PPO Training========\
Training time (PPO):  6.499719953536987  minutes\
======PPO Validation from:  20171003 to  20180103\
PPO Sharpe Ratio:  0.4424940447000427\
======DDPG Training========\
Training time (DDPG):  1.1112995862960815  minutes\
======DDPG Validation from:  20171003 to  20180103\
DDPG Sharpe Ratio:  0.560222443936381\
======TD3 Training========\
Training time (TD3):  1.2453389644622803  minutes\
======TD3 Validation from:  20171003 to  20180103\
TD3 Sharpe Ratio:  0.5944152254898035\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 12.522961 |\
| ent_coef_loss           | -998.307  |\
| entropy                 | -8.119347 |\
| episodes                | 4         |\
| fps                     | 119       |\
| mean 100 episode reward | 183       |\
| n_updates               | 8713      |\
| policy_loss             | 75496.22  |\
| qf1_loss                | 51231.39  |\
| qf2_loss                | 64916.633 |\
| reference_Q_mean        | 2.14      |\
| reference_Q_std         | 2.29      |\
| reference_action_mean   | 0.188     |\
| reference_action_std    | 0.941     |\
| reference_actor_Q_mean  | 2.77      |\
| reference_actor_Q_std   | 2.28      |\
| rollout/Q_mean          | 1.42      |\
| rollout/actions_mean    | 0.134     |\
| rollout/actions_std     | 0.796     |\
| rollout/episode_steps   | 2.2e+03   |\
| rollout/episodes        | 4         |\
| rollout/return          | 158       |\
| rollout/return_history  | 158       |\
| time_elapsed            | 73        |\
| total timesteps         | 8812      |\
| total/duration          | 65.7      |\
| total/episodes          | 4         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 152       |\
| train/loss_actor        | -2.77     |\
| train/loss_critic       | 1.11      |\
| train/param_noise_di... | 0         |\
| value_loss              | 39777.457 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 176.02737  |\
| ent_coef_loss           | -2042.8586 |\
| entropy                 | -8.119346  |\
| episodes                | 8          |\
| fps                     | 120        |\
| mean 100 episode reward | 189        |\
| n_updates               | 17525      |\
| policy_loss             | 1104060.9  |\
| qf1_loss                | 25628016.0 |\
| qf2_loss                | 23260760.0 |\
| time_elapsed            | 146        |\
| total timesteps         | 17624      |\
| value_loss              | 2416217.0  |\
----------------------------------------\
------------------------------------------\
| current_lr              | 0.0003       |\
| ent_coef                | 2474.4587    |\
| ent_coef_loss           | -3086.8303   |\
| entropy                 | -7.7755957   |\
| episodes                | 12           |\
| fps                     | 120          |\
| mean 100 episode reward | 191          |\
| n_updates               | 26337        |\
| policy_loss             | 15539658.0   |\
| qf1_loss                | 2932518400.0 |\
| qf2_loss                | 2702216000.0 |\
| time_elapsed            | 218          |\
| total timesteps         | 26436        |\
| value_loss              | 1906433500.0 |\
------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 34855.234      |\
| ent_coef_loss           | -4124.674      |\
| entropy                 | -7.775596      |\
| episodes                | 16             |\
| fps                     | 121            |\
| mean 100 episode reward | 192            |\
| n_updates               | 35149          |\
| policy_loss             | 215476290.0    |\
| qf1_loss                | 540587520000.0 |\
| qf2_loss                | 471917460000.0 |\
| time_elapsed            | 291            |\
| total timesteps         | 35248          |\
| value_loss              | 109923650000.0 |\
--------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 490984.1           |\
| ent_coef_loss           | -5162.672          |\
| entropy                 | -7.4318457         |\
| episodes                | 20                 |\
| fps                     | 121                |\
| mean 100 episode reward | 192                |\
| n_updates               | 43961              |\
| policy_loss             | 2852544000.0       |\
| qf1_loss                | 1605085100000000.0 |\
| qf2_loss                | 350878860000000.0  |\
| time_elapsed            | 363                |\
| total timesteps         | 44060              |\
| value_loss              | 24866657000000.0   |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 6916551.0     |\
| ent_coef_loss           | -6218.253     |\
| entropy                 | -7.4318457    |\
| episodes                | 24            |\
| fps                     | 121           |\
| mean 100 episode reward | 193           |\
| n_updates               | 52773         |\
| policy_loss             | 15680416000.0 |\
| qf1_loss                | 5.2578217e+18 |\
| qf2_loss                | 2.2247475e+17 |\
| time_elapsed            | 436           |\
| total timesteps         | 52872         |\
| value_loss              | 1.4059771e+18 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 96862540.0    |\
| ent_coef_loss           | -7249.798     |\
| entropy                 | -7.4318457    |\
| episodes                | 28            |\
| fps                     | 121           |\
| mean 100 episode reward | 193           |\
| n_updates               | 61585         |\
| policy_loss             | 71560180000.0 |\
| qf1_loss                | 7.2959748e+19 |\
| qf2_loss                | 2.6076867e+18 |\
| time_elapsed            | 508           |\
| total timesteps         | 61684         |\
| value_loss              | 7.0989045e+20 |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 121      |\
| mean 100 episode reward | 181      |\
| n_updates               | 70397    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 579      |\
| total timesteps         | 70496    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 122      |\
| mean 100 episode reward | 161      |\
| n_updates               | 79209    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 648      |\
| total timesteps         | 79308    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 122      |\
| mean 100 episode reward | 145      |\
| n_updates               | 88021    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 718      |\
| total timesteps         | 88120    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 123      |\
| mean 100 episode reward | 132      |\
| n_updates               | 96833    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 787      |\
| total timesteps         | 96932    |\
| value_loss              | nan      |\
--------------------------------------\
actions (22030, 30)\
obs (22030, 181)\
rewards (22030,)\
episode_returns (10,)\
episode_starts (22030,)\
actions (22030, 30)\
obs (22030, 181)\
rewards (22030,)\
episode_returns (10,)\
episode_starts (22030,)\
Total trajectories: 10\
Total transitions: 22030\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.659 seconds\
computegrad\
done in 0.224 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.11          0\
         1       1.46     0.0681\
         2       1.23     0.0848\
         3       1.96      0.519\
         4      0.669      0.533\
         5      0.917       1.46\
         6        1.5       1.79\
         7        1.7       2.19\
         8       3.13       2.24\
         9       18.3       2.51\
        10      0.313       3.35\
done in 0.451 seconds\
Expected: 0.184 Actual: 0.100\
Stepsize OK!\
vf\
done in 0.158 seconds\
sampling\
done in 4.400 seconds\
computegrad\
done in 0.012 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       4.45          0\
         1        302    0.00312\
         2       90.6     0.0238\
         3       66.5      0.166\
         4       38.4      0.276\
         5       50.8      0.277\
         6       42.8      0.377\
         7       60.3      0.567\
         8   3.07e+03       0.61\
         9       67.7      0.734\
        10        855       2.04\
done in 0.064 seconds\
Expected: 0.270 Actual: 0.057\
Stepsize OK!\
vf\
done in 0.069 seconds\
sampling\
done in 4.590 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       16.5          0\
         1       3.55   3.79e-05\
         2        107     0.0121\
         3       90.8      0.179\
         4        418      0.188\
         5       6.88      0.191\
         6       55.2      0.574\
         7   3.36e+04      0.776\
         8        285       2.74\
         9   2.33e+03        3.1\
        10   3.78e+03       5.62\
done in 0.052 seconds\
Expected: 0.396 Actual: 0.054\
violated KL constraint. shrinking step.\
Expected: 0.396 Actual: 0.030\
Stepsize OK!\
vf\
done in 0.069 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.67917 |           nan |           nan |           nan |       0.57324 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.2e+03     |\
| EpRewMean               | 2459.1167   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 145         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 15.4        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.566715   |\
| explained_variance_t... | 0.0252      |\
| meankl                  | 0.01231306  |\
| optimgain               | 0.029984295 |\
| surrgain                | 0.029984295 |\
-----------------------------------------\
Training time (GAIL):  14.698071813583374  minutes\
======GAIL Validation from:  20171003 to  20180103\
GAIL Sharpe Ratio:  0.4805605227419434\
======Trading from:  20180103 to  20180405\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x1426a60f0>\
previous_total_asset:1286479.967855947\
end_total_asset:1296852.8118182393\
total_reward:10372.843962292187\
total_cost:  2479.497205977821\
total trades:  266\
Sharpe:  0.047785720514277846\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20180103\
======A2C Training========\
Training time (A2C):  1.8642679691314696  minutes\
======A2C Validation from:  20180103 to  20180405\
A2C Sharpe Ratio:  -0.0015367373423918012\
======PPO Training========\
Training time (PPO):  6.520430568854014  minutes\
======PPO Validation from:  20180103 to  20180405\
PPO Sharpe Ratio:  0.06829931380032296\
======DDPG Training========\
Training time (DDPG):  1.1067050655682882  minutes\
======DDPG Validation from:  20180103 to  20180405\
DDPG Sharpe Ratio:  -0.05588340297176881\
======TD3 Training========\
Training time (TD3):  1.2813848336537679  minutes\
======TD3 Validation from:  20180103 to  20180405\
TD3 Sharpe Ratio:  -0.02921280774485695\
======GAIL Training========\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 13.730764  |\
| ent_coef_loss           | -796.45807 |\
| entropy                 | 80.56817   |\
| episodes                | 4          |\
| fps                     | 117        |\
| mean 100 episode reward | 208        |\
| n_updates               | 8965       |\
| policy_loss             | 67911.44   |\
| qf1_loss                | 61240.336  |\
| qf2_loss                | 76641.945  |\
| reference_Q_mean        | 3.85       |\
| reference_Q_std         | 3.05       |\
| reference_action_mean   | 0.0375     |\
| reference_action_std    | 0.985      |\
| reference_actor_Q_mean  | 4.46       |\
| reference_actor_Q_std   | 3.11       |\
| rollout/Q_mean          | 2.84       |\
| rollout/actions_mean    | 0.0203     |\
| rollout/actions_std     | 0.825      |\
| rollout/episode_steps   | 2.27e+03   |\
| rollout/episodes        | 4          |\
| rollout/return          | 166        |\
| rollout/return_history  | 166        |\
| time_elapsed            | 77         |\
| total timesteps         | 9064       |\
| total/duration          | 65.5       |\
| total/episodes          | 4          |\
| total/epochs            | 1          |\
| total/steps             | 9998       |\
| total/steps_per_second  | 153        |\
| train/loss_actor        | -4.52      |\
| train/loss_critic       | 1.26       |\
| train/param_noise_di... | 0          |\
| value_loss              | 45390.027  |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 203.55827  |\
| ent_coef_loss           | -1501.7916 |\
| entropy                 | 102.56817  |\
| episodes                | 8          |\
| fps                     | 117        |\
| mean 100 episode reward | 206        |\
| n_updates               | 18029      |\
| policy_loss             | 943088.1   |\
| qf1_loss                | 27437224.0 |\
| qf2_loss                | 12718894.0 |\
| time_elapsed            | 154        |\
| total timesteps         | 18128      |\
| value_loss              | 48671560.0 |\
----------------------------------------\
------------------------------------------\
| current_lr              | 0.0003       |\
| ent_coef                | 3086.6792    |\
| ent_coef_loss           | -2268.9639   |\
| entropy                 | 102.56817    |\
| episodes                | 12           |\
| fps                     | 117          |\
| mean 100 episode reward | 215          |\
| n_updates               | 27093        |\
| policy_loss             | 14216886.0   |\
| qf1_loss                | 1531384200.0 |\
| qf2_loss                | 1296971300.0 |\
| time_elapsed            | 232          |\
| total timesteps         | 27192        |\
| value_loss              | 676929800.0  |\
------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 46877.133      |\
| ent_coef_loss           | -3028.753      |\
| entropy                 | 102.56817      |\
| episodes                | 16             |\
| fps                     | 116            |\
| mean 100 episode reward | 220            |\
| n_updates               | 36157          |\
| policy_loss             | 214134080.0    |\
| qf1_loss                | 385928270000.0 |\
| qf2_loss                | 396827750000.0 |\
| time_elapsed            | 311            |\
| total timesteps         | 36256          |\
| value_loss              | 64293650000.0  |\
--------------------------------------------\
-----------------------------------------------\
| current_lr              | 0.0003            |\
| ent_coef                | 712020.2          |\
| ent_coef_loss           | -3800.7827        |\
| entropy                 | 102.56817         |\
| episodes                | 20                |\
| fps                     | 116               |\
| mean 100 episode reward | 222               |\
| n_updates               | 45221             |\
| policy_loss             | 3121239000.0      |\
| qf1_loss                | 368253380000000.0 |\
| qf2_loss                | 428907900000000.0 |\
| time_elapsed            | 388               |\
| total timesteps         | 45320             |\
| value_loss              | 11490034000000.0  |\
-----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 10809441.0    |\
| ent_coef_loss           | -4570.961     |\
| entropy                 | 102.56817     |\
| episodes                | 24            |\
| fps                     | 116           |\
| mean 100 episode reward | 224           |\
| n_updates               | 54285         |\
| policy_loss             | 25075532000.0 |\
| qf1_loss                | 1.3968479e+18 |\
| qf2_loss                | 2.6468269e+18 |\
| time_elapsed            | 468           |\
| total timesteps         | 54384         |\
| value_loss              | 2.4919825e+17 |\
-------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 163149760.0    |\
| ent_coef_loss           | -5321.8076     |\
| entropy                 | 102.56817      |\
| episodes                | 28             |\
| fps                     | 115            |\
| mean 100 episode reward | 225            |\
| n_updates               | 63349          |\
| policy_loss             | 111492640000.0 |\
| qf1_loss                | 5.884722e+19   |\
| qf2_loss                | 1.0085963e+20  |\
| time_elapsed            | 547            |\
| total timesteps         | 63448          |\
| value_loss              | 7.192509e+20   |\
--------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 116      |\
| mean 100 episode reward | 204      |\
| n_updates               | 72413    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 621      |\
| total timesteps         | 72512    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 117      |\
| mean 100 episode reward | 182      |\
| n_updates               | 81477    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 694      |\
| total timesteps         | 81576    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 117      |\
| mean 100 episode reward | 164      |\
| n_updates               | 90541    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 773      |\
| total timesteps         | 90640    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 44       |\
| fps                     | 116      |\
| mean 100 episode reward | 149      |\
| n_updates               | 99605    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 859      |\
| total timesteps         | 99704    |\
| value_loss              | nan      |\
--------------------------------------\
actions (22660, 30)\
obs (22660, 181)\
rewards (22660,)\
episode_returns (10,)\
episode_starts (22660,)\
actions (22660, 30)\
obs (22660, 181)\
rewards (22660,)\
episode_returns (10,)\
episode_starts (22660,)\
Total trajectories: 10\
Total transitions: 22660\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.924 seconds\
computegrad\
done in 0.203 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.65          0\
         1       1.95      0.127\
         2       5.04      0.608\
         3       1.68       0.78\
         4       2.91       2.95\
         5       20.7       6.04\
         6       3.24       8.49\
         7       8.77       11.9\
         8       13.7       16.6\
         9       5.71       18.1\
        10         24       19.2\
done in 0.405 seconds\
Expected: 0.511 Actual: 0.105\
Stepsize OK!\
vf\
done in 0.147 seconds\
sampling\
done in 4.510 seconds\
computegrad\
done in 0.012 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        186          0\
         1   5.99e+04         76\
         2   5.22e+04        148\
         3   2.88e+05        439\
         4   2.59e+04        997\
         5   2.41e+04   1.07e+03\
         6   5.56e+04   1.17e+03\
         7   1.23e+04   1.28e+03\
         8   2.74e+03   1.29e+03\
         9   1.61e+03   1.31e+03\
        10   1.49e+03   1.33e+03\
done in 0.063 seconds\
Expected: 18.983 Actual: 0.019\
Stepsize OK!\
vf\
done in 0.081 seconds\
sampling\
done in 4.861 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       4.15          0\
         1       20.4     0.0163\
         2        133      0.101\
         3       21.6      0.576\
         4        135       2.73\
         5       53.6       8.33\
         6        820        8.9\
         7        197       10.4\
         8   2.09e+03       13.3\
         9        513       23.1\
        10        310       35.7\
done in 0.050 seconds\
Expected: 1.089 Actual: 0.074\
Stepsize OK!\
vf\
done in 0.067 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.66927 |           nan |           nan |           nan |       0.59473 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.27e+03    |\
| EpRewMean               | 458.70938   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 143         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 15.9        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.60104    |\
| explained_variance_t... | -0.128      |\
| meankl                  | 0.012506433 |\
| optimgain               | 0.074394315 |\
| surrgain                | 0.074394315 |\
-----------------------------------------\
Training time (GAIL):  15.704723985989888  minutes\
======GAIL Validation from:  20180103 to  20180405\
GAIL Sharpe Ratio:  0.04334997384477795\
======Trading from:  20180405 to  20180705\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x14d49ec88>\
previous_total_asset:1296852.8118182393\
end_total_asset:1302456.4685814641\
total_reward:5603.6567632248625\
total_cost:  6516.6343603767245\
total trades:  999\
Sharpe:  0.03160251368322592\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20180405\
======A2C Training========\
Training time (A2C):  2.0585391481717425  minutes\
======A2C Validation from:  20180405 to  20180705\
A2C Sharpe Ratio:  -0.14303523892618045\
======PPO Training========\
Training time (PPO):  7.299863151709238  minutes\
======PPO Validation from:  20180405 to  20180705\
PPO Sharpe Ratio:  -0.13362855776432858\
======DDPG Training========\
Training time (DDPG):  1.1950487693150837  minutes\
======DDPG Validation from:  20180405 to  20180705\
DDPG Sharpe Ratio:  -0.014188735820396509\
======TD3 Training========\
Training time (TD3):  1.3532398025194803  minutes\
======TD3 Validation from:  20180405 to  20180705\
TD3 Sharpe Ratio:  -0.034850950821910046\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 12.936847 |\
| ent_coef_loss           | -721.1828 |\
| entropy                 | 102.56817 |\
| episodes                | 4         |\
| fps                     | 120       |\
| mean 100 episode reward | 164       |\
| n_updates               | 9217      |\
| policy_loss             | 62243.26  |\
| qf1_loss                | 31424.19  |\
| qf2_loss                | 31633.934 |\
| reference_Q_mean        | 4.09      |\
| reference_Q_std         | 2.34      |\
| reference_action_mean   | -0.0615   |\
| reference_action_std    | 0.939     |\
| reference_actor_Q_mean  | 5.25      |\
| reference_actor_Q_std   | 2.23      |\
| rollout/Q_mean          | 2.43      |\
| rollout/actions_mean    | 0.00937   |\
| rollout/actions_std     | 0.809     |\
| rollout/episode_steps   | 2.33e+03  |\
| rollout/episodes        | 4         |\
| rollout/return          | 172       |\
| rollout/return_history  | 172       |\
| time_elapsed            | 77        |\
| total timesteps         | 9316      |\
| total/duration          | 69.8      |\
| total/episodes          | 4         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 143       |\
| train/loss_actor        | -4.78     |\
| train/loss_critic       | 1.49      |\
| train/param_noise_di... | 0         |\
| value_loss              | 10329.459 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 211.0334   |\
| ent_coef_loss           | -1505.8058 |\
| entropy                 | 102.56817  |\
| episodes                | 8          |\
| fps                     | 120        |\
| mean 100 episode reward | 186        |\
| n_updates               | 18533      |\
| policy_loss             | 980676.4   |\
| qf1_loss                | 9659016.0  |\
| qf2_loss                | 11298174.0 |\
| time_elapsed            | 154        |\
| total timesteps         | 18632      |\
| value_loss              | 10136252.0 |\
----------------------------------------\
------------------------------------------\
| current_lr              | 0.0003       |\
| ent_coef                | 3451.409     |\
| ent_coef_loss           | -2306.303    |\
| entropy                 | 102.56817    |\
| episodes                | 12           |\
| fps                     | 121          |\
| mean 100 episode reward | 203          |\
| n_updates               | 27849        |\
| policy_loss             | 15850673.0   |\
| qf1_loss                | 2601764900.0 |\
| qf2_loss                | 2566198300.0 |\
| time_elapsed            | 230          |\
| total timesteps         | 27948        |\
| value_loss              | 558958340.0  |\
------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 56536.527      |\
| ent_coef_loss           | -3089.7368     |\
| entropy                 | 102.56817      |\
| episodes                | 16             |\
| fps                     | 121            |\
| mean 100 episode reward | 209            |\
| n_updates               | 37165          |\
| policy_loss             | 257615580.0    |\
| qf1_loss                | 799611700000.0 |\
| qf2_loss                | 795183500000.0 |\
| time_elapsed            | 305            |\
| total timesteps         | 37264          |\
| value_loss              | 71869450000.0  |\
--------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 926115.56          |\
| ent_coef_loss           | -3875.7102         |\
| entropy                 | 102.56817          |\
| episodes                | 20                 |\
| fps                     | 120                |\
| mean 100 episode reward | 213                |\
| n_updates               | 46481              |\
| policy_loss             | 3979819500.0       |\
| qf1_loss                | 1134244200000000.0 |\
| qf2_loss                | 1112270100000000.0 |\
| time_elapsed            | 388                |\
| total timesteps         | 46580              |\
| value_loss              | 25412060000000.0   |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 15152327.0    |\
| ent_coef_loss           | -4656.428     |\
| entropy                 | 102.56817     |\
| episodes                | 24            |\
| fps                     | 118           |\
| mean 100 episode reward | 215           |\
| n_updates               | 55797         |\
| policy_loss             | 26636962000.0 |\
| qf1_loss                | 3.9853638e+18 |\
| qf2_loss                | 3.794128e+18  |\
| time_elapsed            | 471           |\
| total timesteps         | 55896         |\
| value_loss              | 5.0873853e+17 |\
-------------------------------------------\
--------------------------------------------\
| current_lr              | 0.0003         |\
| ent_coef                | 246624060.0    |\
| ent_coef_loss           | -5436.615      |\
| entropy                 | 102.56817      |\
| episodes                | 28             |\
| fps                     | 117            |\
| mean 100 episode reward | 217            |\
| n_updates               | 65113          |\
| policy_loss             | 135161000000.0 |\
| qf1_loss                | 1.1436212e+20  |\
| qf2_loss                | 1.09971535e+20 |\
| time_elapsed            | 556            |\
| total timesteps         | 65212          |\
| value_loss              | 1.7787324e+21  |\
--------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 116      |\
| mean 100 episode reward | 197      |\
| n_updates               | 74429    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 639      |\
| total timesteps         | 74528    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 115      |\
| mean 100 episode reward | 175      |\
| n_updates               | 83745    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 723      |\
| total timesteps         | 83844    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 114      |\
| mean 100 episode reward | 158      |\
| n_updates               | 93061    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 812      |\
| total timesteps         | 93160    |\
| value_loss              | nan      |\
--------------------------------------\
actions (23290, 30)\
obs (23290, 181)\
rewards (23290,)\
episode_returns (10,)\
episode_starts (23290,)\
actions (23290, 30)\
obs (23290, 181)\
rewards (23290,)\
episode_returns (10,)\
episode_starts (23290,)\
Total trajectories: 10\
Total transitions: 23290\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.725 seconds\
computegrad\
done in 0.218 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.34          0\
         1      0.597      0.074\
         2        4.3      0.272\
         3      0.421      0.381\
         4      0.298      0.981\
         5       2.75        1.6\
         6      0.539       2.31\
         7       1.09       3.17\
         8        1.3       3.25\
         9      0.216       4.61\
        10      0.872       4.77\
done in 0.433 seconds\
Expected: 0.195 Actual: 0.104\
violated KL constraint. shrinking step.\
Expected: 0.195 Actual: 0.059\
Stepsize OK!\
vf\
done in 0.151 seconds\
sampling\
done in 4.109 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.84          0\
         1       8.18     0.0647\
         2       2.66      0.165\
         3       7.23      0.223\
         4       21.6      0.792\
         5       6.25       1.39\
         6       32.1       1.81\
         7       11.5        2.7\
         8       19.5       4.45\
         9       16.5       5.04\
        10       18.7       7.94\
done in 0.047 seconds\
Expected: 0.370 Actual: 0.065\
violated KL constraint. shrinking step.\
Expected: 0.370 Actual: 0.036\
Stepsize OK!\
vf\
done in 0.062 seconds\
sampling\
done in 4.396 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        3.6          0\
         1       29.3      0.146\
         2       18.1      0.371\
         3       42.1      0.684\
         4       75.4       2.52\
         5         38        6.1\
         6        271         15\
         7        222       22.4\
         8       72.5       27.2\
         9        573       29.4\
        10        481       46.7\
done in 0.049 seconds\
Expected: 1.197 Actual: 0.062\
violated KL constraint. shrinking step.\
Expected: 1.197 Actual: 0.030\
Stepsize OK!\
vf\
done in 0.062 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.75135 |           nan |           nan |           nan |       0.44629 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.33e+03    |\
| EpRewMean               | 913.68005   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 159         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.9        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.5594     |\
| explained_variance_t... | 0.0285      |\
| meankl                  | 0.008774009 |\
| optimgain               | 0.030493043 |\
| surrgain                | 0.030493043 |\
-----------------------------------------\
Training time (GAIL):  15.776282930374146  minutes\
======GAIL Validation from:  20180405 to  20180705\
GAIL Sharpe Ratio:  -0.19431597073360501\
======Trading from:  20180705 to  20181003\
Used Model:  <stable_baselines.ddpg.ddpg.DDPG object at 0x148c6bc50>\
previous_total_asset:1302456.4685814641\
end_total_asset:1315732.0036074033\
total_reward:13275.535025939113\
total_cost:  5540.102917196785\
total trades:  645\
Sharpe:  0.09965403022420734\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20180705\
======A2C Training========\
Training time (A2C):  1.8441065311431886  minutes\
======A2C Validation from:  20180705 to  20181003\
A2C Sharpe Ratio:  0.11591018621788536\
======PPO Training========\
Training time (PPO):  7.527593616644541  minutes\
======PPO Validation from:  20180705 to  20181003\
PPO Sharpe Ratio:  0.1273104432542396\
======DDPG Training========\
Training time (DDPG):  1.126328698794047  minutes\
======DDPG Validation from:  20180705 to  20181003\
DDPG Sharpe Ratio:  0.1807001775979145\
======TD3 Training========\
Training time (TD3):  1.3148720184961955  minutes\
======TD3 Validation from:  20180705 to  20181003\
TD3 Sharpe Ratio:  0.18097666414225924\
======GAIL Training========\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 16.793402  |\
| ent_coef_loss           | -1114.0337 |\
| entropy                 | -8.119347  |\
| episodes                | 4          |\
| fps                     | 107        |\
| mean 100 episode reward | 185        |\
| n_updates               | 9469       |\
| policy_loss             | 103058.16  |\
| qf1_loss                | 43297.07   |\
| qf2_loss                | 60501.758  |\
| reference_Q_mean        | 4.18       |\
| reference_Q_std         | 3.72       |\
| reference_action_mean   | -0.0374    |\
| reference_action_std    | 0.976      |\
| reference_actor_Q_mean  | 5.45       |\
| reference_actor_Q_std   | 3.82       |\
| rollout/Q_mean          | 2.75       |\
| rollout/actions_mean    | 0.128      |\
| rollout/actions_std     | 0.805      |\
| rollout/episode_steps   | 2.39e+03   |\
| rollout/episodes        | 4          |\
| rollout/return          | 255        |\
| rollout/return_history  | 255        |\
| time_elapsed            | 88         |\
| total timesteps         | 9568       |\
| total/duration          | 66.7       |\
| total/episodes          | 4          |\
| total/epochs            | 1          |\
| total/steps             | 9998       |\
| total/steps_per_second  | 150        |\
| train/loss_actor        | -5.43      |\
| train/loss_critic       | 2.24       |\
| train/param_noise_di... | 0          |\
| value_loss              | 65688.22   |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 296.20676  |\
| ent_coef_loss           | -2240.1858 |\
| entropy                 | -7.431846  |\
| episodes                | 8          |\
| fps                     | 112        |\
| mean 100 episode reward | 186        |\
| n_updates               | 19037      |\
| policy_loss             | 1862097.9  |\
| qf1_loss                | 25319380.0 |\
| qf2_loss                | 27171976.0 |\
| time_elapsed            | 170        |\
| total timesteps         | 19136      |\
| value_loss              | 5880027.0  |\
----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 5144.0996     |\
| ent_coef_loss           | -3181.901     |\
| entropy                 | 13.880655     |\
| episodes                | 12            |\
| fps                     | 114           |\
| mean 100 episode reward | 182           |\
| n_updates               | 28605         |\
| policy_loss             | 31214652.0    |\
| qf1_loss                | 28468939000.0 |\
| qf2_loss                | 29833540000.0 |\
| time_elapsed            | 250           |\
| total timesteps         | 28704         |\
| value_loss              | 16310105000.0 |\
-------------------------------------------\
---------------------------------------------\
| current_lr              | 0.0003          |\
| ent_coef                | 89003.836       |\
| ent_coef_loss           | -3980.4565      |\
| entropy                 | 36.9119         |\
| episodes                | 16              |\
| fps                     | 116             |\
| mean 100 episode reward | 180             |\
| n_updates               | 38173           |\
| policy_loss             | 506080300.0     |\
| qf1_loss                | 2793683800000.0 |\
| qf2_loss                | 2724969800000.0 |\
| time_elapsed            | 329             |\
| total timesteps         | 38272           |\
| value_loss              | 231396470000.0  |\
---------------------------------------------\
-----------------------------------------------\
| current_lr              | 0.0003            |\
| ent_coef                | 1568340.6         |\
| ent_coef_loss           | -4975.621         |\
| entropy                 | 36.9119           |\
| episodes                | 20                |\
| fps                     | 116               |\
| mean 100 episode reward | 182               |\
| n_updates               | 47741             |\
| policy_loss             | 7412330500.0      |\
| qf1_loss                | 2.4555732e+16     |\
| qf2_loss                | 1.3314176e+16     |\
| time_elapsed            | 409               |\
| total timesteps         | 47840             |\
| value_loss              | 301327660000000.0 |\
-----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 27646238.0    |\
| ent_coef_loss           | -5999.6436    |\
| entropy                 | 36.56815      |\
| episodes                | 24            |\
| fps                     | 117           |\
| mean 100 episode reward | 183           |\
| n_updates               | 57309         |\
| policy_loss             | 40643240000.0 |\
| qf1_loss                | 2.6460115e+19 |\
| qf2_loss                | 1.492546e+19  |\
| time_elapsed            | 490           |\
| total timesteps         | 57408         |\
| value_loss              | 7.006476e+18  |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 117      |\
| mean 100 episode reward | 184      |\
| n_updates               | 66877    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 569      |\
| total timesteps         | 66976    |\
| value_loss              | nan      |\
--------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 118      |\
| mean 100 episode reward | 161      |\
| n_updates               | 76445    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 645      |\
| total timesteps         | 76544    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 119      |\
| mean 100 episode reward | 143      |\
| n_updates               | 86013    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 721      |\
| total timesteps         | 86112    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 120      |\
| mean 100 episode reward | 129      |\
| n_updates               | 95581    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 796      |\
| total timesteps         | 95680    |\
| value_loss              | nan      |\
--------------------------------------\
actions (23920, 30)\
obs (23920, 181)\
rewards (23920,)\
episode_returns (10,)\
episode_starts (23920,)\
actions (23920, 30)\
obs (23920, 181)\
rewards (23920,)\
episode_returns (10,)\
episode_starts (23920,)\
Total trajectories: 10\
Total transitions: 23920\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.689 seconds\
computegrad\
done in 0.203 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.28          0\
         1       6.51      0.124\
         2       4.21      0.157\
         3       9.86       2.35\
         4       15.3       2.99\
         5       21.3       12.6\
         6       85.2       16.7\
         7        165       17.8\
         8       30.8       28.9\
         9       12.7       40.5\
        10       22.4       60.2\
done in 0.432 seconds\
Expected: 1.089 Actual: 0.065\
Stepsize OK!\
vf\
done in 0.152 seconds\
sampling\
done in 4.469 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        2.7          0\
         1       29.2    0.00586\
         2       16.9     0.0226\
         3       8.64       0.17\
         4       7.63       0.22\
         5       9.87      0.279\
         6   1.27e+03      0.991\
         7       7.02       1.37\
         8       8.34       1.58\
         9       17.1       1.77\
        10         89       1.94\
done in 0.051 seconds\
Expected: 0.213 Actual: 0.044\
Stepsize OK!\
vf\
done in 0.074 seconds\
sampling\
done in 4.382 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.18          0\
         1       9.96    0.00394\
         2       2.32     0.0646\
         3       1.48      0.128\
         4       1.88      0.445\
         5       5.63      0.449\
         6        1.1      0.548\
         7       3.95       1.77\
         8       5.12       2.27\
         9       1.74       2.28\
        10       2.53       2.55\
done in 0.055 seconds\
Expected: 0.174 Actual: 0.095\
Stepsize OK!\
vf\
done in 0.075 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.75583 |           nan |           nan |           nan |       0.53809 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.39e+03    |\
| EpRewMean               | 1352.409    |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 201         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 15.2        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.583534   |\
| explained_variance_t... | -0.155      |\
| meankl                  | 0.012712035 |\
| optimgain               | 0.09532291  |\
| surrgain                | 0.09532291  |\
-----------------------------------------\
Training time (GAIL):  15.080564932028453  minutes\
======GAIL Validation from:  20180705 to  20181003\
GAIL Sharpe Ratio:  0.11985423830929809\
======Trading from:  20181003 to  20190104\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x147ef6400>\
previous_total_asset:1315732.0036074033\
end_total_asset:1327209.8686074037\
total_reward:11477.865000000456\
total_cost:  728.0939999999997\
total trades:  90\
Sharpe:  0.27577602075676805\
============================================\
turbulence_threshold:  171.0940715631016\
======Model training from:  20090000 to  20181003\
======A2C Training========\
Training time (A2C):  1.8864696979522706  minutes\
======A2C Validation from:  20181003 to  20190104\
A2C Sharpe Ratio:  -0.41326149376285864\
======PPO Training========\
Training time (PPO):  6.504899434248606  minutes\
======PPO Validation from:  20181003 to  20190104\
PPO Sharpe Ratio:  -0.44248909100142275\
======DDPG Training========\
Training time (DDPG):  1.2641552805900573  minutes\
======DDPG Validation from:  20181003 to  20190104\
DDPG Sharpe Ratio:  -0.40264793653330877\
======TD3 Training========\
Training time (TD3):  1.2757133007049561  minutes\
======TD3 Validation from:  20181003 to  20190104\
TD3 Sharpe Ratio:  -0.3987290163382515\
======GAIL Training========\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 17.135244  |\
| ent_coef_loss           | -1183.7262 |\
| entropy                 | -29.431852 |\
| episodes                | 4          |\
| fps                     | 118        |\
| mean 100 episode reward | 225        |\
| n_updates               | 9721       |\
| policy_loss             | 115712.59  |\
| qf1_loss                | 146382.81  |\
| qf2_loss                | 301880.0   |\
| reference_Q_mean        | 5.84       |\
| reference_Q_std         | 3.49       |\
| reference_action_mean   | 0.0506     |\
| reference_action_std    | 0.968      |\
| reference_actor_Q_mean  | 6.91       |\
| reference_actor_Q_std   | 3.56       |\
| rollout/Q_mean          | 3.64       |\
| rollout/actions_mean    | -0.0458    |\
| rollout/actions_std     | 0.82       |\
| rollout/episode_steps   | 2.46e+03   |\
| rollout/episodes        | 4          |\
| rollout/return          | 397        |\
| rollout/return_history  | 397        |\
| time_elapsed            | 83         |\
| total timesteps         | 9820       |\
| total/duration          | 74.8       |\
| total/episodes          | 4          |\
| total/epochs            | 1          |\
| total/steps             | 9998       |\
| total/steps_per_second  | 134        |\
| train/loss_actor        | -6.24      |\
| train/loss_critic       | 2.68       |\
| train/param_noise_di... | 0          |\
| value_loss              | 613282.7   |\
----------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 310.20685  |\
| ent_coef_loss           | -2141.063  |\
| entropy                 | 13.193148  |\
| episodes                | 8          |\
| fps                     | 117        |\
| mean 100 episode reward | 183        |\
| n_updates               | 19541      |\
| policy_loss             | 1883992.2  |\
| qf1_loss                | 16836780.0 |\
| qf2_loss                | 19078886.0 |\
| time_elapsed            | 167        |\
| total timesteps         | 19640      |\
| value_loss              | 25059724.0 |\
----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 5903.0625     |\
| ent_coef_loss           | -3239.1123    |\
| entropy                 | 13.193148     |\
| episodes                | 12            |\
| fps                     | 118           |\
| mean 100 episode reward | 169           |\
| n_updates               | 29361         |\
| policy_loss             | 34906372.0    |\
| qf1_loss                | 9999745000.0  |\
| qf2_loss                | 10222979000.0 |\
| time_elapsed            | 247           |\
| total timesteps         | 29460         |\
| value_loss              | 12199199000.0 |\
-------------------------------------------\
---------------------------------------------\
| current_lr              | 0.0003          |\
| ent_coef                | 112351.31       |\
| ent_coef_loss           | -4321.0527      |\
| entropy                 | 14.568148       |\
| episodes                | 16              |\
| fps                     | 119             |\
| mean 100 episode reward | 162             |\
| n_updates               | 39181           |\
| policy_loss             | 655701600.0     |\
| qf1_loss                | 4524390500000.0 |\
| qf2_loss                | 4327915000000.0 |\
| time_elapsed            | 328             |\
| total timesteps         | 39280           |\
| value_loss              | 766858040000.0  |\
---------------------------------------------\
------------------------------------------------\
| current_lr              | 0.0003             |\
| ent_coef                | 2138560.5          |\
| ent_coef_loss           | -5421.117          |\
| entropy                 | 14.568148          |\
| episodes                | 20                 |\
| fps                     | 119                |\
| mean 100 episode reward | 157                |\
| n_updates               | 49001              |\
| policy_loss             | 9732626000.0       |\
| qf1_loss                | 6.0970875e+16      |\
| qf2_loss                | 3.9833898e+16      |\
| time_elapsed            | 411                |\
| total timesteps         | 49100              |\
| value_loss              | 5669561000000000.0 |\
------------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 40593532.0    |\
| ent_coef_loss           | -6529.8994    |\
| entropy                 | 14.568148     |\
| episodes                | 24            |\
| fps                     | 119           |\
| mean 100 episode reward | 155           |\
| n_updates               | 58821         |\
| policy_loss             | 52015075000.0 |\
| qf1_loss                | 1.375371e+19  |\
| qf2_loss                | 8.0672576e+18 |\
| time_elapsed            | 493           |\
| total timesteps         | 58920         |\
| value_loss              | 6.056372e+19  |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 119      |\
| mean 100 episode reward | 148      |\
| n_updates               | 68641    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 575      |\
| total timesteps         | 68740    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 118      |\
| mean 100 episode reward | 129      |\
| n_updates               | 78461    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 662      |\
| total timesteps         | 78560    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 118      |\
| mean 100 episode reward | 115      |\
| n_updates               | 88281    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 746      |\
| total timesteps         | 88380    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 40       |\
| fps                     | 119      |\
| mean 100 episode reward | 103      |\
| n_updates               | 98101    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 824      |\
| total timesteps         | 98200    |\
| value_loss              | nan      |\
--------------------------------------\
actions (24550, 30)\
obs (24550, 181)\
rewards (24550,)\
episode_returns (10,)\
episode_starts (24550,)\
actions (24550, 30)\
obs (24550, 181)\
rewards (24550,)\
episode_returns (10,)\
episode_starts (24550,)\
Total trajectories: 10\
Total transitions: 24550\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.313 seconds\
computegrad\
done in 0.208 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.34          0\
         1       37.3     0.0517\
         2       1.43      0.116\
         3      0.827      0.488\
         4       7.93      0.969\
         5        140       1.76\
         6      0.809       2.03\
         7       4.41       2.97\
         8       1.09       3.65\
         9       24.1        3.7\
        10       1.42        4.9\
done in 0.415 seconds\
Expected: 0.244 Actual: 0.110\
Stepsize OK!\
vf\
done in 0.167 seconds\
sampling\
done in 4.392 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.39          0\
         1       13.7     0.0507\
         2       8.33      0.124\
         3         10       0.57\
         4        153       1.34\
         5       18.9       2.05\
         6       21.9       2.75\
         7       44.8       7.11\
         8       10.8        8.8\
         9        178        9.3\
        10       43.1       19.4\
done in 0.056 seconds\
Expected: 0.656 Actual: 0.078\
Stepsize OK!\
vf\
done in 0.068 seconds\
sampling\
done in 4.699 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       10.9          0\
         1   1.19e+03      0.315\
         2   1.61e+03      0.758\
         3        542       1.08\
         4        170       2.32\
         5   1.27e+03       7.81\
         6        528         26\
         7   1.39e+03       26.2\
         8   2.17e+04       31.4\
         9    1.9e+03       63.3\
        10   1.87e+03        108\
done in 0.048 seconds\
Expected: 2.593 Actual: 0.049\
Stepsize OK!\
vf\
done in 0.063 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.72439 |           nan |           nan |           nan |       0.48242 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.46e+03    |\
| EpRewMean               | 1057.0854   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 196         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 15.2        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.48619    |\
| explained_variance_t... | -0.0552     |\
| meankl                  | 0.012923293 |\
| optimgain               | 0.048738994 |\
| surrgain                | 0.048738994 |\
-----------------------------------------\
Training time (GAIL):  15.24721732934316  minutes\
======GAIL Validation from:  20181003 to  20190104\
GAIL Sharpe Ratio:  -0.3184315319274284\
======Trading from:  20190104 to  20190405\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x148d6a3c8>\
previous_total_asset:1327209.8686074037\
end_total_asset:1380275.4327451068\
total_reward:53065.56413770304\
total_cost:  10978.015875735862\
total trades:  1705\
Sharpe:  0.22138191545224079\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20190104\
======A2C Training========\
Training time (A2C):  1.8907302300135294  minutes\
======A2C Validation from:  20190104 to  20190405\
A2C Sharpe Ratio:  -0.0250754526071219\
======PPO Training========\
Training time (PPO):  6.524213552474976  minutes\
======PPO Validation from:  20190104 to  20190405\
PPO Sharpe Ratio:  0.06239882460537196\
======DDPG Training========\
Training time (DDPG):  1.1529646952946981  minutes\
======DDPG Validation from:  20190104 to  20190405\
DDPG Sharpe Ratio:  0.02585918272886613\
======TD3 Training========\
Training time (TD3):  1.2630927681922912  minutes\
======TD3 Validation from:  20190104 to  20190405\
TD3 Sharpe Ratio:  0.12795245442911996\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 17.299507 |\
| ent_coef_loss           | -803.1084 |\
| entropy                 | 102.56817 |\
| episodes                | 4         |\
| fps                     | 118       |\
| mean 100 episode reward | 220       |\
| n_updates               | 9973      |\
| policy_loss             | 77795.375 |\
| qf1_loss                | 43731.73  |\
| qf2_loss                | 37337.207 |\
| reference_Q_mean        | 4.58      |\
| reference_Q_std         | 4.41      |\
| reference_action_mean   | -0.341    |\
| reference_action_std    | 0.919     |\
| reference_actor_Q_mean  | 6.09      |\
| reference_actor_Q_std   | 4.5       |\
| rollout/Q_mean          | 3.21      |\
| rollout/actions_mean    | -0.159    |\
| rollout/actions_std     | 0.8       |\
| rollout/episode_steps   | 2.52e+03  |\
| rollout/episodes        | 3         |\
| rollout/return          | 343       |\
| rollout/return_history  | 343       |\
| time_elapsed            | 84        |\
| total timesteps         | 10072     |\
| total/duration          | 68.2      |\
| total/episodes          | 3         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 147       |\
| train/loss_actor        | -6.71     |\
| train/loss_critic       | 7.03      |\
| train/param_noise_di... | 0         |\
| value_loss              | 24809.39  |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 354.86923  |\
| ent_coef_loss           | -1657.6448 |\
| entropy                 | 102.56817  |\
| episodes                | 8          |\
| fps                     | 115        |\
| mean 100 episode reward | 217        |\
| n_updates               | 20045      |\
| policy_loss             | 1635039.1  |\
| qf1_loss                | 15955454.0 |\
| qf2_loss                | 16016280.0 |\
| time_elapsed            | 174        |\
| total timesteps         | 20144      |\
| value_loss              | 4087788.8  |\
----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 7284.6533     |\
| ent_coef_loss           | -2514.3757    |\
| entropy                 | 100.84942     |\
| episodes                | 12            |\
| fps                     | 117           |\
| mean 100 episode reward | 211           |\
| n_updates               | 30117         |\
| policy_loss             | 33381594.0    |\
| qf1_loss                | 11950934000.0 |\
| qf2_loss                | 11500564000.0 |\
| time_elapsed            | 258           |\
| total timesteps         | 30216         |\
| value_loss              | 5710699500.0  |\
-------------------------------------------\
---------------------------------------------\
| current_lr              | 0.0003          |\
| ent_coef                | 149655.73       |\
| ent_coef_loss           | -3358.5483      |\
| entropy                 | 102.56817       |\
| episodes                | 16              |\
| fps                     | 117             |\
| mean 100 episode reward | 208             |\
| n_updates               | 40189           |\
| policy_loss             | 678157100.0     |\
| qf1_loss                | 5799556400000.0 |\
| qf2_loss                | 6062659000000.0 |\
| time_elapsed            | 341             |\
| total timesteps         | 40288           |\
| value_loss              | 393415900000.0  |\
---------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 3075251.2     |\
| ent_coef_loss           | -4222.9053    |\
| entropy                 | 102.22442     |\
| episodes                | 20            |\
| fps                     | 115           |\
| mean 100 episode reward | 206           |\
| n_updates               | 50261         |\
| policy_loss             | 10380749000.0 |\
| qf1_loss                | 3.79364e+16   |\
| qf2_loss                | 1.1812625e+17 |\
| time_elapsed            | 434           |\
| total timesteps         | 50360         |\
| value_loss              | 2.9740473e+16 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 62911116.0    |\
| ent_coef_loss           | -5060.4175    |\
| entropy                 | 102.56817     |\
| episodes                | 24            |\
| fps                     | 112           |\
| mean 100 episode reward | 204           |\
| n_updates               | 60333         |\
| policy_loss             | 54902948000.0 |\
| qf1_loss                | 1.5378311e+18 |\
| qf2_loss                | 8.42186e+18   |\
| time_elapsed            | 536           |\
| total timesteps         | 60432         |\
| value_loss              | 1.4199403e+20 |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 109      |\
| mean 100 episode reward | 196      |\
| n_updates               | 70405    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 641      |\
| total timesteps         | 70504    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 109      |\
| mean 100 episode reward | 172      |\
| n_updates               | 80477    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 734      |\
| total timesteps         | 80576    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 109      |\
| mean 100 episode reward | 153      |\
| n_updates               | 90549    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 827      |\
| total timesteps         | 90648    |\
| value_loss              | nan      |\
--------------------------------------\
actions (25180, 30)\
obs (25180, 181)\
rewards (25180,)\
episode_returns (10,)\
episode_starts (25180,)\
actions (25180, 30)\
obs (25180, 181)\
rewards (25180,)\
episode_returns (10,)\
episode_starts (25180,)\
Total trajectories: 10\
Total transitions: 25180\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 5.055 seconds\
computegrad\
done in 0.223 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.82          0\
         1       6.86      0.193\
         2       37.7          1\
         3         23       4.81\
         4        234       9.86\
         5       35.5       24.7\
         6        219       35.9\
         7       32.1       56.7\
         8       39.3       67.7\
         9        117       86.7\
        10       13.4       91.7\
done in 0.440 seconds\
Expected: 1.545 Actual: 0.058\
Stepsize OK!\
vf\
done in 0.386 seconds\
sampling\
done in 5.195 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.45          0\
         1       2.94   0.000138\
         2      0.813     0.0686\
         3       2.91     0.0909\
         4        972      0.199\
         5        2.4      0.312\
         6      0.742      0.454\
         7        818      0.771\
         8       6.25      0.848\
         9       1.25        1.6\
        10       63.7       1.74\
done in 0.053 seconds\
Expected: 0.143 Actual: 0.088\
Stepsize OK!\
vf\
done in 0.091 seconds\
sampling\
done in 5.209 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       4.92          0\
         1       48.8      0.278\
         2       25.6      0.424\
         3        148      0.901\
         4        693       4.44\
         5       60.2       6.61\
         6        276       7.19\
         7   1.13e+03         33\
         8        993         37\
         9        218       54.6\
        10   1.04e+03       58.6\
done in 0.051 seconds\
Expected: 1.481 Actual: 0.067\
Stepsize OK!\
vf\
done in 0.065 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.80179 |           nan |           nan |           nan |       0.41113 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.52e+03    |\
| EpRewMean               | 2772.1948   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 151         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 17.5        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.566933   |\
| explained_variance_t... | 0.0135      |\
| meankl                  | 0.009916691 |\
| optimgain               | 0.0671219   |\
| surrgain                | 0.0671219   |\
-----------------------------------------\
Training time (GAIL):  17.289706798394523  minutes\
======GAIL Validation from:  20190104 to  20190405\
GAIL Sharpe Ratio:  -0.04368775415725262\
======Trading from:  20190405 to  20190708\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x14ac92438>\
previous_total_asset:1380275.4327451068\
end_total_asset:1387436.4067451074\
total_reward:7160.974000000628\
total_cost:  703.1200000000003\
total trades:  88\
Sharpe:  0.46399623484823244\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20190405\
======A2C Training========\
Training time (A2C):  2.2530741691589355  minutes\
======A2C Validation from:  20190405 to  20190708\
A2C Sharpe Ratio:  0.3145242308349101\
======PPO Training========\
Training time (PPO):  7.676651302973429  minutes\
======PPO Validation from:  20190405 to  20190708\
PPO Sharpe Ratio:  -0.044123483867027295\
======DDPG Training========\
Training time (DDPG):  1.3603107333183289  minutes\
======DDPG Validation from:  20190405 to  20190708\
DDPG Sharpe Ratio:  0.28808334384344947\
======TD3 Training========\
Training time (TD3):  1.6007916967074076  minutes\
======TD3 Validation from:  20190405 to  20190708\
TD3 Sharpe Ratio:  0.39358462439038133\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 19.118654 |\
| ent_coef_loss           | -904.8486 |\
| entropy                 | 78.84941  |\
| episodes                | 4         |\
| fps                     | 120       |\
| mean 100 episode reward | 200       |\
| n_updates               | 10225     |\
| policy_loss             | 101003.56 |\
| qf1_loss                | 754089.94 |\
| qf2_loss                | 550830.6  |\
| reference_Q_mean        | 3.79      |\
| reference_Q_std         | 3.29      |\
| reference_action_mean   | -0.07     |\
| reference_action_std    | 0.959     |\
| reference_actor_Q_mean  | 5.12      |\
| reference_actor_Q_std   | 3.26      |\
| rollout/Q_mean          | 2.82      |\
| rollout/actions_mean    | -0.0847   |\
| rollout/actions_std     | 0.807     |\
| rollout/episode_steps   | 2.58e+03  |\
| rollout/episodes        | 3         |\
| rollout/return          | 271       |\
| rollout/return_history  | 271       |\
| time_elapsed            | 85        |\
| total timesteps         | 10324     |\
| total/duration          | 78.9      |\
| total/episodes          | 3         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 127       |\
| train/loss_actor        | -5.75     |\
| train/loss_critic       | 3.01      |\
| train/param_noise_di... | 0         |\
| value_loss              | 374049.25 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 420.84634  |\
| ent_coef_loss           | -1848.6356 |\
| entropy                 | 79.53691   |\
| episodes                | 8          |\
| fps                     | 120        |\
| mean 100 episode reward | 222        |\
| n_updates               | 20549      |\
| policy_loss             | 2109381.0  |\
| qf1_loss                | 26357418.0 |\
| qf2_loss                | 26910588.0 |\
| time_elapsed            | 171        |\
| total timesteps         | 20648      |\
| value_loss              | 18514692.0 |\
----------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 9313.242         |\
| ent_coef_loss           | -2780.6885       |\
| entropy                 | 80.56817         |\
| episodes                | 12               |\
| fps                     | 113              |\
| mean 100 episode reward | 265              |\
| n_updates               | 30873            |\
| policy_loss             | 45638244.0       |\
| qf1_loss                | 12777762000000.0 |\
| qf2_loss                | 12815335000000.0 |\
| time_elapsed            | 272              |\
| total timesteps         | 30972            |\
| value_loss              | 9567715000.0     |\
----------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 206138.45        |\
| ent_coef_loss           | -3725.5564       |\
| entropy                 | 80.56817         |\
| episodes                | 16               |\
| fps                     | 104              |\
| mean 100 episode reward | 287              |\
| n_updates               | 41197            |\
| policy_loss             | 1007554050.0     |\
| qf1_loss                | 11198136000000.0 |\
| qf2_loss                | 11228468000000.0 |\
| time_elapsed            | 396              |\
| total timesteps         | 41296            |\
| value_loss              | 1228766400000.0  |\
----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 4570231.5     |\
| ent_coef_loss           | -4655.5303    |\
| entropy                 | 80.56817      |\
| episodes                | 20            |\
| fps                     | 101           |\
| mean 100 episode reward | 276           |\
| n_updates               | 51521         |\
| policy_loss             | 14949243000.0 |\
| qf1_loss                | 1.4921053e+17 |\
| qf2_loss                | 1.1877767e+17 |\
| time_elapsed            | 507           |\
| total timesteps         | 51620         |\
| value_loss              | 6.2801252e+16 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 100759160.0   |\
| ent_coef_loss           | -5619.3125    |\
| entropy                 | 80.56817      |\
| episodes                | 24            |\
| fps                     | 99            |\
| mean 100 episode reward | 269           |\
| n_updates               | 61845         |\
| policy_loss             | 84915405000.0 |\
| qf1_loss                | 1.4579504e+19 |\
| qf2_loss                | 1.1167105e+19 |\
| time_elapsed            | 620           |\
| total timesteps         | 61944         |\
| value_loss              | 3.712695e+20  |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 100      |\
| mean 100 episode reward | 248      |\
| n_updates               | 72169    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 721      |\
| total timesteps         | 72268    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 100      |\
| mean 100 episode reward | 217      |\
| n_updates               | 82493    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 821      |\
| total timesteps         | 82592    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 101      |\
| mean 100 episode reward | 193      |\
| n_updates               | 92817    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 916      |\
| total timesteps         | 92916    |\
| value_loss              | nan      |\
--------------------------------------\
actions (25810, 30)\
obs (25810, 181)\
rewards (25810,)\
episode_returns (10,)\
episode_starts (25810,)\
actions (25810, 30)\
obs (25810, 181)\
rewards (25810,)\
episode_returns (10,)\
episode_starts (25810,)\
Total trajectories: 10\
Total transitions: 25810\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.895 seconds\
computegrad\
done in 0.222 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        1.4          0\
         1       1.13     0.0541\
         2      0.873     0.0665\
         3      0.346      0.472\
         4      0.778       1.74\
         5       1.42       1.94\
         6       12.6       2.91\
         7      0.623       3.17\
         8      0.689       4.07\
         9       4.34       6.47\
        10      0.269        7.1\
done in 0.418 seconds\
Expected: 0.238 Actual: 0.090\
violated KL constraint. shrinking step.\
Expected: 0.238 Actual: 0.056\
Stepsize OK!\
vf\
done in 0.187 seconds\
sampling\
done in 4.580 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       5.77          0\
         1       11.6    0.00457\
         2       9.32      0.347\
         3       82.5       1.34\
         4       51.5       1.77\
         5        111        3.7\
         6       50.7       3.76\
         7       80.2       12.9\
         8         22       14.9\
         9   5.67e+03       22.2\
        10       83.2       25.8\
done in 0.049 seconds\
Expected: 0.891 Actual: 0.059\
Stepsize OK!\
vf\
done in 0.065 seconds\
sampling\
done in 4.817 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        3.7          0\
         1        211      0.441\
         2       20.8       0.58\
         3        259       11.7\
         4        111       13.5\
         5   3.75e+03       43.5\
         6       95.7       45.7\
         7        156       53.2\
         8        116         86\
         9        115         95\
        10        177       95.2\
done in 0.047 seconds\
Expected: 1.812 Actual: 0.043\
Stepsize OK!\
vf\
done in 0.065 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.65360 |           nan |           nan |           nan |       0.61133 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.58e+03    |\
| EpRewMean               | 2921.1143   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 157         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 16          |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.524723   |\
| explained_variance_t... | 0.0446      |\
| meankl                  | 0.011384867 |\
| optimgain               | 0.04297622  |\
| surrgain                | 0.04297622  |\
-----------------------------------------\
Training time (GAIL):  17.677021332581837  minutes\
======GAIL Validation from:  20190405 to  20190708\
GAIL Sharpe Ratio:  0.2570117913499666\
======Trading from:  20190708 to  20191004\
Used Model:  <stable_baselines.td3.td3.TD3 object at 0x1439bf518>\
previous_total_asset:1387436.4067451074\
end_total_asset:1383280.0977451073\
total_reward:-4156.309000000125\
total_cost:  1982.0199999999993\
total trades:  216\
Sharpe:  -0.11491608855661203\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20190708\
======A2C Training========\
Training time (A2C):  1.9195390462875366  minutes\
======A2C Validation from:  20190708 to  20191004\
A2C Sharpe Ratio:  -0.09377763658855665\
======PPO Training========\
Training time (PPO):  7.778677999973297  minutes\
======PPO Validation from:  20190708 to  20191004\
PPO Sharpe Ratio:  -0.08107867856157676\
======DDPG Training========\
Training time (DDPG):  1.3737501144409179  minutes\
======DDPG Validation from:  20190708 to  20191004\
DDPG Sharpe Ratio:  0.02218924014485924\
======TD3 Training========\
Training time (TD3):  1.5945562481880189  minutes\
======TD3 Validation from:  20190708 to  20191004\
TD3 Sharpe Ratio:  -0.07261436795592216\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 20.933937 |\
| ent_coef_loss           | -857.4518 |\
| entropy                 | 102.56817 |\
| episodes                | 4         |\
| fps                     | 100       |\
| mean 100 episode reward | 284       |\
| n_updates               | 10477     |\
| policy_loss             | 91481.14  |\
| qf1_loss                | 718899.3  |\
| qf2_loss                | 777261.1  |\
| reference_Q_mean        | 4.44      |\
| reference_Q_std         | 3.81      |\
| reference_action_mean   | -0.0816   |\
| reference_action_std    | 0.959     |\
| reference_actor_Q_mean  | 5.73      |\
| reference_actor_Q_std   | 3.82      |\
| rollout/Q_mean          | 2.78      |\
| rollout/actions_mean    | -0.00799  |\
| rollout/actions_std     | 0.801     |\
| rollout/episode_steps   | 2.64e+03  |\
| rollout/episodes        | 3         |\
| rollout/return          | 234       |\
| rollout/return_history  | 234       |\
| time_elapsed            | 104       |\
| total timesteps         | 10576     |\
| total/duration          | 80.8      |\
| total/episodes          | 3         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 124       |\
| train/loss_actor        | -5.9      |\
| train/loss_critic       | 2.85      |\
| train/param_noise_di... | 0         |\
| value_loss              | 1800022.0 |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 499.653    |\
| ent_coef_loss           | -1754.1304 |\
| entropy                 | 102.56817  |\
| episodes                | 8          |\
| fps                     | 100        |\
| mean 100 episode reward | 262        |\
| n_updates               | 21053      |\
| policy_loss             | 2309879.0  |\
| qf1_loss                | 18720562.0 |\
| qf2_loss                | 17188344.0 |\
| time_elapsed            | 211        |\
| total timesteps         | 21152      |\
| value_loss              | 17229662.0 |\
----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 11933.49      |\
| ent_coef_loss           | -2645.5752    |\
| entropy                 | 102.56817     |\
| episodes                | 12            |\
| fps                     | 99            |\
| mean 100 episode reward | 248           |\
| n_updates               | 31629         |\
| policy_loss             | 54650840.0    |\
| qf1_loss                | 18506154000.0 |\
| qf2_loss                | 18728786000.0 |\
| time_elapsed            | 317           |\
| total timesteps         | 31728         |\
| value_loss              | 5697902600.0  |\
-------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 285303.25        |\
| ent_coef_loss           | -3550.6191       |\
| entropy                 | 102.56817        |\
| episodes                | 16               |\
| fps                     | 101              |\
| mean 100 episode reward | 242              |\
| n_updates               | 42205            |\
| policy_loss             | 1266491400.0     |\
| qf1_loss                | 26570677000000.0 |\
| qf2_loss                | 27872478000000.0 |\
| time_elapsed            | 417              |\
| total timesteps         | 42304            |\
| value_loss              | 1278686000000.0  |\
----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 6821448.0     |\
| ent_coef_loss           | -4444.1772    |\
| entropy                 | 102.56817     |\
| episodes                | 20            |\
| fps                     | 101           |\
| mean 100 episode reward | 237           |\
| n_updates               | 52781         |\
| policy_loss             | 15596373000.0 |\
| qf1_loss                | 7.0609523e+17 |\
| qf2_loss                | 9.054059e+17  |\
| time_elapsed            | 519           |\
| total timesteps         | 52880         |\
| value_loss              | 4.9621114e+16 |\
-------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 162025000.0   |\
| ent_coef_loss           | -5330.5796    |\
| entropy                 | 102.56817     |\
| episodes                | 24            |\
| fps                     | 101           |\
| mean 100 episode reward | 234           |\
| n_updates               | 63357         |\
| policy_loss             | 93920510000.0 |\
| qf1_loss                | 5.364464e+19  |\
| qf2_loss                | 6.4846047e+19 |\
| time_elapsed            | 626           |\
| total timesteps         | 63456         |\
| value_loss              | 7.475608e+20  |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 102      |\
| mean 100 episode reward | 209      |\
| n_updates               | 73933    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 723      |\
| total timesteps         | 74032    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 102      |\
| mean 100 episode reward | 183      |\
| n_updates               | 84509    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 828      |\
| total timesteps         | 84608    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 102      |\
| mean 100 episode reward | 162      |\
| n_updates               | 95085    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 931      |\
| total timesteps         | 95184    |\
| value_loss              | nan      |\
--------------------------------------\
actions (26440, 30)\
obs (26440, 181)\
rewards (26440,)\
episode_returns (10,)\
episode_starts (26440,)\
actions (26440, 30)\
obs (26440, 181)\
rewards (26440,)\
episode_returns (10,)\
episode_starts (26440,)\
Total trajectories: 10\
Total transitions: 26440\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 5.236 seconds\
computegrad\
done in 0.272 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.06          0\
         1       1.12     0.0874\
         2      0.602      0.553\
         3      0.747       1.41\
         4       1.07       1.62\
         5       2.14       1.98\
         6      0.549       3.02\
         7       1.34       4.45\
         8      0.764       6.05\
         9       1.54       6.31\
        10       1.12       10.3\
done in 0.479 seconds\
Expected: 0.293 Actual: 0.108\
violated KL constraint. shrinking step.\
Expected: 0.293 Actual: 0.063\
Stepsize OK!\
vf\
done in 0.162 seconds\
sampling\
done in 4.956 seconds\
computegrad\
done in 0.011 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.85          0\
         1       14.4   0.000841\
         2       5.07     0.0125\
         3       4.91     0.0158\
         4       4.51     0.0714\
         5       8.61      0.127\
         6       2.18      0.128\
         7       10.2      0.188\
         8       16.9      0.405\
         9        158      0.503\
        10        351      0.564\
done in 0.058 seconds\
Expected: 0.100 Actual: 0.064\
Stepsize OK!\
vf\
done in 0.075 seconds\
sampling\
done in 5.164 seconds\
computegrad\
done in 0.017 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       11.6          0\
         1       2.01   0.000722\
         2         15    0.00596\
         3       32.2     0.0387\
         4       1.91     0.0908\
         5       6.03      0.107\
         6        899       0.34\
         7       1.82       1.09\
         8        244       1.21\
         9       16.5       2.19\
        10        150       2.38\
done in 0.055 seconds\
Expected: 0.200 Actual: 0.069\
Stepsize OK!\
vf\
done in 0.084 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.74627 |           nan |           nan |           nan |       0.48926 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.64e+03    |\
| EpRewMean               | 2639.0203   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 188         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 17.3        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.594284   |\
| explained_variance_t... | 0.0144      |\
| meankl                  | 0.009574272 |\
| optimgain               | 0.068809494 |\
| surrgain                | 0.068809494 |\
-----------------------------------------\
Training time (GAIL):  17.852482148011525  minutes\
======GAIL Validation from:  20190708 to  20191004\
GAIL Sharpe Ratio:  0.0696579402889574\
======Trading from:  20191004 to  20200106\
Used Model:  <stable_baselines.gail.model.GAIL object at 0x14b07fe48>\
previous_total_asset:1383280.0977451073\
end_total_asset:1382250.084316241\
total_reward:-1030.0134288661648\
total_cost:  239.17652128920557\
total trades:  66\
Sharpe:  -0.29167846106054046\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20191004\
======A2C Training========\
Training time (A2C):  2.392776381969452  minutes\
======A2C Validation from:  20191004 to  20200106\
A2C Sharpe Ratio:  -0.1117669134973712\
======PPO Training========\
Training time (PPO):  6.851035797595978  minutes\
======PPO Validation from:  20191004 to  20200106\
PPO Sharpe Ratio:  -0.0846608757852156\
======DDPG Training========\
Training time (DDPG):  1.1567239165306091  minutes\
======DDPG Validation from:  20191004 to  20200106\
DDPG Sharpe Ratio:  -0.4253525912453331\
======TD3 Training========\
Training time (TD3):  1.4679107666015625  minutes\
======TD3 Validation from:  20191004 to  20200106\
TD3 Sharpe Ratio:  -0.09280170626402652\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 21.690193 |\
| ent_coef_loss           | -936.3993 |\
| entropy                 | 80.56817  |\
| episodes                | 4         |\
| fps                     | 105       |\
| mean 100 episode reward | 234       |\
| n_updates               | 10729     |\
| policy_loss             | 115515.36 |\
| qf1_loss                | 72885.23  |\
| qf2_loss                | 67299.68  |\
| reference_Q_mean        | 3.05      |\
| reference_Q_std         | 2.87      |\
| reference_action_mean   | -0.0291   |\
| reference_action_std    | 0.963     |\
| reference_actor_Q_mean  | 3.81      |\
| reference_actor_Q_std   | 2.76      |\
| rollout/Q_mean          | 2.2       |\
| rollout/actions_mean    | 0.0683    |\
| rollout/actions_std     | 0.805     |\
| rollout/episode_steps   | 2.71e+03  |\
| rollout/episodes        | 3         |\
| rollout/return          | 233       |\
| rollout/return_history  | 233       |\
| time_elapsed            | 102       |\
| total timesteps         | 10828     |\
| total/duration          | 68.4      |\
| total/episodes          | 3         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 146       |\
| train/loss_actor        | -4.08     |\
| train/loss_critic       | 1.35      |\
| train/param_noise_di... | 0         |\
| value_loss              | 27453.11  |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 557.0692   |\
| ent_coef_loss           | -1925.8074 |\
| entropy                 | 80.56817   |\
| episodes                | 8          |\
| fps                     | 101        |\
| mean 100 episode reward | 213        |\
| n_updates               | 21557      |\
| policy_loss             | 2752522.0  |\
| qf1_loss                | 67385970.0 |\
| qf2_loss                | 64127430.0 |\
| time_elapsed            | 214        |\
| total timesteps         | 21656      |\
| value_loss              | 75452620.0 |\
----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 14352.112     |\
| ent_coef_loss           | -2915.1199    |\
| entropy                 | 80.56817      |\
| episodes                | 12            |\
| fps                     | 100           |\
| mean 100 episode reward | 204           |\
| n_updates               | 32385         |\
| policy_loss             | 70685280.0    |\
| qf1_loss                | 42144890000.0 |\
| qf2_loss                | 41729220000.0 |\
| time_elapsed            | 324           |\
| total timesteps         | 32484         |\
| value_loss              | 7757079000.0  |\
-------------------------------------------\
-----------------------------------------------\
| current_lr              | 0.0003            |\
| ent_coef                | 370136.3          |\
| ent_coef_loss           | -3896.512         |\
| entropy                 | 80.56817          |\
| episodes                | 16                |\
| fps                     | 103               |\
| mean 100 episode reward | 200               |\
| n_updates               | 43213             |\
| policy_loss             | 1767762400.0      |\
| qf1_loss                | 98507315000000.0  |\
| qf2_loss                | 109280594000000.0 |\
| time_elapsed            | 418               |\
| total timesteps         | 43312             |\
| value_loss              | 4470428000000.0   |\
-----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 9546524.0     |\
| ent_coef_loss           | -4885.763     |\
| entropy                 | 80.56817      |\
| episodes                | 20            |\
| fps                     | 106           |\
| mean 100 episode reward | 198           |\
| n_updates               | 54041         |\
| policy_loss             | 21351406000.0 |\
| qf1_loss                | 9.594762e+17  |\
| qf2_loss                | 2.5976685e+18 |\
| time_elapsed            | 508           |\
| total timesteps         | 54140         |\
| value_loss              | 5.5383535e+17 |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 24       |\
| fps                     | 108      |\
| mean 100 episode reward | 196      |\
| n_updates               | 64869    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 598      |\
| total timesteps         | 64968    |\
| value_loss              | nan      |\
--------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 109      |\
| mean 100 episode reward | 168      |\
| n_updates               | 75697    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 691      |\
| total timesteps         | 75796    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 111      |\
| mean 100 episode reward | 147      |\
| n_updates               | 86525    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 779      |\
| total timesteps         | 86624    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 112      |\
| mean 100 episode reward | 131      |\
| n_updates               | 97353    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 869      |\
| total timesteps         | 97452    |\
| value_loss              | nan      |\
--------------------------------------\
actions (27070, 30)\
obs (27070, 181)\
rewards (27070,)\
episode_returns (10,)\
episode_starts (27070,)\
actions (27070, 30)\
obs (27070, 181)\
rewards (27070,)\
episode_returns (10,)\
episode_starts (27070,)\
Total trajectories: 10\
Total transitions: 27070\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.467 seconds\
computegrad\
done in 0.213 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.49          0\
         1      0.796      0.075\
         2       4.92      0.414\
         3       1.05      0.739\
         4       1.64       1.94\
         5       2.88       2.82\
         6       2.31       3.31\
         7       1.44       6.42\
         8       8.37       9.14\
         9       3.54       10.5\
        10       6.44       11.6\
done in 0.465 seconds\
Expected: 0.347 Actual: 0.113\
Stepsize OK!\
vf\
done in 0.159 seconds\
sampling\
done in 4.304 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       22.2          0\
         1   1.03e+03     0.0999\
         2   1.97e+03       3.09\
         3        861       7.61\
         4   3.76e+03       10.7\
         5   6.33e+03       24.2\
         6   3.85e+04       54.9\
         7   1.74e+03       59.2\
         8    1.6e+03       62.7\
         9   1.72e+03        184\
        10   1.31e+04        243\
done in 0.046 seconds\
Expected: 4.689 Actual: 0.037\
Stepsize OK!\
vf\
done in 0.062 seconds\
sampling\
done in 4.508 seconds\
computegrad\
done in 0.009 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       2.44          0\
         1       6.08   0.000324\
         2       5.84     0.0994\
         3       5.06      0.148\
         4        274      0.276\
         5       11.6      0.293\
         6       4.77       1.77\
         7        262       3.14\
         8       43.6       3.16\
         9       15.5       3.78\
        10       17.6       6.78\
done in 0.048 seconds\
Expected: 0.357 Actual: 0.065\
Stepsize OK!\
vf\
done in 0.063 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.87991 |           nan |           nan |           nan |       0.36816 |       0.00000\
----------------------------------------\
| EpLenMean               | 2.71e+03   |\
| EpRewMean               | 2222.2627  |\
| EpThisIter              | 1          |\
| EpTrueRewMean           | 162        |\
| EpisodesSoFar           | 1          |\
| TimeElapsed             | 15         |\
| TimestepsSoFar          | 1024       |\
| entloss                 | 0.0        |\
| entropy                 | 42.570324  |\
| explained_variance_t... | -0.0467    |\
| meankl                  | 0.01148174 |\
| optimgain               | 0.06461267 |\
| surrgain                | 0.06461267 |\
----------------------------------------\
Training time (GAIL):  16.34038944641749  minutes\
======GAIL Validation from:  20191004 to  20200106\
GAIL Sharpe Ratio:  -0.419500655450782\
======Trading from:  20200106 to  20200406\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x14aee5860>\
previous_total_asset:1382250.084316241\
end_total_asset:1368064.8741450645\
total_reward:-14185.210171176586\
total_cost:  678.8728323453868\
total trades:  155\
Sharpe:  -0.4108222402177329\
============================================\
turbulence_threshold:  96.08032158358223\
======Model training from:  20090000 to  20200106\
======A2C Training========\
Training time (A2C):  1.9378812313079834  minutes\
======A2C Validation from:  20200106 to  20200406\
A2C Sharpe Ratio:  -0.4321124442863584\
======PPO Training========\
Training time (PPO):  7.049010419845581  minutes\
======PPO Validation from:  20200106 to  20200406\
PPO Sharpe Ratio:  -0.417553562136813\
======DDPG Training========\
Training time (DDPG):  1.1354415814081829  minutes\
======DDPG Validation from:  20200106 to  20200406\
DDPG Sharpe Ratio:  -0.44538477571783114\
======TD3 Training========\
Training time (TD3):  1.2800381342569986  minutes\
======TD3 Validation from:  20200106 to  20200406\
TD3 Sharpe Ratio:  -0.43902697336448554\
======GAIL Training========\
---------------------------------------\
| current_lr              | 0.0003    |\
| ent_coef                | 23.709023 |\
| ent_coef_loss           | -964.8341 |\
| entropy                 | 80.56817  |\
| episodes                | 4         |\
| fps                     | 118       |\
| mean 100 episode reward | 298       |\
| n_updates               | 10981     |\
| policy_loss             | 122561.0  |\
| qf1_loss                | 268459.5  |\
| qf2_loss                | 175933.86 |\
| reference_Q_mean        | 3.73      |\
| reference_Q_std         | 2.95      |\
| reference_action_mean   | 0.156     |\
| reference_action_std    | 0.951     |\
| reference_actor_Q_mean  | 4.65      |\
| reference_actor_Q_std   | 2.99      |\
| rollout/Q_mean          | 2.27      |\
| rollout/actions_mean    | 0.0312    |\
| rollout/actions_std     | 0.811     |\
| rollout/episode_steps   | 2.77e+03  |\
| rollout/episodes        | 3         |\
| rollout/return          | 283       |\
| rollout/return_history  | 283       |\
| time_elapsed            | 93        |\
| total timesteps         | 11080     |\
| total/duration          | 67.1      |\
| total/episodes          | 3         |\
| total/epochs            | 1         |\
| total/steps             | 9998      |\
| total/steps_per_second  | 149       |\
| train/loss_actor        | -4.37     |\
| train/loss_critic       | 2.33      |\
| train/param_noise_di... | 0         |\
| value_loss              | 996011.6  |\
---------------------------------------\
----------------------------------------\
| current_lr              | 0.0003     |\
| ent_coef                | 654.07965  |\
| ent_coef_loss           | -1971.9813 |\
| entropy                 | 80.56817   |\
| episodes                | 8          |\
| fps                     | 116        |\
| mean 100 episode reward | 324        |\
| n_updates               | 22061      |\
| policy_loss             | 3244970.5  |\
| qf1_loss                | 48889400.0 |\
| qf2_loss                | 46895670.0 |\
| time_elapsed            | 190        |\
| total timesteps         | 22160      |\
| value_loss              | 28607384.0 |\
----------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 18175.19      |\
| ent_coef_loss           | -2990.634     |\
| entropy                 | 80.56817      |\
| episodes                | 12            |\
| fps                     | 117           |\
| mean 100 episode reward | 330           |\
| n_updates               | 33141         |\
| policy_loss             | 89551430.0    |\
| qf1_loss                | 77190320000.0 |\
| qf2_loss                | 77044430000.0 |\
| time_elapsed            | 282           |\
| total timesteps         | 33240         |\
| value_loss              | 8349878300.0  |\
-------------------------------------------\
----------------------------------------------\
| current_lr              | 0.0003           |\
| ent_coef                | 505420.62        |\
| ent_coef_loss           | -4015.2747       |\
| entropy                 | 79.88066         |\
| episodes                | 16               |\
| fps                     | 118              |\
| mean 100 episode reward | 333              |\
| n_updates               | 44221            |\
| policy_loss             | 2406320600.0     |\
| qf1_loss                | 3.9276248e+16    |\
| qf2_loss                | 3.931486e+16     |\
| time_elapsed            | 373              |\
| total timesteps         | 44320            |\
| value_loss              | 11451624000000.0 |\
----------------------------------------------\
-------------------------------------------\
| current_lr              | 0.0003        |\
| ent_coef                | 14039483.0    |\
| ent_coef_loss           | -5004.4546    |\
| entropy                 | 80.56817      |\
| episodes                | 20            |\
| fps                     | 119           |\
| mean 100 episode reward | 335           |\
| n_updates               | 55301         |\
| policy_loss             | 29574853000.0 |\
| qf1_loss                | 3.0388093e+18 |\
| qf2_loss                | 5.536507e+18  |\
| time_elapsed            | 465           |\
| total timesteps         | 55400         |\
| value_loss              | 6.3677494e+17 |\
-------------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:135: RuntimeWarning: invalid value encountered in less\
  sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:136: RuntimeWarning: invalid value encountered in greater\
  buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 24       |\
| fps                     | 118      |\
| mean 100 episode reward | 336      |\
| n_updates               | 66381    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 558      |\
| total timesteps         | 66480    |\
| value_loss              | nan      |\
--------------------------------------\
/Users/mac/Deep-reinforcement-learning-code/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020copy/env/EnvMultipleStock_train.py:111: RuntimeWarning: invalid value encountered in double_scalars\
  df_total_value['daily_return'].std()\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 28       |\
| fps                     | 119      |\
| mean 100 episode reward | 288      |\
| n_updates               | 77461    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 647      |\
| total timesteps         | 77560    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 32       |\
| fps                     | 120      |\
| mean 100 episode reward | 252      |\
| n_updates               | 88541    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 735      |\
| total timesteps         | 88640    |\
| value_loss              | nan      |\
--------------------------------------\
--------------------------------------\
| current_lr              | 0.0003   |\
| ent_coef                | nan      |\
| ent_coef_loss           | nan      |\
| entropy                 | nan      |\
| episodes                | 36       |\
| fps                     | 120      |\
| mean 100 episode reward | 224      |\
| n_updates               | 99621    |\
| policy_loss             | nan      |\
| qf1_loss                | nan      |\
| qf2_loss                | nan      |\
| time_elapsed            | 824      |\
| total timesteps         | 99720    |\
| value_loss              | nan      |\
--------------------------------------\
actions (27700, 30)\
obs (27700, 181)\
rewards (27700,)\
episode_returns (10,)\
episode_starts (27700,)\
actions (27700, 30)\
obs (27700, 181)\
rewards (27700,)\
episode_returns (10,)\
episode_starts (27700,)\
Total trajectories: 10\
Total transitions: 27700\
Average returns: 0.0\
Std for returns: 0.0\
********** Iteration 0 ************\
Optimizing Policy...\
sampling\
done in 4.493 seconds\
computegrad\
done in 0.215 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.33          0\
         1       11.1     0.0583\
         2       1.98      0.107\
         3       2.34      0.133\
         4       1.03      0.561\
         5      0.903       1.61\
         6       32.7       2.14\
         7       4.26       2.35\
         8       7.35        3.3\
         9        2.4       3.42\
        10      0.928       3.92\
done in 0.420 seconds\
Expected: 0.220 Actual: 0.103\
Stepsize OK!\
vf\
done in 0.158 seconds\
sampling\
done in 4.223 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0       1.68          0\
         1       5.36      0.118\
         2       3.19      0.172\
         3       15.7      0.403\
         4       21.8       1.01\
         5       18.4        1.5\
         6       5.61       1.88\
         7       49.5       3.21\
         8       11.6       4.37\
         9       14.1        5.1\
        10       18.7       5.48\
done in 0.051 seconds\
Expected: 0.314 Actual: 0.074\
Stepsize OK!\
vf\
done in 0.073 seconds\
sampling\
done in 4.448 seconds\
computegrad\
done in 0.010 seconds\
conjugate_gradient\
      iter residual norm  soln norm\
         0        3.5          0\
         1       62.8   0.000806\
         2       18.7      0.016\
         3       2.57     0.0217\
         4       1.78      0.117\
         5        214      0.122\
         6       2.68       0.16\
         7       65.5      0.395\
         8       54.3      0.432\
         9       8.32      0.434\
        10       3.79      0.705\
done in 0.056 seconds\
Expected: 0.113 Actual: 0.061\
Stepsize OK!\
vf\
done in 0.077 seconds\
Optimizing Discriminator...\
generator_loss |   expert_loss |       entropy |  entropy_loss | generator_acc |    expert_acc\
      0.76359 |           nan |           nan |           nan |       0.49707 |       0.00000\
-----------------------------------------\
| EpLenMean               | 2.77e+03    |\
| EpRewMean               | 2373.8145   |\
| EpThisIter              | 1           |\
| EpTrueRewMean           | 165         |\
| EpisodesSoFar           | 1           |\
| TimeElapsed             | 14.9        |\
| TimestepsSoFar          | 1024        |\
| entloss                 | 0.0         |\
| entropy                 | 42.580074   |\
| explained_variance_t... | -0.123      |\
| meankl                  | 0.008214368 |\
| optimgain               | 0.06146431  |\
| surrgain                | 0.06146431  |\
-----------------------------------------\
Training time (GAIL):  15.19173462788264  minutes\
======GAIL Validation from:  20200106 to  20200406\
GAIL Sharpe Ratio:  -0.46464347072280343\
======Trading from:  20200406 to  20200707\
Used Model:  <stable_baselines.ppo2.ppo2.PPO2 object at 0x1485a1358>\
previous_total_asset:1368064.8741450645\
end_total_asset:1369631.7790712777\
total_reward:1566.904926213203\
total_cost:  495.35724465473646\
total trades:  107\
Sharpe:  0.12239801828013686\
Ensemble Strategy took:  502.8138222694397  minutes}